{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyerarchical RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard RL learns a monolithic policy to map states to actions. However, in complex, long-horizon environments, such flat policies struggle with sparse rewards, Learn inefficient exploration strategies and fail to transfer knowledge across tasks.\n",
    "Hierarchical RL addresses this by decomposing decision-making into multiple levels of abstractions, allowing an agent to reason over long time horizons and reuse behaviors across contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Decomposition\n",
    "HRL proposes a decomposition of the agent's behavior into two(o more) levels of policies:\n",
    "- **Low-level policy (Worker)**: executes the chosen sub-policy or primitive actions.\n",
    "- **High-level policy (Manager)**: selects goals, sub-policies or options.\n",
    "This supports temporal abstraction: high-level decisions are made less frequently and operate over extended periodso of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option Framework\n",
    "The most common formalism in HRL ius the Options Framework. It extends the standard MDP to inclde temporally extended actions, called options.\n",
    "### Options\n",
    "An option $\\omega$ is a tuple:\n",
    "$$\\omega = <I_\\omega, \\pi_\\omega, \\beta_\\omega>$$\n",
    "- $I_\\omega \\subseteq S$: initiation set.\n",
    "- $\\pi_\\omega(a|s)$: intra-policy, policy used while the option is active.\n",
    "- $\\beta_\\omega(s) \\rightarrow [0,1]$: termination condition, the probability the option ends in state s. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-Markov Decision Process (SMDP)\n",
    "The use of temporally extended options means we must move from standard MDPs to Semi-Markov Decision Processes, where actions(options) may last for multiple time steps.\n",
    "The agent makes decisions at time t, then executes a chosen option $\\omega_t$ for k steps,until termination condition is met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value function in HRL\n",
    "Let $\\Omega$ be the set of all options. The high-level policy chooses options $\\omega \\in \\Omega$.\n",
    "We define:\n",
    "- $Q_\\Omega(s,\\omega)$: expected return starting from state s, choosing option $\\omega$, and acting optimally afterward.\n",
    "- SMDP Q-value Bellman:\n",
    "$$Q_\\Omega(s,\\omega) = \\mathbb{E}[\\sum_{t=0}^{k-1}\\gamma^t r_t + \\gamma^k V_\\Omega(s')]$$\n",
    "where $s'$ is the state where the option terminates after k steps.\n",
    "- Intra-option Bellman:\n",
    "We can define an intra-option Q-function $Q(s,a,\\omega)$ as:\n",
    "$$Q(s,a,\\omega) = r(s,a) + \\gamma \\mathbb{E}_{s'}[(1-\\beta_\\omega(s'))Q_\\Omega(s',\\omega) + \\beta_\\omega(s')\\max_{\\omega'}Q_\\Omega(s',\\omega')]$$\n",
    "This formula is used in the Option-Critic architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Optimization\n",
    "We aim to learn:\n",
    "- The high-level policy $\\pi_\\Omega(\\omega|s)$.\n",
    "- The low-level policy $\\pi_\\omega(a|s)$.\n",
    "- The termination function $\\beta_\\omega(s)$.\n",
    "\n",
    "Each component can be parameterized using NN and optimized using gradient-based methods.\n",
    "\n",
    "### Intra-option policy\n",
    "Leg $\\theta$ parametrize $\\pi_\\omega(a|s)$. The intra-option policy gradient is:\n",
    "$$\\nabla_\\theta J = \\mathbb{E}[\\sum_t \\nabla_\\theta \\log \\pi_{\\omega_t}(a_t|s_t) \\cdot Q(s_t, a_t, \\omega_t)]$$\n",
    "### Termination function\n",
    "The termination gradient encourages continuing an option if it's successful:\n",
    "$$\\nabla_\\phi J=\\mathbb{E}[\\nabla_\\phi\\beta_\\omega(s_t) \\cdot (Q_\\Omega(s_t, \\omega)-V_\\Omega(s_t))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Abstraction and Efficiency\n",
    "\n",
    "A key idea in HRL is acting over multiple time scales:\n",
    "- High-level decisions (every k steps) guide long-term planning\n",
    "- Low-level policies (every step) execute short-term actions.\n",
    "This leads to:\n",
    "- Better exploration\n",
    "- More efficient credit assignment\n",
    "- Modular skills that can be reused across tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skill Discovery\n",
    "Skills (options) can be predefined or discovered autonomously. Autonomous discovery methods include:\n",
    "- Diversity-driven: DIAYN, HAC\n",
    "- Information-theoretic: maximize mutual information between skills and outcomes\n",
    "- Clustering trajectories: detect patterns and segment skills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option-Critic Architecture\n",
    "Introduces temporal abstraction by modeling options, which are like skills or macro-actions.\n",
    "Each option includes:\n",
    "- its own policy\n",
    "- termination condition\n",
    "- high-level policy that selects which option to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Markov Option $\\omega = (\\mathcal{I}_\\omega, \\pi_\\omega, \\beta_\\omega)$**\n",
    "- $\\mathcal{I}_\\omega$: initiation set\n",
    "- $\\pi_\\omega$: policy\n",
    "- $\\beta_\\omega$: termination condition\n",
    "Agent selects an option $\\omega$ at each time step, then executes it until termination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, num_options):\n",
    "        super().__init__()\n",
    "        self.q_net = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(state_dim + action_dim, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 1)\n",
    "            ) for _ in range(num_options)\n",
    "        ])\n",
    "\n",
    "    def forward(self, state, action, option):\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        return self.q_net[option](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntraOption(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.intra_option = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.intra_option(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Termination(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.net(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Manager(nn.Module):\n",
    "    def __init__(self, state_dim, num_options):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_options),\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        logits = self.net(state)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptionCriticAgent:\n",
    "    def __init__(self, state_dim,action_dim, num_options):\n",
    "        self.num_options = num_options\n",
    "        self.actor_heads = nn.ModuleList([IntraOption(state_dim, action_dim) for _ in range(num_options)])\n",
    "        self.termination_heads = nn.ModuleList([Termination(state_dim) for _ in range(num_options)])\n",
    "        self.critic = Critic(state_dim, action_dim, num_options)\n",
    "        self.manager = Manager(state_dim, num_options)\n",
    "\n",
    "        self.optim = optim.Adam(\n",
    "            list(self.critic.parameters()) +\n",
    "            list(self.manager.parameters()) +\n",
    "            [p for net in self.actor_heads for p in net.parameters()]+\n",
    "            [p for net in self.termination_heads for p in net.parameters()],\n",
    "            lr = 1e-4\n",
    "        )\n",
    "\n",
    "        self.current_option = None\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state_tensor= torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        option_probs = self.manager(state_tensor)\n",
    "        dist = torch.distributions.Categorical(option_probs)\n",
    "        return dist.sample().item()\n",
    "    \n",
    "    def should_terminate(self, state, option):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        termination_prob =  self.termination_heads[option](state_tensor)\n",
    "        return torch.rand(1).item() < termination_prob.item()\n",
    "    \n",
    "    def select_action(self, state, option):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        action = self.actor_heads[option](state_tensor)\n",
    "        return action.detach().numpy()\n",
    "    \n",
    "    def train(self, state, action, reward, next_state, done, option):\n",
    "        self.optim.zero_grad()\n",
    "\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        action_tensor = torch.tensor(action, dtype=torch.float32).unsqueeze(0)\n",
    "        next_state_tensor = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "        reward_tensor = torch.tensor([reward], dtype=torch.float32)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_action = self.actor_heads[option](next_state_tensor)\n",
    "            q_next = self.critic(next_state_tensor, next_action, option)\n",
    "            target = reward_tensor + 0.99 * q_next * (1 - int(done))\n",
    "\n",
    "        q_pred = self.critic(state_tensor, action_tensor, option)\n",
    "        critic_loss = nn.MSELoss()(q_pred, target)\n",
    "\n",
    "        pred_action = self.actor_heads[option](state_tensor)\n",
    "        q_val = self.critic(state_tensor, pred_action, option)\n",
    "        actor_loss = -q_val.mean()\n",
    "\n",
    "        beta = self.termination_heads[option](state_tensor)\n",
    "        q_option = self.critic(next_state_tensor, self.actor_heads[option](next_state_tensor), option)\n",
    "        v = sum([self.critic(next_state_tensor, self.actor_heads[o](next_state_tensor), o) for o in range(self.num_options)]) / self.num_options\n",
    "        termination_loss = beta * (q_option - v).detach()\n",
    "\n",
    "        total_loss = actor_loss + critic_loss + termination_loss.mean()\n",
    "        total_loss.backward()\n",
    "        self.optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env= gym.make(\"Pendulum-v1\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "num_options = 2\n",
    "\n",
    "agent = OptionCriticAgent(state_dim, action_dim, num_options)\n",
    "\n",
    "for ep in range(300):\n",
    "    state, _ = env.reset()\n",
    "    agent.current_option = agent.select_action(state)\n",
    "    total_reward = 0\n",
    "\n",
    "    for step in range(200):\n",
    "        if agent.should_terminate(state, agent.current_option):\n",
    "            agent.current_option = agent.select_action(state)\n",
    "\n",
    "        action = agent.select_action(state, agent.current_option)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        agent.train(state, action, reward, next_state, done, agent.current_option)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        if done: break\n",
    "\n",
    "    if ep %10 == 0:\n",
    "        print(\"Episode: {}, Total Reward: {}\".format(ep, total_reward:.2f))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Option-Critic\n",
    "Implement Soft Q-learning for better exploration and stability.\n",
    "Soft Q-function:\n",
    "$$Q_\\omega(s,a) = r + \\gamma \\mathbb{E}_{s'}[(1-\\beta_\\omega(s'))V_\\omega(s') + \\beta_\\omega(s')V(s')]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftOptionActor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128,127), nn.ReLU(),\n",
    "        )\n",
    "        self.mean = nn.Linear(128, action_dim)\n",
    "        self.log_std = nn.Linear(128, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.fc(state)\n",
    "        mean = self.mean(x)\n",
    "        log_std = torch.clamp(self.log_std(x), -20,2)\n",
    "        std = torch.exp(log_std)\n",
    "        return mean, std\n",
    "    \n",
    "    def sample(self, state):\n",
    "        mean, std = self.forward(state)\n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "        x_t = normal.rsample()\n",
    "        action = torch.tanh(x_t)\n",
    "        log_prob = normal.log_prob(x_t).sum(dim=-1)\n",
    "        log_prob -= torch.log(1-action.pow(2)+1e-6).sum(dim=-1)\n",
    "        return action, log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftOptionCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.q1 = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        self.q2 = nn.Sequential(\n",
    "            nn.Linear(state_dim+action_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        sa = torch.cat([state, action], dim=-1)\n",
    "        return self.q1(sa), self.q2(sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerminationHead(nn.Sequential):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 1), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self,state):\n",
    "        return self.net(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptionValue(nn.Module):\n",
    "    def __init__(self, state_dim, num_options):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, num_options)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.net(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftOptionCriticAgent:\n",
    "    def __init__(self, state_dim, action_dim, num_options, alpha=0.2):\n",
    "        self.num_options = num_options\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.actors = nn.ModuleList([SoftOptionActor(state_dim, action_dim) for _ in range(num_options)])\n",
    "        self.critics = nn.ModuleList([SoftOptionCritic(state_dim, action_dim) for _ in range(num_options)])\n",
    "        self.target_critics = nn.ModuleList([SoftOptionCritic(state_dim, action_dim) for _ in range(num_options)])\n",
    "        self.terminations = nn.ModuleList([TerminationHead(state_dim) for _ in range(num_options)])\n",
    "        self.option_value = OptionValue(state_dim, num_options)\n",
    "\n",
    "        self.optimizers = []\n",
    "        for i in range(num_options):\n",
    "            self.target_critics[i].load_state_dict(self.critics[i].state_dict())\n",
    "            self.optimizers.append(optim.Adam(\n",
    "                list(self.actors[i].parameters()) +\n",
    "                list(self.critics[i].parameters()) +\n",
    "                list(self.terminations[i].parameters()),\n",
    "                lr = 3e-4\n",
    "            ))\n",
    "        \n",
    "        self.value_optimizer = optim.Adam(self.option_value.parameters(), lr=3e-4)\n",
    "        self.current_option = None\n",
    "\n",
    "    def select_option(self, state):\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "            option_vals = self.option_value(state_tensor)\n",
    "            probs = torch.softmax(option_vals /self.alpha, dim=-1)\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            return dist.sample().item()\n",
    "        \n",
    "    def should_terminate(self, state, option):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        action, _ = self.actors[option].sample(state_tensor)\n",
    "        return action.squeeze(0).detach().numpy()\n",
    "    \n",
    "    def select_action(self, state, option):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        action, _ = self.actors[option].sample(state_tensor)\n",
    "        return action.squeeze(0).detach().numpy()\n",
    "\n",
    "    def train(self, batch, option):\n",
    "        states, actions, rewards, next_states, dones = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_actions, next_log_probs = self.actors[option].sample(next_states)\n",
    "            q1_target, q2_target = self.target_critics[option](next_states, next_actions)\n",
    "            q_min = torch.min(q1_target, q2_target)\n",
    "            target_q = rewards + (1-dones) * 0.99 * (q_min - self.alpha * next_log_probs)\n",
    "\n",
    "        q1, q2 = self.critics[option](states, actions)\n",
    "        critic_loss = nn.MSELoss()(q1, target_q) + nn.MSELoss()(q2, target_q)\n",
    "\n",
    "        new_actions, new_log_probs = self.actors[option].sample(states)\n",
    "        q1_pi, q2_pi = self.critics[option](states, new_actions)\n",
    "        q_pi = torch.min(q1_pi, q2_pi)\n",
    "        actor_loss = (self.alpha * new_log_probs - q_pi).mean()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            current_v = self.option_value(states).gather(1, torch.tensor([[option]]*states.size(0)))\n",
    "\n",
    "        termination = self.terminations[option](states)\n",
    "        termination_loss = termination * (q_pi.detach()-current_v).detach()\n",
    "        termination_loss = termination_loss.mean()\n",
    "\n",
    "        total_loss=actor_loss + critic_loss + termination_loss\n",
    "\n",
    "        self.optimizers[option].zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.optimizers[option].step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=1000000):\n",
    "        self.buffer = []\n",
    "        self.max_size = max_size\n",
    "        \n",
    "    def add(self, *transition):\n",
    "        if len(self.buffer) >= self.max_size:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            torch.tensor(states, dtype=torch.float32),\n",
    "            torch.tensor(actions, dtype=torch.float32),\n",
    "            torch.tensor(rewards, dtype=torch.float32).unsqueeze(-1),\n",
    "            torch.tensor(next_states, dtype=torch.float32),\n",
    "            torch.tensor(dones, dtype=torch.float32).unsqueeze(-1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "num_options = 2\n",
    "\n",
    "agent = SoftOptionCriticAgent(state_dim, action_dim, num_options)\n",
    "replay_buffer = ReplayBuffer()\n",
    "total_rewards=[]\n",
    "\n",
    "episodes = 200\n",
    "batch_size = 64\n",
    "warmup_steps = 1000\n",
    "update_every = 1\n",
    "\n",
    "for ep in range(episodes):\n",
    "    state, _ = env.reset()\n",
    "    option = agent.select_option(state)\n",
    "    ep_reward = 0\n",
    "\n",
    "    for step in range(200):\n",
    "        if agent.should_terminate(state, option):\n",
    "            option = agent.select_option(state)\n",
    "        \n",
    "        action = agent.select_action(state, option)\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        replay_buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "        state = next_state\n",
    "        ep_reward += reward\n",
    "\n",
    "        if len(replay_buffer.buffer) > warmup_steps and step % update_every == 0:\n",
    "            batch = replay_buffer.sample(batch_size)\n",
    "            agent.train(batch, option)\n",
    "\n",
    "        if done: break\n",
    "\n",
    "    total_rewards.append(ep_reward)\n",
    "    if ep % 10 == 0:\n",
    "        print(\"Episode: {}, Total Reward: {}\".format(ep, ep_reward))\n",
    "\n",
    "plt.plot(total_rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feudal RL and DIAYN (Diversity is All You Need)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Feudal RL**\n",
    "Introduces a hierachical structure where a manager sets goals, and a worker executes actions to fullfill them.\n",
    "The key idea is to decompose control into high-level goals and low-level actions.\n",
    "The worker policy is:\n",
    "$$\\pi(a_t | s_t, g_t)$$\n",
    "where $g_t$ is the goal set by the manager.\n",
    "The manager emits a goal every k steps:\n",
    "$$g_t = f_m(s_t)$$\n",
    "the intrinsic Reward(Feudal Signal) is given based on how much progress it was made towards the goal:\n",
    "$$r_t^{intr} = g_t^T(f(s_{t+1}-f(s_t)))$$\n",
    "Where:\n",
    "- $f(\\cdot)$ is a state encoder that maps raw states into a latent space\n",
    "- $g_t$ the goal vector\n",
    "- $f(s_{t+1}-f(s_t))$ is the vector of change in latent features\n",
    "the inner product measures alignment of movement and goal- the more the agent moves in the goal's direction, the higher the reward.\n",
    "**Optimization Objective**\n",
    "Worker:\n",
    "$$J_w = \\mathbb{E}[\\sum_t r_t^{intr}]$$\n",
    "Manager:\n",
    "$$J_m = \\mathbb{E}[\\sum_t r_t^{extr}]$$\n",
    "where $r_t^{extr}$ is the extrinsic reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, image_shape, latent_dim):\n",
    "        super().__init__()\n",
    "        c,h,w = image_shape\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(c, 32, 8,1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4,2), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3,1), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1,c,h,w)\n",
    "            conv_out_dim = self.conv(dummy).view(1,-1).shape[1]\n",
    "        self.fc = nn.Linear(conv_out_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x / 255.\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Manager(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        goal = self.fc(x)\n",
    "        return F.normalize(goal, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker(nn.Module):\n",
    "    def __init__(self, latent_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(latent_dim*2, 128), nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, encoder_state, goal):\n",
    "        x = torch.cat([encoder_state, goal], dim=-1)\n",
    "        return torch.tanh(self.fc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_intrinsic_reward(f_st, f_st1, goal):\n",
    "    delta = f_st1 - f_st\n",
    "    return torch.sum(goal*delta, dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeudalAgent(nn.Module):\n",
    "    def __init__(self, image_shape, action_dim, latent_dim, goal_interval=10):\n",
    "        super().__init__()\n",
    "        self.encoder = ConvEncoder(image_shape, latent_dim)\n",
    "        self.manager = Manager(latent_dim)\n",
    "        self.worker = Worker(latent_dim, action_dim)\n",
    "\n",
    "        self.goal_interval = goal_interval\n",
    "        self.latent_dim = latent_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.encoder_optim = optim.Adam(self.encoder.parameters(), lr=1e-4)\n",
    "        self.manager_optim = optim.Adam(self.manager.parameters(), lr=1e-4)\n",
    "        self.worker_optim = optim.Adam(self.worker.parameters(), lr=1e-4)\n",
    "\n",
    "    def get_goal(self, encoded_state):\n",
    "        return self.manager(encoded_state)\n",
    "    \n",
    "    def get_action(self, encoded_state, goal):\n",
    "        return self.worker(encoded_state, goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CarRacing-v2\", render_mode=\"rgb_array\")\n",
    "obs_shape = (3,96,96)\n",
    "agent =FeudalAgent(obs_shape, 3, 64)\n",
    "rewards = []\n",
    "\n",
    "for ep in range(100):\n",
    "    obs, _ = env.reset()\n",
    "    obs = torch.tensor(obs, dtype=torch.float32).permute(2,0,1).unsqueeze(0)\n",
    "\n",
    "    f_st = agent.encoder(obs)\n",
    "    goal = agent.get_goal(f_st)\n",
    "    ep_reward = 0\n",
    "\n",
    "    for t in range(1000):\n",
    "        if t % agent.goal_interval == 0:\n",
    "            goal = agent.get_goal(f_st.detach())\n",
    "\n",
    "        action = agent.get_action(f_st, goal)\n",
    "        np_action = action[0].detach().numpy()\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(np_action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        next_obs_tensor = torch.tensor(next_obs, dtype=torch.float32).permute(2,0,1).unsqueeze(0)\n",
    "        f_st1 = agent.encoder(next_obs_tensor)\n",
    "\n",
    "        #Compute Intrinsic reward\n",
    "        r_intr = compute_intrinsic_reward(f_st, f_st1, goal)\n",
    "\n",
    "        #Update Worker\n",
    "        agent.worker_optim.zero_grad()\n",
    "        pred_action = agent.get_action(f_st, goal)\n",
    "        action_loss = F.mse_loss(pred_action, action)\n",
    "        (-r_intr.mean()+action_loss).backward()\n",
    "        agent.worker_optim.step()\n",
    "\n",
    "        #Update Manager\n",
    "        agent.manager_optim.zero_grad()\n",
    "        (-torch.tensor(reward)).backward()\n",
    "        agent.manager_optim.step()\n",
    "\n",
    "        f_st = f_st1\n",
    "        ep_reward += reward\n",
    "        if done: break\n",
    "\n",
    "    rewards.append(ep_reward)\n",
    "    if ep % 10 == 0:\n",
    "        print(\"Episode: {}, Total Reward: {}\".format(ep, ep_reward))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DIAYN (Diversity is All You Need)\n",
    "Proposes a framework where agents learn useful, diverse skills without any extrinsic reward and do so in an unsupervised way. These skills can later be reused for downstream tasks or transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIYAN is based on mutual information:\n",
    "$$\\mathcal{I}(S;Z) = H(S) - H(S|Z)$$\n",
    "where:\n",
    "- $S$ is the state\n",
    "- $Z$ is the skill\n",
    "\n",
    "DIAYN maximizes this by:\n",
    "- Ensuring skills are diverse\n",
    "- Ensuring each skill is predictable from the state\n",
    "\n",
    "No reward function needed the agent rewards itself for doing things differently across skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Skill-Conditioned Policy**:\n",
    "$$\\pi(a|s,z) \\space \\text{where} \\space z \\sim p(z)$$\n",
    "- One shared policy $\\pi(a|s,z)$\n",
    "- Latent skill z sampled from uniform prior\n",
    "\n",
    "**Skill Discriminator**:\n",
    "$$D_\\phi(z|s)$$\n",
    "Learns to classifu the skill z from state s, used to compute the intrinsic reward:\n",
    "$$r^{intr}(s,z) = \\log D_\\phi(z|s) - \\log p(z)$$\n",
    "this encourages:\n",
    "- High skill identifiability\n",
    "- Maximally diverse behaviors\n",
    "\n",
    "**Optimization Objective**:\n",
    "1. Policy:\n",
    "$$\\mathbb{E}_{s,z}[\\log D_\\phi(z|s)]$$\n",
    "2. Discriminator:\n",
    "$$\\mathcal{L}_D = -\\mathbb{E}_{s,z}[\\log D_\\phi(z|s)]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, image_shape, latent_dim):\n",
    "        super().__init__()\n",
    "        c, h, w = image_shape\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(c, 32, kernel_size=8, stride=4), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1), nn.ReLU()\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, c, h, w)\n",
    "            conv_out_dim = self.conv(dummy).view(1, -1).shape[1]\n",
    "        self.fc = nn.Linear(conv_out_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x / 255.0\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkillPolicy(nn.Module):\n",
    "    def __init__(self, latent_dim, skill_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(latent_dim+skill_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, state_feat, skill_onehot):\n",
    "        x = torch.cat([state_feat, skill_onehot], dim=-1)\n",
    "        return torch.tanh(self.fc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, latent_dim, skill_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, skill_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, state_feat):\n",
    "        return self.fc(state_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DIAYNAgent:\n",
    "    def __init__(self,image_shape, action_dim, latent_dim=64, skill_dim=10, lr=1e-4):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.skill_dim = skill_dim\n",
    "\n",
    "        self.encoder = ConvEncoder(image_shape, latent_dim)\n",
    "        self.skill_policy = SkillPolicy(latent_dim, skill_dim, action_dim)\n",
    "        self.discriminator = Discriminator(latent_dim, skill_dim)\n",
    "\n",
    "        self.policy_optim = optim.Adam(list(self.encoder.parameters())+list(self.skill_policy.parameters()), lr=lr)\n",
    "        self.discriminator_optim = optim.Adam(self.discriminator.parameters(), lr=lr)\n",
    "        self.skill_dist = torch.distributions.Categorical(torch.ones(skill_dim)/skill_dim)\n",
    "\n",
    "    def sample_skill(self, batch_size=1):\n",
    "        return self.skill_dist.sample((batch_size,))\n",
    "    \n",
    "    def one_hot(self, skill_idx):\n",
    "        return F.one_hot(skill_idx, num_classes=self.skill_dim).float()\n",
    "    \n",
    "    def get_action(self, obs, skill_idx):\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32).permute(2,0,1).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            feat = self.encoder(obs_tensor)\n",
    "            skill_onehot = self.one_hot(torch.tensor([skill_idx]))\n",
    "            action = self.skill_policy(feat, skill_onehot)\n",
    "        return action[0].detach().numpy()\n",
    "    \n",
    "    def compute_intrinsic_reward(self, feat, skill_idx):\n",
    "        with torch.no_grad():\n",
    "            logits = self.discriminator(feat)\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            log_pz = torch.log(torch.tensor(1.0/self.skill_dim))\n",
    "            return log_probs[0, skill_idx]-log_pz\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CarRacing-v2\", render_mode=\"rgb_array\")\n",
    "image_shape = (3,96,96)\n",
    "action_dim = env.action_space.shape[0]\n",
    "agent = DIAYNAgent(image_shape, action_dim)\n",
    "\n",
    "rewards = []\n",
    "\n",
    "for ep in range(100):\n",
    "    skill = agent.sample_skill().item()\n",
    "    obs, _ = env.reset()\n",
    "    ep_reward = 0\n",
    "\n",
    "    for t in range(500):\n",
    "        action = agent.get_action(obs, skill)\n",
    "        next_obs, _, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32).permute(2,0,1).unsqueeze(0)\n",
    "        next_obs_tensor = torch.tensor(next_obs, dtype=torch.float32).permute(2,0,1).unsqueeze(0)\n",
    "        feat = agent.encoder(obs_tensor)\n",
    "        next_feat = agent.encoder(next_obs_tensor)\n",
    "\n",
    "        logits = agent.discriminator(next_feat)\n",
    "        target = torch.tensor([skill], dtype=torch.long)\n",
    "        disc_loss = F.cross_entropy(logits, target)\n",
    "\n",
    "        agent.discriminator_optim.zero_grad()\n",
    "        disc_loss.backward()\n",
    "        agent.discriminator_optim.step()\n",
    "\n",
    "        logits = agent.discriminator(next_feat)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        intrinsic_reward = log_probs[0, skill]-np.log(1.0/agent.skill_dim)\n",
    "\n",
    "        action_pred = agent.skill_policy(feat, agent.one_hot(torch.tensor([skill])))\n",
    "        policy_loss = -intrinsic_reward\n",
    "\n",
    "        agent.policy_optim.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        agent.policy_optim.step()\n",
    "\n",
    "        obs = next_obs\n",
    "        ep_reward += intrinsic_reward\n",
    "        if done: break\n",
    "\n",
    "    rewards.append(ep_reward)\n",
    "    if ep % 10 == 0:\n",
    "        print(\"Episode: {}, Total Reward: {}\".format(ep, ep_reward))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
