{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIAYN (Diversity is All You Need)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skill discovery in RL aims to learn useful behavioral primitives (skills/options) without external rewards. DIAYN proposes that diversity alone, without specific goals, can be a powerful unsupervised objective.\n",
    "The central idea is:\n",
    "\"Can we learn a diverse set of skills such that each skill leads to distinct behavior, even without any extrinsic reward?\"\n",
    "DIAYN does this by:\n",
    "- Assigining a fixed skill at the beginning of each episode.\n",
    "- Encouraging the policy to behave as distinctively as possible so that a discriminator can easily infer which skill was used, given only the observed state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $z \\in {1,2,...,K}$ a categorical skill ID, the policy $\\pi(a_t|s_t,z)$ is conditioned on the skill, the goal is to make the skill predictable from the agent's behavior, maximize:\n",
    "$$I(s_t;z)$$\n",
    "Where I denotes mutual information.\n",
    "A discriminator $q_\\phi(z|s_t)$ is trained to predict the skill given a state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent receives an intrinsic reward proportional to how easily the discriminator can identify the skill:\n",
    "$$r_t^{intr} = \\log q_\\phi(z|s_t)$$\n",
    "This leads to the following unsupervised RL objective:\n",
    "$$\\max_\\pi \\mathbb{E}_{z \\sim p(z)} \\mathbb{E}_{\\tau  \\sim \\pi(\\cdot | z)}[\\sum_t \\log q_\\phi(z|s_t)]$$\n",
    "Where:\n",
    "- $\\pi(a|s,z)$ skill-conditioned policy\n",
    "- $q_\\phi(z|s)$ learned discriminator\n",
    "- $p(z)$ uniform skill prior\n",
    "- $s_t$ observation at time t\n",
    "\n",
    "This alligns with empowerment and variational information maximization, where high mutual information implies better skill controllability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Derivation of Mutual Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mutual info can be written as:\n",
    "$$I(s;z) = H(z) - H(z|s)$$\n",
    "Since $H(z)$ is constant (uniform prior), maximizing I(s;z) is equivalent to minimizing $H(z|s)$, making skill predictable from states.\n",
    "We use variational lower bound:\n",
    "$$I(s;z) \\geq \\mathbb{E}_{z \\sim p(z), s\\sim\\pi(z)}[\\log q_\\phi(z|s)]$$\n",
    "This becomes our intrinsic reward training objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIAYN ensures:\n",
    "- Diversity: each skill reaches different parts of the state space\n",
    "- Disnetaglement: the agent learns a latent representation where skills correspond to semantically distinct behaviors.\n",
    "- No collapse: the discriminator prevents the policy from learning degenerate skills that lead to the same state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge in pixel based envs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State representation $s_t$ must be learned from raw pixel observations $o_t$, we can use a ConvNet encoder to map $o_t \\to s_t \\in \\mathbb{R}^d$.\n",
    "This requires the encoder to be trainable and expressive enough to:\n",
    "- retain spatial structure\n",
    "- encode meaningful behavioral info.\n",
    "This entire discriminator is trained on the latent state $s_t$, which is the output of the encoder:\n",
    "$$q_\\phi(z|f_\\theta(o_t))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Option-Critic: DIAYN can be interpreted as a special case where skills/options are selected stochastically and not terminated until episode end.\n",
    "- DADS, VALOR: Continuous skill analogs that extend DIAYN with continuous latent skills and better control mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelEncoder(nn.Module):\n",
    "    def __init__(self, input_shape=(3,96,96), latent_dim=256):\n",
    "        super().__init__()\n",
    "        c,h,w = input_shape\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(c, 32, kernel_size=4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2), nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, *input_shape)\n",
    "            out_dim = self.conv(dummy).view(1,-1).shape[1]\n",
    "        \n",
    "        self.fc = nn.Linear(out_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x /255.0\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkillPolicy(nn.Module):\n",
    "    def __init__(self, obs_dim, skill_dim, action_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim + skill_dim, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, skill):\n",
    "        x = torch.cat([obs, skill], dim=-1)\n",
    "        return torch.tanh(self.net(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkillDiscriminator(nn.Module):\n",
    "    def __init__(self, obs_dim, num_skills, hidden_dim = 256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_skills)\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return self.net(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_intrinsic_reward(discriminator, state_latent, skill_id):\n",
    "    logits = discriminator(state_latent)\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    return log_probs.gather(1, skill_id.unsqueeze(1)).squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DIAYNAgent(nn.Module):\n",
    "    def __init__(self, image_shape, num_skill, action_dim, latent_dim=256):\n",
    "        super().__init__()\n",
    "        self.encoder = PixelEncoder(image_shape, latent_dim)\n",
    "        self.skill_embedding = nn.Embedding(num_skill, latent_dim)\n",
    "        self.skill_policy = SkillPolicy(latent_dim, num_skill, action_dim)\n",
    "        self.skill_discriminator = SkillDiscriminator(latent_dim, num_skill)\n",
    "\n",
    "    def act(self, obs, skill):\n",
    "        with torch.no_grad():\n",
    "            obs_encoded = self.encoder(obs)\n",
    "            skill = self.skill_embedding(skill)\n",
    "            return self.skill_policy(obs_encoded, skill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100_000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, obs, action, reward, next_obs, done, skill):\n",
    "        self.buffer.append((obs, action, reward, next_obs, done, skill))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        obs, actions, rewards, next_obs, dones, skills = zip(*batch)\n",
    "        return (\n",
    "            torch.tensor(np.stack(obs), dtype=torch.float32).to(device),\n",
    "            torch.tensor(np.stack(actions), dtype=torch.float32).to(device),\n",
    "            torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(device),\n",
    "            torch.tensor(np.stack(next_obs), dtype=torch.float32).to(device),\n",
    "            torch.tensor(dones, dtype=torch.float32).unsqueeze(1).to(device),\n",
    "            torch.tensor(skills, dtype=torch.long).to(device)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(target, source, tau):\n",
    "    for t_param, s_param in zip(target.parameters(), source.parameters()):\n",
    "        t_param.data.copy_(t_param.data * (1.0 - tau) + s_param.data * tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(agent, replay_buffer, batch_size=128, gamma=0.99, alpha=0.2):\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return\n",
    "\n",
    "    batch = replay_buffer.sample(batch_size)\n",
    "    obs, action, next_obs, reward, done, skill = batch\n",
    "\n",
    "    # Encode current and next observations\n",
    "    latent_s = agent.encoder(obs)\n",
    "    latent_next_s = agent.encoder(next_obs).detach()\n",
    "\n",
    "    # === Critic update ===\n",
    "    with torch.no_grad():\n",
    "        next_action, logp_next = agent.policy.sample(latent_next_s, skill)\n",
    "        target_q1, target_q2 = agent.target_critic(latent_next_s, next_action, skill)\n",
    "        target_q = torch.min(target_q1, target_q2) - alpha * logp_next\n",
    "        target = reward + gamma * (1 - done) * target_q\n",
    "\n",
    "    q1, q2 = agent.critic(latent_s, action, skill)\n",
    "    critic_loss = F.mse_loss(q1, target) + F.mse_loss(q2, target)\n",
    "\n",
    "    agent.critic_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    agent.critic_optimizer.step()\n",
    "\n",
    "    # === Policy update ===\n",
    "    action_pi, logp_pi = agent.policy.sample(latent_s, skill)\n",
    "    q1_pi, q2_pi = agent.critic(latent_s, action_pi, skill)\n",
    "    policy_loss = (alpha * logp_pi - torch.min(q1_pi, q2_pi)).mean()\n",
    "\n",
    "    agent.policy_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    agent.policy_optimizer.step()\n",
    "\n",
    "    # === Discriminator update ===\n",
    "    logits = agent.discriminator(latent_s.detach())\n",
    "    disc_loss = F.cross_entropy(logits, skill)\n",
    "\n",
    "    agent.disc_optimizer.zero_grad()\n",
    "    disc_loss.backward()\n",
    "    agent.disc_optimizer.step()\n",
    "\n",
    "    # === Target Network Update ===\n",
    "    soft_update(agent.target_critic, agent.critic, tau=0.005)\n",
    "\n",
    "    return {\n",
    "        \"critic_loss\": critic_loss.item(),\n",
    "        \"policy_loss\": policy_loss.item(),\n",
    "        \"disc_loss\": disc_loss.item()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_skills(env, agent, num_skills=10, episodes_per_skill=1, render=False):\n",
    "    skill_trajectories = []\n",
    "\n",
    "    for skill_id in range(num_skills):\n",
    "        for _ in range(episodes_per_skill):\n",
    "            obs, _ = env.reset()\n",
    "            done = False\n",
    "            episode = []\n",
    "            skill = torch.tensor([skill_id], dtype=torch.long).to(device)\n",
    "\n",
    "            while not done:\n",
    "                obs_tensor = preprocess(obs)\n",
    "                with torch.no_grad():\n",
    "                    latent = agent.encoder(obs_tensor.unsqueeze(0))\n",
    "                    action = agent.policy(latent, skill.unsqueeze(0)).cpu().numpy()[0]\n",
    "                obs, _, done, _, _ = env.step(action)\n",
    "                episode.append(obs)\n",
    "\n",
    "                if render:\n",
    "                    env.render()\n",
    "\n",
    "            skill_trajectories.append((skill_id, episode))\n",
    "\n",
    "    return skill_trajectories\n",
    "\n",
    "def discriminator_accuracy(agent, dataloader):\n",
    "    total, correct = 0, 0\n",
    "    for obs_batch, skill_batch in dataloader:\n",
    "        latent = agent.encoder(obs_batch.to(device))\n",
    "        logits = agent.discriminator(latent)\n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "        correct += (pred == skill_batch.to(device)).sum().item()\n",
    "        total += len(skill_batch)\n",
    "    return correct / total"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "procgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
