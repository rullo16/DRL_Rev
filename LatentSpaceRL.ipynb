{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent-Space Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In traditional MBRL approaches like MBPO, the agent learns a predictive model of the environment in raw state space. This means the model directly learns to predict next states and rewards based on previous states and actions. However, this approach struggles in:\n",
    "1. High-Dimensional Environments:\n",
    "    - Raw observations contain redundant and unstructured information\n",
    "    - Learning directly from raw pixels is computationally expensive and requires extensive data.\n",
    "2. Long-horizon planning:\n",
    "    - Model errors accumulate over long rollouts, leading to unreliable predictions.\n",
    "To overcome these challenges, Latent Space MBRL learns a compressed, meaningful representation of the state rather than working directly in raw state space. This approach extracts essential features and enables more accurate predictions with fewer samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A latent space is a low-dimensional, compressed representation of the environment state that preserves only essential features. Instead of working with full observations, the model encodes states into this latent space and makes predictions there.\n",
    "Common methods to learn latent representations:\n",
    "- **Autoencoders**: Compress and reconstruct inputs through an encoder-decoder architecture.\n",
    "- **Variational Autoencoders (VAEs)**: Learn a probabilistic distribution over the latent space.\n",
    "- **Contrastive Learning**: Learn embeddings that maximize similarity between positive pairs of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of modeling the environment in raw state space, we define:\n",
    "1. **Encoder**: $E(s) \\rightarrow$ Maps high.dimensional state $s_t$ to a latent representation $z_t$.\n",
    "2. **Latent Transition Model** $f(z_t, a_t) \\rightarrow$ predicts the next latent state $z_{t+1}$ \n",
    "3. **Reward Model** $R(z_t, a_t) \\rightarrow$ Predicts the expected reward\n",
    "4. **Decoder** $D(z) \\rightarrow$ Optionally reconstructs $s_t$ from $z_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Encoding Observations into Latent Space\n",
    "We first encode a raw state $s_t$ into a low-dimensional latent state $z_T$:\n",
    "$$z_t = E_\\phi(s_t)$$\n",
    "where $E_\\phi$ is the encoder network with parameters $\\phi$.\n",
    "2. Learning a Latent Transition Model\n",
    "We train a dynamics model in the latent space:\n",
    "$$\\hat{z}_{t+1} = f_\\theta(z_t, a_t)$$\n",
    "where $f_\\theta$ is the latent transition model.\n",
    "3. Predicting Rewards in Latent Space\n",
    "Instead of predicting rewards in raw observation space, we use:\n",
    "$$\\hat{r}_t = R_\\psi(z_t, a_t)$$\n",
    "where $R_\\psi$ is the reward model.\n",
    "4. Reconstructing States (Optional)\n",
    "If needed, we reconstruct the original state form the latent representation:\n",
    "$$\\hat{s}_t = D_\\omega(z_t)$$\n",
    "where $D_\\omega$ is the decoder network with parameters $\\omega$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions\n",
    "To train the latent space, we optimize the following:\n",
    "- Latent Transition Loss:\n",
    "$$\\mathcal{L_{trans}} = \\sum_t || \\hat{z}_{t+1} - z_{t+1}||^2$$\n",
    "- Reconstruction Loss (if using autoencoder):\n",
    "$$\\mathcal{L_{rec}} = \\sum_t || D_\\omega(z_t) - s_t||^2$$\n",
    "- Reward Prediction Loss:\n",
    "$$\\mathcal{L_{rew}} = \\sum_t || \\hat{r}_t - r_t||^2$$\n",
    "By optimizing these loss functions jointly, we ensure the latent space is both useful and predictive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import random\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=int(1e6)):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        state = np.array(state, dtype=np.float32).flatten()\n",
    "        action = np.array(action, dtype=np.float32).flatten()\n",
    "        next_state = np.array(next_state, dtype=np.float32).flatten()\n",
    "        reward = np.float32(reward)\n",
    "        done = np.float32(done)\n",
    "\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = zip(*batch)\n",
    "        return (torch.tensor(np.stack(state), dtype=torch.float32),\n",
    "                torch.tensor(np.stack(action), dtype=torch.float32),\n",
    "                torch.tensor(reward, dtype=torch.float32).unsqueeze(1),\n",
    "                torch.tensor(np.stack(next_state), dtype=torch.float32),\n",
    "                torch.tensor(done, dtype=torch.float32).unsqueeze(1))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, state_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc_mu = nn.Linear(256, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256, latent_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        return mu+std*torch.randn_like(std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentTransitionModel(nn.Module):\n",
    "    def __init__(self, latent_dim, action_dim):\n",
    "        super(LatentTransitionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim+action_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, latent_dim)\n",
    "        \n",
    "    def forward(self, z, a):\n",
    "        x = torch.cat([z, a], dim=-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, latent_dim, action_dim):\n",
    "        super(RewardModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim+action_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "    def forward(self, z, a):\n",
    "        x = torch.cat([z, a], dim=-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, latent_dim, action_dim, max_action, hidden_dim=256):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.mean = nn.Linear(hidden_dim, action_dim)\n",
    "        self.log_std = nn.Linear(hidden_dim, action_dim)\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, latent):\n",
    "        x = F.relu(self.fc1(latent))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mean = self.mean(x)\n",
    "        log_std = self.log_std(x)\n",
    "        log_std = torch.clamp(log_std, -20, 2)\n",
    "        return mean, log_std\n",
    "    \n",
    "    def sample(self, latent):\n",
    "        mean, log_std = self.forward(latent)\n",
    "        std = torch.exp(log_std)\n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "        x = normal.rsample()\n",
    "        y = torch.tanh(x)\n",
    "        action = y * self.max_action\n",
    "        log_prob = normal.log_prob(x) - torch.log(1 - y.pow(2) + 1e-6)\n",
    "        log_prob = log_prob.sum(1, keepdim=True)\n",
    "        return action, log_prob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentMBPO:\n",
    "    def __init__(self, state_dim, action_dim, latent_dim, max_action):\n",
    "        self.encoder = Encoder(state_dim, latent_dim)\n",
    "        self.dynamics_model = LatentTransitionModel(latent_dim, action_dim)\n",
    "        self.reward_model = RewardModel(latent_dim, action_dim)\n",
    "        self.policy = Actor(state_dim, action_dim, max_action)\n",
    "        \n",
    "        self.encoder_optimizer = optim.Adam(self.encoder.parameters(), lr=3e-4)\n",
    "        self.dynamics_model_optimizer = optim.Adam(self.dynamics_model.parameters(), lr=3e-4)\n",
    "        self.reward_model_optimizer = optim.Adam(self.reward_model.parameters(), lr=3e-4)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        z = self.encoder(torch.tensor(state, dtype=torch.float32).unsqueeze(0))\n",
    "        action, _ = self.policy.sample(z)\n",
    "        return action.detach().cpu().numpy().squeeze(0)\n",
    "    \n",
    "    def train(self, real_buffer):\n",
    "        state, action, reward, next_state, done = real_buffer.sample(256)\n",
    "        \n",
    "        z = self.encoder(state)\n",
    "        z_next = self.encoder(next_state)\n",
    "\n",
    "        z_pred = self.dynamics_model(z, action)\n",
    "        r_pred = self.reward_model(z, action)\n",
    "\n",
    "        transition_loss = F.mse_loss(z_pred, z_next)\n",
    "        reward_loss = F.mse_loss(r_pred, reward)\n",
    "\n",
    "        # Update dynamics model\n",
    "        self.dynamics_model_optimizer.zero_grad()\n",
    "        transition_loss.backward(retain_graph=True)\n",
    "        self.dynamics_model_optimizer.step()\n",
    "\n",
    "        # Update reward model\n",
    "        self.reward_model_optimizer.zero_grad()\n",
    "        reward_loss.backward()\n",
    "        self.reward_model_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "env = gym.make('HalfCheetah-v5')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "agent = LatentMBPO(state_dim, action_dim, 17, max_action)\n",
    "real_buffer = ReplayBuffer()\n",
    "\n",
    "num_episodes = 500\n",
    "rewards = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0.0\n",
    "\n",
    "    for step in range(1000):\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "        real_buffer.add(state, action, reward, next_state, done)\n",
    "        state = next_state if not done else env.reset()\n",
    "\n",
    "        if len(real_buffer) > 10000:\n",
    "            agent.train(real_buffer)\n",
    "\n",
    "    rewards.append(total_reward)\n",
    "    print(f'Episode: {episode+1}, Reward: {total_reward}')\n",
    "\n",
    "plt.plot(rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('HalfCheetah-v5')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
