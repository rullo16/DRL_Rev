{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enables RL agents to quickly adapt to new tasks with minimal addtional experience. Traditional RL algorithms typically require extensive interactions with the environment to master each new task. In contrast, Meta-RL trains models to adapt rapidly by leveraging past experiences from multiple related tasks.\n",
    "Meta-RL agents adapt to new scenarios or variations of tasks rapidly, requires fewer environment interactions when learning new tasks and have better performence across a dicerse set of tasks or environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meta-RL introduces two levels of learning:\n",
    "- **Inner loop**: The agent interacts with the environment to learn a task. This is the standard RL learning process.\n",
    "- **Outer loop**: The agent learns how to learn new tasks quickly. This is the meta-learning process.\n",
    "\n",
    "Important concepts in Meta-RL:\n",
    "- **Task distribution**: The distribution of tasks that the agent will encounter during training and testing.\n",
    "- **Adaptation steps**: The number of gradient updates applied for a new task.\n",
    "- **Meta-policy**: The policy that generates the parameters of the inner-loop policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meta-RL involves training an agent on lupltiple tasks drawn from a task distrpibution $p(\\mathcal{T})$. We define a set of tasks $\\mathcal(T)_i$, each represented by an MDP $(S,A,P_i, r_i, \\gamma)$, where $P_i$ is the transition dynamics, $r_i$ is the reward function, and $\\gamma$ is the discount factor. The agent is trained to adapt to new tasks quickly by leveraging past experiences from related tasks.\n",
    "Meta-RL aims to learn policy paramteres $\\theta$ that can quickly adapt to new tasks $\\mathcal{T}_{new}$, the objective is then generalized to:\n",
    "$$\\theta^* = arg \\max_{\\theta} \\mathbb{E}_{\\mathcal{T}_i \\sim p(\\mathcal{T})} [V_{\\mathcal{T}_i}(\\theta')]$$\n",
    "where $\\theta'$ is obtained by adapting $\\theta$ on a small amount of task-specific data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAML(Model-Agnostic Meta-Learning) is a popular Meta-RL approach providing a clear mathematical structture:\n",
    "- Inner Loop adaptation (task-specific update):\n",
    "Given initial paramters $\\theta$, adapt using a small number of gradient steps K:\n",
    "$$\\theta' = \\theta - \\alpha \\nabla_{\\theta} \\mathcal{L}_{\\mathcal{T}_i}(\\theta)$$\n",
    "Here, $\\mathcal{L}_{\\mathcal{T}_i}(\\theta)$ is the task-specific loss, and $\\alpha$ is the step size.\n",
    "- Outer Loop optimization (meta-learning):\n",
    "Update original parameters $\\theta$ using the adapted parameters $\\theta'_i$:\n",
    "$$\\theta \\leftarrow \\theta - \\beta \\nabla_{\\theta} \\sum_{\\mathcal{T}_i} \\mathcal{L}_{\\mathcal{T}_i}(\\theta'_i)$$\n",
    "Where $\\beta$ is the meta-learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, act_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAMLAgent:\n",
    "    def __init__(self, obs_dim, act_dim, alpha=0.01, beta=0.001):\n",
    "        self.policy = Policy(obs_dim, act_dim).to(device)\n",
    "        self.alpha = alpha  # Inner loop learning rate\n",
    "        self.beta = beta    # Outer loop learning rate\n",
    "        self.meta_optimizer = optim.Adam(self.policy.parameters(), lr=self.beta)\n",
    "\n",
    "    def adapt(self, support_set):\n",
    "        adapted_policy = Policy(support_set['obs'].shape[1], support_set['act'].shape[1]).to(device)\n",
    "        adapted_policy.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        optimizer = optim.SGD(adapted_policy.parameters(), lr=self.alpha)\n",
    "\n",
    "        obs, acts = support_set['obs'], support_set['act']\n",
    "        preds = adapted_policy(obs)\n",
    "        loss = nn.MSELoss()(preds, acts)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        return adapted_policy\n",
    "\n",
    "    def meta_update(self, tasks):\n",
    "        meta_loss = 0.0\n",
    "        for task in tasks:\n",
    "            adapted_policy = self.adapt(task['support'])\n",
    "            query_obs, query_acts = task['query']['obs'], task['query']['act']\n",
    "            query_preds = adapted_policy(query_obs)\n",
    "            loss = nn.MSELoss()(query_preds, query_acts)\n",
    "            meta_loss += loss\n",
    "\n",
    "        meta_loss /= len(tasks)\n",
    "        self.meta_optimizer.zero_grad()\n",
    "        meta_loss.backward()\n",
    "        self.meta_optimizer.step()\n",
    "        return meta_loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Meta Loss 0.0000\n",
      "Epoch 1: Meta Loss 0.0000\n",
      "Epoch 2: Meta Loss 0.0000\n",
      "Epoch 3: Meta Loss 0.0000\n",
      "Epoch 4: Meta Loss 0.0000\n",
      "Epoch 5: Meta Loss 0.0000\n",
      "Epoch 6: Meta Loss 0.0000\n",
      "Epoch 7: Meta Loss 0.0000\n",
      "Epoch 8: Meta Loss 0.0000\n",
      "Epoch 9: Meta Loss 0.0000\n",
      "Epoch 10: Meta Loss 0.0000\n",
      "Epoch 11: Meta Loss 0.0000\n",
      "Epoch 12: Meta Loss 0.0000\n",
      "Epoch 13: Meta Loss 0.0000\n",
      "Epoch 14: Meta Loss 0.0000\n",
      "Epoch 15: Meta Loss 0.0000\n",
      "Epoch 16: Meta Loss 0.0000\n",
      "Epoch 17: Meta Loss 0.0000\n",
      "Epoch 18: Meta Loss 0.0000\n",
      "Epoch 19: Meta Loss 0.0000\n",
      "Epoch 20: Meta Loss 0.0000\n",
      "Epoch 21: Meta Loss 0.0000\n",
      "Epoch 22: Meta Loss 0.0000\n",
      "Epoch 23: Meta Loss 0.0000\n",
      "Epoch 24: Meta Loss 0.0000\n",
      "Epoch 25: Meta Loss 0.0000\n",
      "Epoch 26: Meta Loss 0.0000\n",
      "Epoch 27: Meta Loss 0.0000\n",
      "Epoch 28: Meta Loss 0.0000\n",
      "Epoch 29: Meta Loss 0.0000\n",
      "Epoch 30: Meta Loss 0.0000\n",
      "Epoch 31: Meta Loss 0.0000\n",
      "Epoch 32: Meta Loss 0.0000\n",
      "Epoch 33: Meta Loss 0.0000\n",
      "Epoch 34: Meta Loss 0.0000\n",
      "Epoch 35: Meta Loss 0.0000\n",
      "Epoch 36: Meta Loss 0.0000\n",
      "Epoch 37: Meta Loss 0.0000\n",
      "Epoch 38: Meta Loss 0.0000\n",
      "Epoch 39: Meta Loss 0.0000\n",
      "Epoch 40: Meta Loss 0.0000\n",
      "Epoch 41: Meta Loss 0.0000\n",
      "Epoch 42: Meta Loss 0.0000\n",
      "Epoch 43: Meta Loss 0.0000\n",
      "Epoch 44: Meta Loss 0.0000\n",
      "Epoch 45: Meta Loss 0.0000\n",
      "Epoch 46: Meta Loss 0.0000\n",
      "Epoch 47: Meta Loss 0.0000\n",
      "Epoch 48: Meta Loss 0.0000\n",
      "Epoch 49: Meta Loss 0.0000\n",
      "Epoch 50: Meta Loss 0.0000\n",
      "Epoch 51: Meta Loss 0.0000\n",
      "Epoch 52: Meta Loss 0.0000\n",
      "Epoch 53: Meta Loss 0.0000\n",
      "Epoch 54: Meta Loss 0.0000\n",
      "Epoch 55: Meta Loss 0.0000\n",
      "Epoch 56: Meta Loss 0.0000\n",
      "Epoch 57: Meta Loss 0.0000\n",
      "Epoch 58: Meta Loss 0.0000\n",
      "Epoch 59: Meta Loss 0.0000\n",
      "Epoch 60: Meta Loss 0.0000\n",
      "Epoch 61: Meta Loss 0.0000\n",
      "Epoch 62: Meta Loss 0.0000\n",
      "Epoch 63: Meta Loss 0.0000\n",
      "Epoch 64: Meta Loss 0.0000\n",
      "Epoch 65: Meta Loss 0.0000\n",
      "Epoch 66: Meta Loss 0.0000\n",
      "Epoch 67: Meta Loss 0.0000\n",
      "Epoch 68: Meta Loss 0.0000\n",
      "Epoch 69: Meta Loss 0.0000\n",
      "Epoch 70: Meta Loss 0.0000\n",
      "Epoch 71: Meta Loss 0.0000\n",
      "Epoch 72: Meta Loss 0.0000\n",
      "Epoch 73: Meta Loss 0.0000\n",
      "Epoch 74: Meta Loss 0.0000\n",
      "Epoch 75: Meta Loss 0.0000\n",
      "Epoch 76: Meta Loss 0.0000\n",
      "Epoch 77: Meta Loss 0.0000\n",
      "Epoch 78: Meta Loss 0.0000\n",
      "Epoch 79: Meta Loss 0.0000\n",
      "Epoch 80: Meta Loss 0.0000\n",
      "Epoch 81: Meta Loss 0.0000\n",
      "Epoch 82: Meta Loss 0.0000\n",
      "Epoch 83: Meta Loss 0.0000\n",
      "Epoch 84: Meta Loss 0.0000\n",
      "Epoch 85: Meta Loss 0.0000\n",
      "Epoch 86: Meta Loss 0.0000\n",
      "Epoch 87: Meta Loss 0.0000\n",
      "Epoch 88: Meta Loss 0.0000\n",
      "Epoch 89: Meta Loss 0.0000\n",
      "Epoch 90: Meta Loss 0.0000\n",
      "Epoch 91: Meta Loss 0.0000\n",
      "Epoch 92: Meta Loss 0.0000\n",
      "Epoch 93: Meta Loss 0.0000\n",
      "Epoch 94: Meta Loss 0.0000\n",
      "Epoch 95: Meta Loss 0.0000\n",
      "Epoch 96: Meta Loss 0.0000\n",
      "Epoch 97: Meta Loss 0.0000\n",
      "Epoch 98: Meta Loss 0.0000\n",
      "Epoch 99: Meta Loss 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Assume tasks are variations of Pendulum environment\n",
    "def sample_task():\n",
    "    env = gym.make(\"Pendulum-v1\")\n",
    "    return env\n",
    "\n",
    "def collect_data(env, policy, episodes=1):\n",
    "    obs_list, act_list = [], []\n",
    "    obs, _ = env.reset()\n",
    "    for _ in range(episodes * 200):\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        act = policy(obs_tensor).detach().cpu().numpy()[0]\n",
    "        next_obs, _, terminated, truncated, _ = env.step(act)\n",
    "        obs_list.append(obs)\n",
    "        act_list.append(act)\n",
    "        obs = next_obs\n",
    "        if terminated or truncated:\n",
    "            obs, _ = env.reset()\n",
    "    return {\n",
    "        'obs': torch.tensor(np.array(obs_list), dtype=torch.float32).to(device),\n",
    "        'act': torch.tensor(np.array(act_list), dtype=torch.float32).to(device)\n",
    "    }\n",
    "\n",
    "agent = MAMLAgent(obs_dim=3, act_dim=1)\n",
    "\n",
    "meta_epochs = 100\n",
    "tasks_per_epoch = 5\n",
    "\n",
    "for epoch in range(meta_epochs):\n",
    "    tasks = []\n",
    "    for _ in range(tasks_per_epoch):\n",
    "        env = sample_task()\n",
    "        support_set = collect_data(env, agent.policy, episodes=1)\n",
    "        query_set = collect_data(env, agent.policy, episodes=1)\n",
    "        tasks.append({'support': support_set, 'query': query_set})\n",
    "\n",
    "    meta_loss = agent.meta_update(tasks)\n",
    "    print(f\"Epoch {epoch}: Meta Loss {meta_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $RL^2$ (Fast RL via Slow RL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizes a RNN to perform meta-learning by encoding experiences in the hidden state of the policy. This allows rapid adaptation based on recent experiences without explicitly performing gradient updates during test time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentPolicy(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(obs_dim + act_dim + 1, hidden_size, batch_first=True)\n",
    "        self.actor = nn.Linear(hidden_size, act_dim)\n",
    "    \n",
    "    def forward(self, obs, prev_act, prev_reward, hidden_state):\n",
    "        x = torch.cat([obs, prev_act, prev_reward], dim=-1).unsqueeze(1)  # Shape: (batch, seq=1, features)\n",
    "        output, hidden_state = self.lstm(x, hidden_state)\n",
    "        action = torch.tanh(self.actor(output.squeeze(1)))\n",
    "        return action, hidden_state\n",
    "\n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return (torch.zeros(1, batch_size, 128).to(device),\n",
    "                torch.zeros(1, batch_size, 128).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, policy, optimizer=None, train=True):\n",
    "    obs, _ = env.reset()\n",
    "    hidden_state = policy.init_hidden()\n",
    "    prev_action = torch.zeros(1, env.action_space.shape[0]).to(device)\n",
    "    prev_reward = torch.zeros(1, 1).to(device)\n",
    "\n",
    "    total_reward = 0\n",
    "    log_probs = []  # To accumulate differentiable loss terms\n",
    "    rewards = []\n",
    "\n",
    "    for step in range(200):\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        action, hidden_state = policy(obs_tensor, prev_action, prev_reward, hidden_state)\n",
    "\n",
    "        # Assume continuous action; add exploration noise\n",
    "        action_dist = torch.distributions.Normal(action, 0.1)\n",
    "        sampled_action = action_dist.rsample()\n",
    "        log_prob = action_dist.log_prob(sampled_action).sum(dim=-1)\n",
    "        \n",
    "        next_obs, reward, terminated, truncated, _ = env.step(sampled_action.detach().cpu().numpy()[0])\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if train:\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float32).to(device))\n",
    "\n",
    "        prev_action = sampled_action.detach()\n",
    "        prev_reward = torch.tensor([[reward]], dtype=torch.float32).to(device)\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Compute loss at episode end (REINFORCE-like update)\n",
    "    if train and optimizer:\n",
    "        returns = []\n",
    "        R = 0\n",
    "        gamma = 0.99\n",
    "        for r in reversed(rewards):\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0, R)\n",
    "\n",
    "        returns = torch.cat(returns).detach()\n",
    "        log_probs = torch.stack(log_probs)\n",
    "\n",
    "        loss = -(log_probs * returns).mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Average Reward: -758.08\n",
      "Epoch 10, Average Reward: -774.72\n",
      "Epoch 20, Average Reward: -766.77\n",
      "Epoch 30, Average Reward: -776.45\n",
      "Epoch 40, Average Reward: -543.27\n",
      "Epoch 50, Average Reward: -672.09\n",
      "Epoch 60, Average Reward: -776.11\n",
      "Epoch 70, Average Reward: -912.63\n",
      "Epoch 80, Average Reward: -719.13\n",
      "Epoch 90, Average Reward: -933.36\n"
     ]
    }
   ],
   "source": [
    "env_names = [\"Pendulum-v1\", \"MountainCarContinuous-v0\"]\n",
    "\n",
    "env_dims = {\n",
    "    \"Pendulum-v1\": {\"obs_dim\": 3, \"act_dim\": 1},\n",
    "    \"MountainCarContinuous-v0\": {\"obs_dim\": 2, \"act_dim\": 1}\n",
    "}\n",
    "\n",
    "policies = {}\n",
    "optimizers = {}\n",
    "\n",
    "# Initialize policies for each environment\n",
    "for env_name in env_names:\n",
    "    dims = env_dims[env_name]\n",
    "    policy = RecurrentPolicy(obs_dim=dims[\"obs_dim\"], act_dim=dims[\"act_dim\"]).to(device)\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "    policies[env_name] = policy\n",
    "    optimizers[env_name] = optimizer\n",
    "\n",
    "meta_epochs = 100\n",
    "\n",
    "for epoch in range(meta_epochs):\n",
    "    rewards = []\n",
    "    for env_name in env_names:\n",
    "        env = gym.make(env_name)\n",
    "        policy = policies[env_name]\n",
    "        optimizer = optimizers[env_name]\n",
    "\n",
    "        ep_reward = run_episode(env, policy, optimizer=optimizer, train=True)\n",
    "        rewards.append(ep_reward)\n",
    "\n",
    "    avg_reward = np.mean(rewards)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Average Reward: {avg_reward:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Episode 0, Reward: -1089.51\n",
      "Test Episode 1, Reward: -1109.47\n",
      "Test Episode 2, Reward: -1065.80\n",
      "Test Episode 3, Reward: -1071.88\n",
      "Test Episode 4, Reward: -1898.60\n",
      "Test Episode 5, Reward: -1595.76\n",
      "Test Episode 6, Reward: -1276.89\n",
      "Test Episode 7, Reward: -1332.22\n",
      "Test Episode 8, Reward: -1661.24\n",
      "Test Episode 9, Reward: -1896.60\n",
      "Average Test Reward: -1399.80\n"
     ]
    }
   ],
   "source": [
    "env_test = gym.make(\"Pendulum-v1\")\n",
    "policy_test = policies[\"Pendulum-v1\"]  # <-- clearly select the correct policy for this environment\n",
    "test_rewards = []\n",
    "\n",
    "for episode in range(10):\n",
    "    ep_reward = run_episode(env_test, policy_test, train=False)\n",
    "    test_rewards.append(ep_reward)\n",
    "    print(f\"Test Episode {episode}, Reward: {ep_reward:.2f}\")\n",
    "\n",
    "print(f\"Average Test Reward: {np.mean(test_rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
