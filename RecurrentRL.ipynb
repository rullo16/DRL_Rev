{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7dc97bf",
   "metadata": {},
   "source": [
    "# Recurrent Reinforcement Learning (LSTM, GRU, GRU-RSSM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc2ac8f",
   "metadata": {},
   "source": [
    "Most RL algorithms assume the agent observes a complete Markov state $s_t$. But in reality:\n",
    "- Observations are partial or noisy\n",
    "- Current observations $o_t$ is not sufficient for optimal decisions\n",
    "The soulution is to use a recurrent model that can accumulate information over time to form a belief about the hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed04754d",
   "metadata": {},
   "source": [
    "## Partial Observable MArkov Decision Process (POMDP)\n",
    "A POMDP is a tuple $(S, A, O, T, R, \\gamma)$ where:\n",
    "- $S$: set of states\n",
    "- $A$: set of actions\n",
    "- $O$: set of observations\n",
    "- $T$: state transition function $T(s'|s,a)$\n",
    "- $R$: reward function $R(s,a)$\n",
    "- $\\gamma$: discount factor\n",
    "\n",
    "Because the agent can't directly observe $s_t$, it must infer a belief state $b_t$ using all past observations and actions:\n",
    "$$b_t = f(o_{1:t}, a_{1:t})$$\n",
    "This is where RNNs, LSTMs, and GRUs come in, they compress the sequence history into a hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7659be86",
   "metadata": {},
   "source": [
    "## Recurrent Policies: LSTM and GRU\n",
    "A recurrent policy in RL looks like this:\n",
    "$$h_T = RNN(h_{t-1}, o_t)$$\n",
    "$$a_t \\sim \\pi(a_t|h_t)$$\n",
    "$$V_t = V(h_t)$$\n",
    "Where:\n",
    "- $h_t$: hidden state of the RNN\n",
    "- $\\pi$: policy function\n",
    "- $V$: value function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e46d6a",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "LSTM introduces gated mechanisms to control information flow:\n",
    "- Input gate: Controls how much of the new input to keep\n",
    "- Forget gate: Controls how much of the previous state to keep\n",
    "- Output gate: Controls how much of the current state to output\n",
    "Formally, at time t:\n",
    "$$f_t = \\sigma(W_f[h_{t-1}, x_t] + b_f)$$\n",
    "$$i_t = \\sigma(W_i[h_{t-1}, x_t] + b_i)$$\n",
    "$$o_t = \\sigma(W_o[h_{t-1}, x_t] + b_o)$$\n",
    "$$\\tilde{c}_t = \\tanh(W_c[h_{t-1}, x_t] + b_c)$$\n",
    "$$c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t$$\n",
    "$$h_t = o_t \\odot \\tanh(c_t)$$\n",
    "Where $\\odot$ is the element-wise product, $W$ are weight matrices, and $b$ are biases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2f06ad",
   "metadata": {},
   "source": [
    "### GRU\n",
    "A lighter alternative to LSTM with fewer gates:\n",
    "$$z_t = \\sigma(W_zx_t + U_zh_{t-1})\\ (update\\ gate)$$\n",
    "$$r_t = \\sigma(W_rx_t + U_rh_{t-1})\\ (reset\\ gate)$$\n",
    "$$\\tilde{h}_t = \\tanh(W_hx_t + U(r_t \\odot h_{t-1}))$$\n",
    "$$h_t = (1-z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$$\n",
    "GRUs are computationally cheaper and often perform just as well as LSTMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c09645",
   "metadata": {},
   "source": [
    "## GRU-RSSM: Recurrent State Space Model\n",
    "The GRU-RSSM is a latent dynamics model combining:\n",
    "- Deterministic memory (GRU)\n",
    "- Stochastic latent states (variational)\n",
    "Used in PlaNet and Dreamer, it models:\n",
    "$$h_t = GRU(h_{t-1}, a_{t-1}, s_{t-1})$$\n",
    "$$s_t \\sim p(s_t|h_t)\\ (prior)$$\n",
    "$$s_t \\sim q(s_t|h_t, e(o_t))\\ (posterior)$$\n",
    "Then use $(s_t, h_t)$ to:\n",
    "- Predict future latent states\n",
    "- Decode rewards, observations\n",
    "- Imagine rollouts for planning or policy learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15119120",
   "metadata": {},
   "source": [
    "## RL Algorithms + Recurrent Policies\n",
    "Recurrent policies can be trained using:\n",
    "- **REINFORCE** (policy gradient with history)\n",
    "- **A2C/PPO** with recurrent rollouts\n",
    "- **SAC** (with RNN or GRU-based actor/critic)\n",
    "- **Dreamer** (learned latent rollouts from GRU-RSSM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e41938",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de446c44",
   "metadata": {},
   "source": [
    "### LSTM-based Policy with REINFORCE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b2dfb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b898e5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Loss: -0.2010430097579956, Return: 31.0\n",
      "Episode 50, Loss: 0.03944242000579834, Return: 18.0\n",
      "Episode 100, Loss: -0.5274049043655396, Return: 108.0\n",
      "Episode 150, Loss: 2.6946613788604736, Return: 39.0\n",
      "Episode 200, Loss: -0.16298528015613556, Return: 87.0\n",
      "Episode 250, Loss: -8.582036972045898, Return: 500.0\n"
     ]
    }
   ],
   "source": [
    "class LSTMPolicy(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(obs_dim, hidden_dim, batch_first=True)\n",
    "        self.actor = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        out, hidden = self.lstm(x,hidden)\n",
    "        logits = self.actor(out)\n",
    "        return logits, hidden\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "policy = LSTMPolicy(obs_dim, action_dim)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "\n",
    "def train_lstm_reinforce(episode=300):\n",
    "    gamma = 0.99\n",
    "    for ep in range(episode):\n",
    "        obs, _ = env.reset()\n",
    "        hidden = (torch.zeros(1,1,64), torch.zeros(1,1,64))\n",
    "        rewards, log_probs = [], []\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "            logits, hidden = policy(obs_tensor, hidden)\n",
    "            probs = torch.softmax(logits[0,0], dim=-1)\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "            obs, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            done = terminated or truncated\n",
    "            rewards.append(reward)\n",
    "            log_probs.append(log_prob)\n",
    "\n",
    "        R = 0\n",
    "        returns = []\n",
    "        for r in reversed(rewards):\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0, R)\n",
    "\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-6)\n",
    "\n",
    "        loss = -sum(lp * R for lp, R in zip(log_probs, returns))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if ep % 50 == 0:\n",
    "            print(f\"Episode {ep}, Loss: {loss.item()}, Return: {sum(rewards)}\")\n",
    "    env.close()\n",
    "\n",
    "train_lstm_reinforce()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744ce6ca",
   "metadata": {},
   "source": [
    "### GRU-RSSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7ff8b767",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRURSSM(nn.Module):\n",
    "    def __init__(self, action_dim, latent_dim=30, hidden_dim=200, obs_embed_dim=1024):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.gru = nn.GRUCell(latent_dim + action_dim, hidden_dim)\n",
    "\n",
    "        self.prior = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 2*latent_dim)\n",
    "        )\n",
    "\n",
    "        self.posterior = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + obs_embed_dim, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 2*latent_dim)\n",
    "        )\n",
    "\n",
    "    def init_state(self, batch_size):\n",
    "        h = torch.zeros(batch_size, self.hidden_dim)\n",
    "        s = torch.zeros(batch_size, self.latent_dim)\n",
    "        return h, s\n",
    "    def get_dist(self, stats):\n",
    "        mean, log_std = torch.chunk(stats, 2, dim=-1)\n",
    "        std = F.softplus(log_std)\n",
    "        return torch.distributions.Normal(mean, std)\n",
    "    \n",
    "    def forward(self, h_prev, s_prev, a_prev, obs_embed=None):\n",
    "        x = torch.cat([s_prev, a_prev], dim=-1)\n",
    "        h = self.gru(x, h_prev)\n",
    "\n",
    "        prior_stats = self.prior(h)\n",
    "        prior_dist = self.get_dist(prior_stats)\n",
    "\n",
    "        if obs_embed is not None:\n",
    "            post_input = torch.cat([h, obs_embed], dim=-1)\n",
    "            post_stats = self.posterior(post_input)\n",
    "            post_dist = self.get_dist(post_stats)\n",
    "            s = post_dist.rsample()\n",
    "        else:\n",
    "            post_stats = prior_stats\n",
    "            post_dist = prior_dist\n",
    "            s = prior_dist.rsample()\n",
    "        return (h,s), prior_dist, post_dist, prior_stats, post_stats\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "779aa40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObsEncoder(nn.Module):\n",
    "    def __init__(self, obs_shape=(3,), embed_dim=1024):\n",
    "        super().__init__()\n",
    "        input_dim = np.prod(obs_shape)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, embed_dim), nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.encoder(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "954af8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObsDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim=30, hidden_dim=200, obs_shape=(3,)):\n",
    "        super().__init__()\n",
    "        output_dim = np.prod(obs_shape)\n",
    "        self.obs_shape = obs_shape\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim + hidden_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, s, h):\n",
    "        x = torch.cat([s, h], dim=-1)\n",
    "        return self.net(x).view(-1, *self.obs_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "34a619d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim=30, hidden_dim=200):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim + hidden_dim, 200), nn.ReLU(),\n",
    "            nn.Linear(200, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, s, h):\n",
    "        x = torch.cat([s, h], dim=-1)\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d023c7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DreamerActor(nn.Module):\n",
    "    def __init__(self, latent_dim=30, hidden_dim=200, action_dim=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim + hidden_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 256), nn.ReLU(),\n",
    "        )\n",
    "        self.mean = nn.Linear(256, action_dim)\n",
    "        self.log_std = nn.Linear(256, action_dim)\n",
    "\n",
    "    def forward(self, s, h):\n",
    "        x = torch.cat([s, h], dim=-1)\n",
    "        x = self.net(x)\n",
    "        mean = self.mean(x)\n",
    "        log_std = torch.clamp(self.log_std(x), -5, 2)\n",
    "        std = log_std.exp()\n",
    "        dist = torch.distributions.Normal(mean, std)\n",
    "        return dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "db0b59cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DreamerCritic(nn.Module):\n",
    "    def __init__(self, latent_dim=30, hidden_dim=200):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim + hidden_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, s, h):\n",
    "        x = torch.cat([s, h], dim=-1)\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "480d3161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imagine_rollout(h, s, actor, rssm, horizon=15):\n",
    "    s_list, h_list, a_list, logp_list = [], [], [], []\n",
    "    for _ in range(horizon):\n",
    "        dist = actor(s, h)\n",
    "        a = torch.tanh(dist.rsample())\n",
    "        logp = dist.log_prob(a).sum(-1, keepdim=True) - torch.log(1 - a.pow(2) + 1e-6).sum(-1, keepdim=True)\n",
    "\n",
    "        (h, s), *_ = rssm(h, s, a, obs_embed=None)\n",
    "        s_list.append(s)\n",
    "        h_list.append(h)\n",
    "        a_list.append(a)\n",
    "        logp_list.append(logp)\n",
    "\n",
    "    return torch.stack(s_list), torch.stack(h_list), torch.stack(a_list), torch.stack(logp_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "559e2267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_elbo_loss(obs_seq, act_seq, rew_seq, encoder, decoder_obs, decoder_r, rssm, beta=1.0):\n",
    "    B, T, *_ = obs_seq.shape\n",
    "    h, s = rssm.init_state(B)\n",
    "\n",
    "    loss_obs, loss_r, loss_kl = 0.0, 0.0, 0.0\n",
    "    for t in range(T):\n",
    "        o_t = obs_seq[:, t]\n",
    "        a_t = act_seq[:, t]\n",
    "        r_t = rew_seq[:, t]\n",
    "\n",
    "        emb = encoder(o_t)\n",
    "        (h, s), prior, post, _, _ = rssm(h, s, a_t, emb)\n",
    "\n",
    "        o_hat = decoder_obs(s, h)\n",
    "        r_hat = decoder_r(s, h)\n",
    "\n",
    "        loss_obs += F.mse_loss(o_hat, o_t, reduction=\"mean\")\n",
    "        loss_r += F.mse_loss(r_hat, r_t, reduction=\"mean\")\n",
    "        loss_kl += torch.distributions.kl.kl_divergence(post, prior).mean()\n",
    "\n",
    "    total = loss_obs + loss_r + beta * loss_kl\n",
    "    return total, loss_obs, loss_r, loss_kl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "77843277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_loss(critic, s_rollout, h_rollout, logp_rollout, entropy_weight=1.0):\n",
    "    values = critic(s_rollout, h_rollout).squeeze(-1)\n",
    "    logp = logp_rollout.squeeze(-1)\n",
    "    return -(values + entropy_weight * logp).mean()\n",
    "\n",
    "def value_loss(critic, s_rollout, h_rollout, rewards, gamma=0.99):\n",
    "    target = []\n",
    "    G = torch.zeros_like(rewards[0])\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        target.insert(0, G.clone())\n",
    "    target = torch.stack(target)\n",
    "    values = critic(s_rollout, h_rollout).squeeze(-1)\n",
    "    return F.mse_loss(values, target.detach())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e6077751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dreamer_train_step(batch, encoder, decoder_obs, decoder_r, rssm, actor, critic, \n",
    "                       optimizer_world, optimizer_actor, optimizer_critic):\n",
    "    obs_seq, act_seq, rew_seq = batch  # (B, T, ...)\n",
    "    \n",
    "    # === World Model Update ===\n",
    "    elbo_loss, _, _, _ = compute_elbo_loss(obs_seq, act_seq, rew_seq,\n",
    "                                           encoder, decoder_obs, decoder_r, rssm)\n",
    "    optimizer_world.zero_grad()\n",
    "    elbo_loss.backward()\n",
    "    optimizer_world.step()\n",
    "\n",
    "    # === Imagination ===\n",
    "    o_t = obs_seq[:, -1]\n",
    "    a_t = act_seq[:, -1]\n",
    "    emb = encoder(o_t)\n",
    "    h, s = rssm.init_state(obs_seq.size(0))\n",
    "    (h, s), *_ = rssm(h, s, a_t, emb)\n",
    "\n",
    "    s_imag, h_imag, a_imag, logp = imagine_rollout(h, s, actor, rssm, horizon=15)\n",
    "    rewards = decoder_r(s_imag, h_imag).squeeze(-1)\n",
    "\n",
    "    # === Value Function Update ===\n",
    "    loss_v = value_loss(critic, s_imag.detach(), h_imag.detach(), rewards.detach())\n",
    "    optimizer_critic.zero_grad()\n",
    "    loss_v.backward()\n",
    "    optimizer_critic.step()\n",
    "\n",
    "    # === Policy Update ===\n",
    "    loss_a = actor_loss(critic, s_imag, h_imag, logp)\n",
    "    optimizer_actor.zero_grad()\n",
    "    loss_a.backward()\n",
    "    optimizer_actor.step()\n",
    "\n",
    "    return elbo_loss.item(), loss_a.item(), loss_v.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1f327417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sample_batch(replay, batch_size, seq_len):\n",
    "    idxs = np.random.randint(0, len(replay) - seq_len, size=batch_size)\n",
    "    obs_batch = []\n",
    "    act_batch = []\n",
    "    rew_batch = []\n",
    "\n",
    "    for i in idxs:\n",
    "        seq = replay[i:i+seq_len]\n",
    "        obs_seq = torch.tensor([s[0] for s in seq], dtype=torch.float32)\n",
    "        act_seq = torch.tensor([s[1] for s in seq], dtype=torch.float32)\n",
    "        rew_seq = torch.tensor([[s[2]] for s in seq], dtype=torch.float32)\n",
    "        obs_batch.append(obs_seq)\n",
    "        act_batch.append(act_seq)\n",
    "        rew_batch.append(rew_seq)\n",
    "\n",
    "    return (torch.stack(obs_batch),\n",
    "            torch.stack(act_batch),\n",
    "            torch.stack(rew_batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4c5d4a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dreamer_agent(env, num_epochs, steps_per_epoch, batch_size,\n",
    "                        encoder, decoder_obs, decoder_r,\n",
    "                        rssm, actor, critic):\n",
    "\n",
    "    # Initialize replay buffer\n",
    "    replay = []  # list of (obs, act, rew)\n",
    "    max_buffer_size = 100000\n",
    "\n",
    "    # Initialize optimizers\n",
    "    optim_world = torch.optim.Adam(\n",
    "        list(encoder.parameters()) +\n",
    "        list(decoder_obs.parameters()) +\n",
    "        list(decoder_r.parameters()) +\n",
    "        list(rssm.parameters()), lr=1e-3)\n",
    "\n",
    "    optim_actor = torch.optim.Adam(actor.parameters(), lr=8e-5)\n",
    "    optim_critic = torch.optim.Adam(critic.parameters(), lr=8e-5)\n",
    "\n",
    "    obs = env.reset()[0]\n",
    "    done = False\n",
    "    total_steps = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for step in range(steps_per_epoch):\n",
    "            h, s = rssm.init_state(1)\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "            emb = encoder(obs_tensor)\n",
    "            a = torch.zeros(1, action_dim)  # dummy initial action\n",
    "            (h, s), prior, post, _, _ = rssm(h, s, a, emb)\n",
    "\n",
    "            dist = actor(s, h)\n",
    "            action = torch.tanh(dist.sample()).squeeze(0).cpu().numpy()\n",
    "\n",
    "            next_obs, reward, done, _, _ = env.step(action)\n",
    "            replay.append((obs, action, reward))\n",
    "            if len(replay) > max_buffer_size:\n",
    "                replay.pop(0)\n",
    "\n",
    "            obs = next_obs\n",
    "            if done:\n",
    "                obs = env.reset()[0]\n",
    "\n",
    "            total_steps += 1\n",
    "\n",
    "        # Sample batches from replay\n",
    "        obs_seq, act_seq, rew_seq = sample_batch(replay, batch_size=batch_size, seq_len=15)\n",
    "\n",
    "        elbo, loss_a, loss_v = dreamer_train_step(\n",
    "            (obs_seq, act_seq, rew_seq),\n",
    "            encoder, decoder_obs, decoder_r,\n",
    "            rssm, actor, critic,\n",
    "            optim_world, optim_actor, optim_critic\n",
    "        )\n",
    "\n",
    "        print(f\"[Epoch {epoch}] ELBO: {elbo:.2f} | Actor: {loss_a:.2f} | Critic: {loss_v:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "910edcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] ELBO: 902.70 | Actor: 0.25 | Critic: 1.64\n",
      "[Epoch 1] ELBO: 639.60 | Actor: 0.39 | Critic: 2.77\n",
      "[Epoch 2] ELBO: 516.12 | Actor: 0.40 | Critic: 5.46\n",
      "[Epoch 3] ELBO: 425.52 | Actor: 0.34 | Critic: 8.47\n",
      "[Epoch 4] ELBO: 589.68 | Actor: 0.35 | Critic: 10.92\n",
      "[Epoch 5] ELBO: 457.91 | Actor: 0.36 | Critic: 20.20\n",
      "[Epoch 6] ELBO: 366.66 | Actor: 0.31 | Critic: 39.98\n",
      "[Epoch 7] ELBO: 267.97 | Actor: 0.42 | Critic: 51.31\n",
      "[Epoch 8] ELBO: 368.64 | Actor: 0.43 | Critic: 56.83\n",
      "[Epoch 9] ELBO: 255.15 | Actor: 0.38 | Critic: 46.16\n",
      "[Epoch 10] ELBO: 239.39 | Actor: 0.31 | Critic: 42.86\n",
      "[Epoch 11] ELBO: 286.59 | Actor: 0.38 | Critic: 50.22\n",
      "[Epoch 12] ELBO: 215.89 | Actor: 0.36 | Critic: 57.63\n",
      "[Epoch 13] ELBO: 207.47 | Actor: 0.38 | Critic: 57.44\n",
      "[Epoch 14] ELBO: 185.93 | Actor: 0.46 | Critic: 81.13\n",
      "[Epoch 15] ELBO: 179.80 | Actor: 0.39 | Critic: 115.11\n",
      "[Epoch 16] ELBO: 155.46 | Actor: 0.53 | Critic: 186.59\n",
      "[Epoch 17] ELBO: 120.53 | Actor: 0.45 | Critic: 309.65\n",
      "[Epoch 18] ELBO: 112.69 | Actor: 0.43 | Critic: 414.12\n",
      "[Epoch 19] ELBO: 120.53 | Actor: 0.45 | Critic: 672.93\n",
      "[Epoch 20] ELBO: 102.59 | Actor: 0.37 | Critic: 845.15\n",
      "[Epoch 21] ELBO: 78.93 | Actor: 0.46 | Critic: 819.65\n",
      "[Epoch 22] ELBO: 53.36 | Actor: 0.41 | Critic: 948.51\n",
      "[Epoch 23] ELBO: 71.87 | Actor: 0.09 | Critic: 976.58\n",
      "[Epoch 24] ELBO: 94.21 | Actor: 0.25 | Critic: 1549.29\n",
      "[Epoch 25] ELBO: 61.52 | Actor: 0.00 | Critic: 1784.04\n",
      "[Epoch 26] ELBO: 96.78 | Actor: -0.27 | Critic: 2111.01\n",
      "[Epoch 27] ELBO: 51.72 | Actor: 0.10 | Critic: 2727.70\n",
      "[Epoch 28] ELBO: 59.08 | Actor: -0.52 | Critic: 3610.96\n",
      "[Epoch 29] ELBO: 69.19 | Actor: -0.60 | Critic: 3593.46\n",
      "[Epoch 30] ELBO: 58.26 | Actor: -0.92 | Critic: 3857.11\n",
      "[Epoch 31] ELBO: 68.21 | Actor: -0.91 | Critic: 3627.33\n",
      "[Epoch 32] ELBO: 54.52 | Actor: -1.05 | Critic: 4118.99\n",
      "[Epoch 33] ELBO: 59.84 | Actor: -1.62 | Critic: 4056.55\n",
      "[Epoch 34] ELBO: 50.28 | Actor: -0.87 | Critic: 2948.59\n",
      "[Epoch 35] ELBO: 56.37 | Actor: -0.86 | Critic: 2824.21\n",
      "[Epoch 36] ELBO: 50.15 | Actor: -1.60 | Critic: 3263.78\n",
      "[Epoch 37] ELBO: 47.01 | Actor: -1.19 | Critic: 2945.33\n",
      "[Epoch 38] ELBO: 36.80 | Actor: -1.03 | Critic: 3050.15\n",
      "[Epoch 39] ELBO: 35.16 | Actor: -1.80 | Critic: 2824.42\n",
      "[Epoch 40] ELBO: 33.29 | Actor: -1.57 | Critic: 2853.70\n",
      "[Epoch 41] ELBO: 38.87 | Actor: -1.22 | Critic: 2384.21\n",
      "[Epoch 42] ELBO: 31.01 | Actor: -1.56 | Critic: 2149.46\n",
      "[Epoch 43] ELBO: 28.78 | Actor: -1.41 | Critic: 1971.60\n",
      "[Epoch 44] ELBO: 31.28 | Actor: -1.46 | Critic: 1845.81\n",
      "[Epoch 45] ELBO: 35.74 | Actor: -1.67 | Critic: 1432.68\n",
      "[Epoch 46] ELBO: 32.48 | Actor: -1.44 | Critic: 1467.55\n",
      "[Epoch 47] ELBO: 29.93 | Actor: -1.19 | Critic: 1496.59\n",
      "[Epoch 48] ELBO: 27.86 | Actor: -0.98 | Critic: 1141.60\n",
      "[Epoch 49] ELBO: 26.29 | Actor: -1.79 | Critic: 1462.62\n",
      "[Epoch 50] ELBO: 25.97 | Actor: -1.06 | Critic: 1323.93\n",
      "[Epoch 51] ELBO: 22.99 | Actor: -1.96 | Critic: 1403.49\n",
      "[Epoch 52] ELBO: 22.75 | Actor: -2.14 | Critic: 1410.42\n",
      "[Epoch 53] ELBO: 22.92 | Actor: -1.77 | Critic: 1301.20\n",
      "[Epoch 54] ELBO: 20.95 | Actor: -1.55 | Critic: 1313.37\n",
      "[Epoch 55] ELBO: 17.80 | Actor: -1.52 | Critic: 1642.69\n",
      "[Epoch 56] ELBO: 21.71 | Actor: -2.07 | Critic: 1677.30\n",
      "[Epoch 57] ELBO: 20.84 | Actor: -1.43 | Critic: 1815.83\n",
      "[Epoch 58] ELBO: 20.56 | Actor: -1.55 | Critic: 1954.63\n",
      "[Epoch 59] ELBO: 21.71 | Actor: -1.90 | Critic: 1920.38\n",
      "[Epoch 60] ELBO: 18.01 | Actor: -1.42 | Critic: 2036.88\n",
      "[Epoch 61] ELBO: 24.14 | Actor: -1.47 | Critic: 2305.17\n",
      "[Epoch 62] ELBO: 18.78 | Actor: -0.96 | Critic: 2481.96\n",
      "[Epoch 63] ELBO: 20.28 | Actor: -0.85 | Critic: 2824.59\n",
      "[Epoch 64] ELBO: 20.33 | Actor: -0.72 | Critic: 2890.36\n",
      "[Epoch 65] ELBO: 18.75 | Actor: -0.98 | Critic: 2549.16\n",
      "[Epoch 66] ELBO: 19.26 | Actor: -0.23 | Critic: 2476.09\n",
      "[Epoch 67] ELBO: 20.19 | Actor: 0.16 | Critic: 2628.09\n",
      "[Epoch 68] ELBO: 20.34 | Actor: 0.08 | Critic: 2561.15\n",
      "[Epoch 69] ELBO: 18.75 | Actor: 0.05 | Critic: 2367.55\n",
      "[Epoch 70] ELBO: 20.41 | Actor: 0.97 | Critic: 2601.11\n",
      "[Epoch 71] ELBO: 20.58 | Actor: 0.67 | Critic: 2719.07\n",
      "[Epoch 72] ELBO: 15.64 | Actor: 1.24 | Critic: 2329.47\n",
      "[Epoch 73] ELBO: 23.28 | Actor: 1.82 | Critic: 3142.26\n",
      "[Epoch 74] ELBO: 25.56 | Actor: 2.11 | Critic: 2997.07\n",
      "[Epoch 75] ELBO: 16.53 | Actor: 3.09 | Critic: 3297.48\n",
      "[Epoch 76] ELBO: 15.33 | Actor: 3.20 | Critic: 3283.34\n",
      "[Epoch 77] ELBO: 27.73 | Actor: 1.95 | Critic: 2472.29\n",
      "[Epoch 78] ELBO: 16.05 | Actor: 2.75 | Critic: 2124.02\n",
      "[Epoch 79] ELBO: 14.08 | Actor: 2.54 | Critic: 1900.39\n",
      "[Epoch 80] ELBO: 14.61 | Actor: 2.61 | Critic: 1741.15\n",
      "[Epoch 81] ELBO: 17.93 | Actor: 3.41 | Critic: 2088.16\n",
      "[Epoch 82] ELBO: 16.65 | Actor: 4.06 | Critic: 2353.29\n",
      "[Epoch 83] ELBO: 13.30 | Actor: 4.04 | Critic: 2525.12\n",
      "[Epoch 84] ELBO: 15.66 | Actor: 4.91 | Critic: 2757.72\n",
      "[Epoch 85] ELBO: 16.46 | Actor: 6.07 | Critic: 3023.88\n",
      "[Epoch 86] ELBO: 14.36 | Actor: 4.97 | Critic: 2080.53\n",
      "[Epoch 87] ELBO: 13.88 | Actor: 5.49 | Critic: 1851.59\n",
      "[Epoch 88] ELBO: 16.36 | Actor: 5.73 | Critic: 1639.77\n",
      "[Epoch 89] ELBO: 18.82 | Actor: 5.98 | Critic: 1704.60\n",
      "[Epoch 90] ELBO: 11.49 | Actor: 5.71 | Critic: 1727.71\n",
      "[Epoch 91] ELBO: 14.55 | Actor: 6.77 | Critic: 2275.13\n",
      "[Epoch 92] ELBO: 16.29 | Actor: 9.01 | Critic: 3117.82\n",
      "[Epoch 93] ELBO: 11.87 | Actor: 10.06 | Critic: 3089.01\n",
      "[Epoch 94] ELBO: 11.47 | Actor: 9.64 | Critic: 2975.67\n",
      "[Epoch 95] ELBO: 14.40 | Actor: 9.55 | Critic: 2335.15\n",
      "[Epoch 96] ELBO: 11.86 | Actor: 8.33 | Critic: 1515.90\n",
      "[Epoch 97] ELBO: 11.55 | Actor: 8.51 | Critic: 1389.54\n",
      "[Epoch 98] ELBO: 11.94 | Actor: 8.78 | Critic: 1410.18\n",
      "[Epoch 99] ELBO: 12.28 | Actor: 9.80 | Critic: 1333.00\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pendulum-v1\")\n",
    "obs_shape = env.observation_space.shape\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "encoder = ObsEncoder(obs_shape=obs_shape)\n",
    "decoder_obs = ObsDecoder(obs_shape=obs_shape)\n",
    "decoder_r = RewardDecoder()\n",
    "rssm = GRURSSM(action_dim=action_dim)\n",
    "actor = DreamerActor(action_dim=action_dim)\n",
    "critic = DreamerCritic()\n",
    "\n",
    "train_dreamer_agent(\n",
    "    env=env,\n",
    "    num_epochs=100,\n",
    "    steps_per_epoch=500,\n",
    "    batch_size=16,\n",
    "    encoder=encoder,\n",
    "    decoder_obs=decoder_obs,\n",
    "    decoder_r=decoder_r,\n",
    "    rssm=rssm,\n",
    "    actor=actor,\n",
    "    critic=critic\n",
    ")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cfefb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "procgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
