{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8776b9a7",
   "metadata": {},
   "source": [
    "# PlaNet (Planning LAtent Dynamics for RL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d5b41e",
   "metadata": {},
   "source": [
    "Latent Dynamics model for pixel-based continuous control, which:\n",
    "- Learns from high-dimensional observations(images)\n",
    "- Encodes them into a latent state space\n",
    "- Predicts latent dynamics using a Recurrent State-Space Model (RSSM)\n",
    "- Plans sequences of actions using Cross-Entropy Method (CEM) in the latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9c0bc2",
   "metadata": {},
   "source": [
    "## Background_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a3bdf6",
   "metadata": {},
   "source": [
    "### World Model\n",
    "Learns to predict future observations and rewards given past observations and actions.\\\n",
    "Consists of three components:\n",
    "- Encoder $e(o_t)$: that maps observations to a latent state\n",
    "- RSSM that models latent transitions\n",
    "- Decoder $d_o(s_t, h_t), d_r(s_t, h_t)$: that reconstruct observations/rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a554fc",
   "metadata": {},
   "source": [
    "## RSSM (Recurrent State-Space Model)\n",
    "Combines deterministic and stochastic transitions:\n",
    "- $h_t$: deterministic hidden state (RNN, GRU)\n",
    "- $s_t$: stochastic latent variable\n",
    "This allows both flexibility and memory/temporal abstraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d590672",
   "metadata": {},
   "source": [
    "### Variational Inference\n",
    "Since the true posterior $p(s_t|o_{1:t}, a_{1:t})$ is intractable, we approximate it with a learned encoder $q(s_t|h_t, o_t)$.\n",
    "### Planning\n",
    "Once the model is trained, actions are selected using CEM in the latent space, making planning tractable and data efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed02ddde",
   "metadata": {},
   "source": [
    "## Math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887f7184",
   "metadata": {},
   "source": [
    "### Notation\n",
    "- $o_t\\in\\mathbb{R}^{H\\times W\\times C}$: observation (image)\n",
    "- $a_t\\in\\mathbb{R}^{n}$: action\n",
    "- $r_t\\in\\mathbb{R}$: reward\n",
    "- $h_t\\in\\mathbb{R}^{d}$: hidden state\n",
    "- $s_t\\in\\mathbb{R}^{z}$: stochastic latent state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61476c5",
   "metadata": {},
   "source": [
    "### Generative Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e16148",
   "metadata": {},
   "source": [
    "The full model defines:\n",
    "$$p(o_{1:T}, r_{1:T}, s_{1:T}, h_{1:T}|a_{1:T}) = \\prod_{t=1}^T p(o_t|s_t, h_t) \\cdot p(r_t|s_t, h_t) \\cdot p(s_t|h_t) \\cdot p(h_t|h_{t-1}, s_{t-1}, a_{t-1})$$\n",
    "Where:\n",
    "- Observation decoder: $p(o_t|s_t, h_t)$ - reconstructs image\n",
    "- Reward Model: $p(r_t|s_t, h_t)$ - predicts scalar reward\n",
    "- Stochastic transition: $p(s_t|h_t)$ - samples latent state from prior\n",
    "- Deterministic transition: $h_t = f(h_{t-1}, s_{t-1}, a_{t-1})$ - GRU/RNN update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a513d0",
   "metadata": {},
   "source": [
    "### Inference Model\n",
    "Since posterior is intractable, we use:\n",
    "$$q(s_t|h_t, o_t) = \\mathcal{N}(\\mu_t, \\sigma_t^2)$$\n",
    "Where $\\mu_t, \\sigma_t$ are ouptuts of the encoder network that take $h_t$ and $o_t$ as input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2fc5cc",
   "metadata": {},
   "source": [
    "### Learning Objective: ELBO\n",
    "The training loss is the Evidence Lower Bound (ELBO) over a sequence:\n",
    "$$\\mathcal{L}_{ELBO} = \\sum_{t=1}^T \\mathbb{E}_{q(s_t)}[\\log p(o_t|s_t, h_t) + \\log p(r_t|s_t, h_t)] - D_{KL}[q(s_t|h_t, o_t)||p(s_t|h_t)]$$\n",
    "Intuition:\n",
    "Encourage the latent state to reconstruct the observation and reward and penalize the divergence between the inferred posterior and the prior dynamics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac66ef38",
   "metadata": {},
   "source": [
    "## Planning in Latent Space using CEM\n",
    "Once the RSSM is trained, we can simulate the future and plan actions without real interactions.\n",
    "**Procedure**:\\\n",
    "1. From the current $(h_t, s_t)$, sample a batch of action sequences:\n",
    "$$a_{t:t+H} \\sim \\mathcal{N}(\\mu, \\sigma)$$\n",
    "2. Roll out the latent model for each sequence to get predicted rewards\n",
    "3. Select top K sequences with highest rewards\n",
    "4. Refit $\\mu, \\sigma$ to top samples\n",
    "5. Repeat for N iterations\n",
    "This yields an action plan maximizing predicted rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6c86d2",
   "metadata": {},
   "source": [
    "### CEM Math\n",
    "Sample N action sequences of length H:\n",
    "$$A^i = (a^i_t, a^i_{t+1}, \\ldots, a^i_{t+H})$$\n",
    "Exptected reward:\n",
    "$$R^i = \\sum_{j=0}^H \\hat{r}^i_{t+j}$$\n",
    "Select top K based on $R^i$\\\n",
    "Update mean and std:\n",
    "$$\\mu_{new} = \\frac{1}{K} \\sum_{i\\in topK} A^i, \\quad \\sigma_{new} = std(A^i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b62b5df",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "622af7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8d91255",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSSM(nn.Module):\n",
    "    def __init__(self, action_dim, latent_dim=30, hidden_dim=200, obs_embed_dim=1024):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.gru = nn.GRUCell(latent_dim + action_dim, latent_dim)\n",
    "\n",
    "        self.fc_prior = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LineaR(hidden_dim, 2*latent_dim)\n",
    "        )\n",
    "\n",
    "        self.fc_posterior = nn.Sequential(\n",
    "            nn.Linear(latent_dim + obs_embed_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 2*latent_dim)\n",
    "        )\n",
    "\n",
    "    def init_state(self, batch_size):\n",
    "        h = torch.zeros(batch_size, self.hidden_dim)\n",
    "        s = torch.zeros(batch_size, self.latent_dim)\n",
    "        return h, s\n",
    "    \n",
    "    def get_dist(self,stats):\n",
    "        mean, std = torch.chunk(stats, 2, dim=-1)\n",
    "        std = F.softplus(std)+ 1e-4\n",
    "        return torch.distributions.Normal(mean, std)\n",
    "    \n",
    "    def forward(self, prev_state, action, embed_obs=None):\n",
    "        h_prev, s_prev = prev_state\n",
    "        x = torch.cat([s_prev, action], dim=-1)\n",
    "        h = self.gru(x, h_prev)\n",
    "\n",
    "        prior_stats = self.fc_prior(h)\n",
    "        prior_dist = self.get_dist(prior_stats)\n",
    "\n",
    "        if embed_obs is not None:\n",
    "            x_post = torch.cat([h, embed_obs], dim=-1)\n",
    "            post_stats = self.fc_posterior(x_post)\n",
    "            post_dist = self.get_dist(post_stats)\n",
    "            s = post_dist.rsample()\n",
    "        else:\n",
    "            post_stats = prior_stats\n",
    "            post_dist = prior_dist\n",
    "            s\n",
    "\n",
    "        return (h, s), prior_dist, post_dist, prior_stats, post_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c574410",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObsEncoder(nn.Module):\n",
    "    def __init__(self, obs_shap=(3,64,64), embed_dim = 1024):\n",
    "        super().__init__()\n",
    "        C,H,W = obs_shap\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(C,32, 4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(32,64,4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(64,128,4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(128,256,4, stride=2), nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256*3*3, embed_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return self.encoder(obs)\n",
    "    \n",
    "class ObsDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim=30, hidden_dim=200, obs_shape=(3,64,64)):\n",
    "        super().__init__()\n",
    "        C,H,W = obs_shape\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(latent_dim+hidden_dim, 1024), nn.ReLU(),\n",
    "            nn.Linear(1024, 256*3*3), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.deconv=nn.Sequential(\n",
    "            nn.ConvTranspose2d(256,128,5, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128,64,5, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64,32,6, stride=2), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, C, 6, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, s,h):\n",
    "        x = torch.cat([s,h], dim=-1)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, 256, 3, 3)\n",
    "        return self.deconv(x)\n",
    "    \n",
    "class RewardDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim=30, hidden_dim=200):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(latent_dim+hidden_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, s,h):\n",
    "        x = torch.cat([s,h], dim=-1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09b995ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_elbo_loss(obs_seq, action_seq, reward_seq, encoder, decoder_obs, decoder_r, rssm, beta=1.0):\n",
    "    B,T,C,H,W = obs_seq.shape\n",
    "    loss_obs = 0.0\n",
    "    loss_reward = 0.0\n",
    "    loss_kl = 0.0\n",
    "\n",
    "    h, s = rssm.init_state(B)\n",
    "\n",
    "    for t in range(T):\n",
    "        o_t = obs_seq[:,t]\n",
    "        a_t = action_seq[:,t]\n",
    "        r_t = reward_seq[:,t]\n",
    "\n",
    "        emb_o = encoder(o_t)\n",
    "        (h,s), prior, posterior, prior_stats, post_stats = rssm((h,s), a_t, embed_obs=emb_o)\n",
    "\n",
    "        o_pred = decoder_obs(s,h)\n",
    "        r_pred = decoder_r(s,h)\n",
    "\n",
    "        recon_loss = F.mse_loss(o_pred, o_t, reduction='mean')\n",
    "        reward_loss = F.mse_loss(r_pred, r_t, reduction='mean')\n",
    "        kl_div = torch.distributions.kl.kl_divergence(posterior, prior).mean()\n",
    "\n",
    "        loss_obs += recon_loss\n",
    "        loss_reward += reward_loss\n",
    "        loss_kl += kl_div\n",
    "\n",
    "    total_loss = loss_obs + loss_reward + beta * loss_kl\n",
    "    return total_loss, loss_obs, loss_reward, kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b0d5334",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CEMPlanner:\n",
    "    def __init__(self, rssm, reward_model, action_dim, plan_horizon=12, optim_iters=5, candidates=1000, top_k=100, device='cpu'):\n",
    "        self.rssm = rssm\n",
    "        self.reward_model = reward_model\n",
    "        self.action_dim = action_dim\n",
    "        self.plan_horizon = plan_horizon\n",
    "        self.optim_iters = optim_iters\n",
    "        self.candidates = candidates\n",
    "        self.top_k = top_k\n",
    "        self.device = device\n",
    "\n",
    "    def plan(self, h,s):\n",
    "        B = h.size(0)\n",
    "        mean = torch.zeros(B, self.plan_horizon, self.action_dim).to(self.device)\n",
    "        std = torch.ones_like(mean)*0.3\n",
    "\n",
    "        for _ in range(self.optim_iters):\n",
    "            actions = torch.normal(mean.unsqueeze(1).expand(-1, self.candidates, -1,-1),\n",
    "                                   std.unsqueeze(1).expand(-1, self.candidates, -1,-1))\n",
    "            actions = actions.clamp(-1, 1)\n",
    "\n",
    "            B,C,H,A = actions.shape\n",
    "            acitons = actions.view(B*C, H, A)\n",
    "            hs = h.repeat_interleave(C, dim=0)\n",
    "            ss = s.repeat_interleave(C, dim=0)\n",
    "\n",
    "            total_reward = torch.zeros(B*C).to(self.device)\n",
    "\n",
    "            for t in range(H):\n",
    "                at = actions[:,t]\n",
    "                (hs, ss), prior, _,_,_ = self.rssm((hs, ss), at, embed_obs=None)\n",
    "                reward = self.reward_model(hs, ss).squeeze(-1)\n",
    "                total_reward += reward\n",
    "\n",
    "            total_reward = total_reward.view(B,C)\n",
    "            topk = torch.topk(total_reward, self.top_k, dim=-1).indices\n",
    "\n",
    "            elites = []\n",
    "            for i in range(B):\n",
    "                elites.append(actions.view(B,C,H,A)[i, topk[i]])\n",
    "            elites = torch.stack(elites)\n",
    "\n",
    "            mean = elites.mean(dim=1)\n",
    "            std = elites.std(dim=1)+1e-5\n",
    "\n",
    "        return mean[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fadc20a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "procgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
