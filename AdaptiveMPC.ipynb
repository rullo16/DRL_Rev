{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive MPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Predictive Control is a powerful model-based control strategy used in robotics, autonomous driving and RL. Unlike traditional RL methods, which rely on trial and error learning, MPC explicitly optimizes future actions at every step by solving an optimizaiton problem.\n",
    "MPC works by:\n",
    "1. Predicting future states using a learned dynamics model.\n",
    "2. Optimizing a sequence of actions over a plianning horizon.\n",
    "3. Executing onlu the first action, then repeating the process at the next timestep.\n",
    "\n",
    "The problem with standard MPC is the fixed horizon(H) throughout training which is not optimal:\n",
    "-Short horizons lead to myopic (short-sighted) behavior.\n",
    "-Long horizons increase computation time and amplify model inaccuracies\n",
    "-Uncertainty changes over tims, a fixed horizon does not adapt to model accuracy.\n",
    "\n",
    "Adaptive MPC dynamically adjusts the planning horizon H based on model uncertainty.\n",
    "- Low uncertainty -> Long horizon (uses full model capacity)\n",
    "- High uncertainty -> Short horizon (reduces error propagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MPC is an optimal control algorithm that repeatedly:\n",
    "1. Solves an optimization problem over a finite horizon.\n",
    "2. Finds the best control sequence minimizing a cost function.\n",
    "3. Executes the first control action, then repeats the process.\n",
    "\n",
    "At each timestep t, MPC solves:\n",
    "$$ \\min_{a_{t:t+H}}\\sum_{k=t}^{t+H} C(s_k,a_k)+\\lambda||a_k||$$\n",
    "subject to:\n",
    "$$s_{k+1} = f(s_k, a_k)$$\n",
    "\n",
    "where:\n",
    "- $H$ is the planning horizon.\n",
    "- $C(s_k,a_k)$ is the cost function.\n",
    "- $\\lambda$ is the regularization parameter.\n",
    "- $f(s_k, a_k)$ is the dynamics model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uncertainty-aware planning**:\n",
    "To make MPC adaptive, we define a confidence score:\n",
    "$$Uncertainty(s_k) = Var(\\hat{s}_{k+1})$$\n",
    "where $Var(\\hat{s}_{k+1})$ is an ensemble of dynamics models predicting the next state.\n",
    "The adaptive planning horizon is then defined as:\n",
    "$$H_t = \\max(H_{min}, \\min(H_{max}, H_{base}\\cdot \\exp^{-\\beta \\cdot Uncertainty(s_t)}))$$\n",
    "where:\n",
    "- $H_{min}$ is the minimum horizon.\n",
    "- $H_{max}$ is the maximum horizon.\n",
    "- $\\beta$ is the scaling factor, sensitivity parameter.\n",
    "This ensures that MPC makes longer plans in familiar situations and shorter plans when uncertainty is high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adaptive Horizon Computation**:\n",
    "At each timestep:\n",
    "1. Compute the model uncertainty using an ensemble of models.\n",
    "2. Adjust the horizon $H_t$ accordingly:\n",
    "- If uncertainty is low, increase the horizon.\n",
    "- If uncertainty is high, decrease the horizon.\n",
    "**Ensemble-Based Uncertainty Estimation**:\n",
    "We use N learned dynamics models, each predicting next states:\n",
    "$$\\hat{s}_{t+1}^{(i)} = f_{\\theta_i}(s_t, a_t), i\\in{1,2,...,N}$$\n",
    "The variance across models fives uncertainty:\n",
    "$$Uncertainty(s_t) = \\frac{1}{N} \\sum_{i=1}^{N} (\\hat{s}_{t+1}^{(i)} - \\bar{s}_{t+1})^2$$\n",
    "where $\\bar{s}_{t+1}$ is the mean prediction across models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=int(1e6)):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        state = np.array(state, dtype=np.float32).flatten()\n",
    "        action = np.array(action, dtype=np.float32).flatten()\n",
    "        next_state = np.array(next_state, dtype=np.float32).flatten()\n",
    "        reward = np.float32(reward)\n",
    "        done = np.float32(done)\n",
    "        \n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        return (torch.tensor(np.stack(states), dtype=torch.float32),\n",
    "                torch.tensor(np.stack(actions), dtype=torch.float32),\n",
    "                torch.tensor(rewards, dtype=torch.float32).unsqueeze(1),\n",
    "                torch.tensor(np.stack(next_states), dtype=torch.float32),\n",
    "                torch.tensor(dones, dtype=torch.float32).unsqueeze(1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class AdaptiveMPC:\n",
    "    def __init__(self, state_dim, action_dim, num_models=5, beta=0.1):\n",
    "        self.models = [self._build_model(state_dim, action_dim) for _ in range(num_models)]\n",
    "        self.num_models = num_models\n",
    "        self.beta = beta\n",
    "        self.rollout_horizon = 5 # initial value\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "    def _build_model(self, state_dim, action_dim):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, state_dim+1)\n",
    "        )\n",
    "    \n",
    "    def predict(self, state, action):\n",
    "        inputs = torch.cat([state, action], dim=-1)\n",
    "        predictions = [model(inputs) for model in self.models]\n",
    "        predictions = torch.stack(predictions)\n",
    "\n",
    "        mean_prediction = predictions.mean(dim=0)\n",
    "        uncertainty = predictions.var(dim=0).mean().item()\n",
    "        \n",
    "        self.update_horizon(uncertainty)\n",
    "        return mean_prediction[..., :-1], mean_prediction[..., -1], uncertainty\n",
    "    \n",
    "    def plan(self, state, num_samples=500):\n",
    "        mean = np.zeros((self.rollout_horizon, self.action_dim))\n",
    "        std = np.ones((self.rollout_horizon, self.action_dim))\n",
    "\n",
    "        for _ in range(5):\n",
    "            actions = np.random.normal(mean, std, (num_samples, self.rollout_horizon, self.action_dim))\n",
    "            returns = np.zeros(num_samples)\n",
    "\n",
    "            for i in range(num_samples):\n",
    "                returns[i] = self.evaluate_sequence(state, actions[i])\n",
    "\n",
    "            elite_idxs = returns.argsort()[-int(0.1*num_samples):]\n",
    "            elite_actions = actions[elite_idxs]\n",
    "\n",
    "            mean = elite_actions.mean(axis=0)\n",
    "            std = elite_actions.std(axis=0)\n",
    "        \n",
    "        return mean[0]\n",
    "\n",
    "    def evaluate_sequence(self, state, actions):\n",
    "        total_pred_reward = 0.0\n",
    "        uncertainty_sum = 0.0\n",
    "\n",
    "        for action in actions:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "            action_tensor = torch.tensor(action, dtype=torch.float32).unsqueeze(0)\n",
    "            next_state_pred, reward_pred, uncertainty = self.predict(state_tensor, action_tensor)\n",
    "\n",
    "            total_pred_reward += reward_pred.item()\n",
    "            state = next_state_pred.detach().numpy().squeeze(0)\n",
    "            uncertainty_sum += uncertainty\n",
    "        \n",
    "        avg_uncertainty = uncertainty_sum / len(actions)\n",
    "        self.update_horizon(avg_uncertainty)\n",
    "        return total_pred_reward\n",
    "    \n",
    "    def update_horizon(self, uncertainty):\n",
    "        self.rollout_horizon = int(max(1, min(20, 5*np.exp(-self.beta*uncertainty))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize environment\n",
    "env = gym.make(\"HalfCheetah-v5\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "# Initialize components\n",
    "adaptive_mpc = AdaptiveMPC(state_dim, action_dim)\n",
    "real_buffer = ReplayBuffer(max_size=int(1e6))\n",
    "\n",
    "num_episodes = 500\n",
    "adaptive_mpc_rewards = []\n",
    "\n",
    "# Training loop\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    for step in range(1000):  # Max steps per episode\n",
    "        action = adaptive_mpc.plan(state)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        real_buffer.add(state, action, reward, next_state, done)\n",
    "        total_reward += reward\n",
    "        state = next_state if not done else env.reset()\n",
    "\n",
    "        if len(real_buffer) > 10000:\n",
    "            # Train model using real buffer\n",
    "            states, actions, rewards, next_states, _ = real_buffer.sample(256)\n",
    "            predicted_next_states, predicted_rewards, _ = adaptive_mpc.predict(states, actions)\n",
    "            model_loss = nn.MSELoss()(predicted_next_states, next_states) + nn.MSELoss()(predicted_rewards, rewards)\n",
    "            \n",
    "            optimizer = torch.optim.Adam(\n",
    "                [param for model in adaptive_mpc.models for param in model.parameters()], lr=1e-3\n",
    "            )\n",
    "            optimizer.zero_grad()\n",
    "            model_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    adaptive_mpc_rewards.append(total_reward)\n",
    "    print(f\"Episode {episode}: Total Reward = {total_reward}\")\n",
    "\n",
    "# Plot results\n",
    "plt.plot(adaptive_mpc_rewards, label=\"Adaptive MPC\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"Adaptive MPC on HalfCheetah-v4\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
