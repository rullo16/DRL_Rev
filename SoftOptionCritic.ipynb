{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bcd30ee",
   "metadata": {},
   "source": [
    "# Soft-Option Critic (SOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea84131",
   "metadata": {},
   "source": [
    "Extends Option-Critic by incorporating maximum entropy RL principles. This means instead of just maximizing return, the agent also tries to maximize entropy, promoting stochastic and exploratory behaviors.\\\n",
    "Introduces these key ideas:\n",
    "- Soft Q-learning: Objective instead of standard Q-learning\n",
    "- Options that are optimized using soft value functions\n",
    "- Encouraging diverse behaviors through entropy regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ea508e",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5c3180",
   "metadata": {},
   "source": [
    "### Maximum Entropy Reinforcement Learning\n",
    "Instead of maximizing cumulative reward R, we maximize:\n",
    "$$\\mathbb{E}[\\sum_t r(s_t, a_t)+\\alpha \\mathcal{H}(\\pi(\\cdot|s_t))]$$\n",
    "Where $\\mathcal{H}$ is the entropy and $\\alpha$ is a temperature parameter controlling the entropy/reward trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c7a5c1",
   "metadata": {},
   "source": [
    "### SAC\n",
    "Off-policy actor-critic method that uses stochastic policies and entropy regularization.\\\n",
    "Soft Q-value is:\n",
    "$$Q^\\pi(s,a)=r(s,a)+\\gamma\\mathbb{E}_{s'}[V^\\pi(s')]$$\n",
    "$$V^\\pi(s)=\\mathbb{E}_{a\\sim\\pi}[Q^\\pi(s,a)-\\alpha\\log\\pi(a|s)]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918abdea",
   "metadata": {},
   "source": [
    "## Math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215b1ccb",
   "metadata": {},
   "source": [
    "### Soft Option Value Function\n",
    "$$Q_\\Omega(s,o) = \\mathbb{E}_{a\\sim\\pi°(\\cdot|s)}[Q_U(s,p,a)-\\log\\pi°(a|s)]$$\n",
    "### Option Termination\n",
    "$$U(s',o) = (1-\\beta°(s'))Q_\\Omega(s',o)+\\beta°(s')V_\\Omega(s')$$\n",
    "### State Value with Options\n",
    "$$V_\\Omega(s) = \\mathbb{E}_{o\\sim\\pi°(\\cdot|s)}[Q_\\Omega(s,o)-\\log\\pi°(o|s)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aa73f7",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8638d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class SoftOptionPolicy(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, num_options):\n",
    "        super().__init__()\n",
    "        self.policies = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(state_dim, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, action_dim),\n",
    "            ) for _ in range(num_options)\n",
    "        ])\n",
    "\n",
    "    def forward(self, state,option):\n",
    "        logits = self.policies[option](state)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        return probs, log_probs\n",
    "\n",
    "class SoftTermination(nn.Module):\n",
    "    def __init__(self, state_dim, num_options):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_options),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "\n",
    "class SoftOptionCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, num_options):\n",
    "        super().__init__()\n",
    "        self.q_option = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_options),\n",
    "        )\n",
    "        self.option_policy = SoftOptionPolicy(state_dim, action_dim, num_options)\n",
    "        self.termination = SoftTermination(state_dim, num_options)\n",
    "        self.pi_o = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_options),\n",
    "        )\n",
    "\n",
    "    def get_option_probs(self, state):\n",
    "        logits = self.pi_o(state)\n",
    "        return F.softmax(logits, dim=-1), F.log_softmax(logits, dim=-1)\n",
    "\n",
    "    def get_action(self, state, option):\n",
    "        with torch.no_grad():\n",
    "            probs, _ = self.option_policy(state, option)\n",
    "            return torch.multinomial(probs, 1).item()\n",
    "\n",
    "    def get_option(self, state):\n",
    "        with torch.no_grad():\n",
    "            probs, _ = self.get_option_probs(state)\n",
    "            return torch.multinomial(probs, 1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d03a65e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 complete\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x5 and 6x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Q-values\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([states, actions], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 55\u001b[0m q_vals \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_option\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, options)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     58\u001b[0m     next_opt_probs, next_log_probs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_option_probs(next_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/procgen/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/procgen/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/procgen/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/procgen/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/procgen/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/procgen/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x5 and 6x64)"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "# Replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, *transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        return map(np.array, zip(*batch))\n",
    "    \n",
    "# Training hyperparameters\n",
    "gamma = 0.99\n",
    "alpha = 0.1\n",
    "batch_size = 64\n",
    "num_options = 2\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "model = SoftOptionCritic(state_dim, action_dim, num_options)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "buffer = ReplayBuffer()\n",
    "\n",
    "# Training loop\n",
    "for episode in range(500):\n",
    "    state = torch.tensor(env.reset()[0], dtype=torch.float32)\n",
    "    option = model.get_option(state)\n",
    "\n",
    "    for t in range(200):\n",
    "        action = model.get_action(state, option)\n",
    "        next_state_np, reward, done, _, _ = env.step(action)\n",
    "        next_state = torch.tensor(next_state_np, dtype=torch.float32)\n",
    "\n",
    "        # Store transition\n",
    "        buffer.push(state.numpy(), action, reward, next_state.numpy(), float(done), option)\n",
    "        state = next_state\n",
    "\n",
    "        # Train\n",
    "        if len(buffer.buffer) >= batch_size:\n",
    "            states, actions, rewards, next_states, dones, options = buffer.sample(batch_size)\n",
    "            states = torch.tensor(states, dtype=torch.float32)\n",
    "            actions = torch.tensor(actions, dtype=torch.int64).unsqueeze(1)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
    "            next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
    "            options = torch.tensor(options, dtype=torch.int64).unsqueeze(1)\n",
    "\n",
    "            # Q-values\n",
    "            input = torch.cat([states, actions], dim=-1)\n",
    "            q_vals = model.q_option(input).gather(1, options)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                next_opt_probs, next_log_probs = model.get_option_probs(next_states)\n",
    "                next_q_vals = model.q_option(next_states)\n",
    "                next_v = (next_opt_probs * (next_q_vals - next_log_probs)).sum(dim=-1, keepdim=True)\n",
    "                target_q = rewards + gamma * (1 - dones) * next_v\n",
    "\n",
    "            q_loss = F.mse_loss(q_vals, target_q)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            q_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if episode % 50 == 0:\n",
    "        print(f\"Episode {episode} complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9190bc57",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cafbc0",
   "metadata": {},
   "source": [
    "## SOC for Continuous Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702d8bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Gaussian Policy per option\n",
    "class GaussianPolicy(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 256), nn.ReLU()\n",
    "        )\n",
    "        self.mean = nn.Linear(256, action_dim)\n",
    "        self.log_std = nn.Linear(256, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.fc(state)\n",
    "        mean = self.mean(x)\n",
    "        log_std = torch.clamp(self.log_std(x), -20, 2)\n",
    "        std = log_std.exp()\n",
    "        return mean, std\n",
    "\n",
    "    def sample(self, state):\n",
    "        mean, std = self.forward(state)\n",
    "        dist = torch.distributions.Normal(mean, std)\n",
    "        z = dist.rsample()\n",
    "        action = torch.tanh(z)\n",
    "        log_prob = dist.log_prob(z) - torch.log(1 - action.pow(2) + 1e-6)\n",
    "        return action, log_prob.sum(-1, keepdim=True), mean, std\n",
    "\n",
    "# Q-network (per option)\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.q = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        return self.q(x)\n",
    "\n",
    "# Option selector\n",
    "class OptionSelector(nn.Module):\n",
    "    def __init__(self, state_dim, num_options):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, num_options)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        logits = self.net(state)\n",
    "        return F.softmax(logits, dim=-1), F.log_softmax(logits, dim=-1)\n",
    "\n",
    "# Termination network\n",
    "class Termination(nn.Module):\n",
    "    def __init__(self, state_dim, num_options):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, num_options), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "\n",
    "# Complete SOC+SAC Agent\n",
    "class SoftOptionCriticContinuous(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, num_options):\n",
    "        super().__init__()\n",
    "        self.num_options = num_options\n",
    "        self.policies = nn.ModuleList([GaussianPolicy(state_dim, action_dim) for _ in range(num_options)])\n",
    "        self.q_funcs = nn.ModuleList([QNetwork(state_dim, action_dim) for _ in range(num_options)])\n",
    "        self.q_targets = nn.ModuleList([QNetwork(state_dim, action_dim) for _ in range(num_options)])\n",
    "        self.option_selector = OptionSelector(state_dim, num_options)\n",
    "        self.termination = Termination(state_dim, num_options)\n",
    "\n",
    "    def sample_action(self, state, option):\n",
    "        return self.policies[option].sample(state)\n",
    "\n",
    "    def get_q(self, state, action, option):\n",
    "        return self.q_funcs[option](state, action)\n",
    "\n",
    "    def get_target_q(self, state, action, option):\n",
    "        return self.q_targets[option](state, action)\n",
    "\n",
    "    def get_option_probs(self, state):\n",
    "        return self.option_selector(state)\n",
    "\n",
    "    def get_termination_probs(self, state):\n",
    "        return self.termination(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdc3771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, *transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        return map(np.array, zip(*batch))\n",
    "\n",
    "# Hyperparameters\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "num_options = 2\n",
    "gamma = 0.99\n",
    "alpha = 0.2\n",
    "batch_size = 64\n",
    "lr = 3e-4\n",
    "tau = 0.005\n",
    "\n",
    "# Initialize agent and target networks\n",
    "agent = SoftOptionCriticContinuous(state_dim, action_dim, num_options)\n",
    "buffer = ReplayBuffer()\n",
    "optimizers = {\n",
    "    \"q\": [optim.Adam(q.parameters(), lr=lr) for q in agent.q_funcs],\n",
    "    \"policy\": [optim.Adam(pi.parameters(), lr=lr) for pi in agent.policies],\n",
    "    \"option\": optim.Adam(agent.option_selector.parameters(), lr=lr),\n",
    "    \"termination\": optim.Adam(agent.termination.parameters(), lr=lr)\n",
    "}\n",
    "\n",
    "# Soft update for target networks\n",
    "def soft_update(target, source):\n",
    "    for t_param, s_param in zip(target.parameters(), source.parameters()):\n",
    "        t_param.data.copy_(tau * s_param.data + (1 - tau) * t_param.data)\n",
    "\n",
    "# Main Training Loop\n",
    "for episode in range(500):\n",
    "    state = torch.tensor(env.reset()[0], dtype=torch.float32)\n",
    "    option = agent.get_option_probs(state.unsqueeze(0))[0].multinomial(1).item()\n",
    "\n",
    "    for t in range(200):\n",
    "        action, logp, _, _ = agent.sample_action(state.unsqueeze(0), option)\n",
    "        action_np = action.squeeze().detach().numpy()\n",
    "        next_state_np, reward, done, _, _ = env.step(action_np)\n",
    "        next_state = torch.tensor(next_state_np, dtype=torch.float32)\n",
    "\n",
    "        buffer.push(state.numpy(), action_np, reward, next_state.numpy(), float(done), option, logp.item())\n",
    "        state = next_state\n",
    "\n",
    "        # Sample and train\n",
    "        if len(buffer.buffer) >= batch_size:\n",
    "            s, a, r, s2, d, o, logp = buffer.sample(batch_size)\n",
    "            s = torch.tensor(s, dtype=torch.float32)\n",
    "            a = torch.tensor(a, dtype=torch.float32)\n",
    "            r = torch.tensor(r, dtype=torch.float32).unsqueeze(1)\n",
    "            s2 = torch.tensor(s2, dtype=torch.float32)\n",
    "            d = torch.tensor(d, dtype=torch.float32).unsqueeze(1)\n",
    "            o = torch.tensor(o, dtype=torch.int64).unsqueeze(1)\n",
    "            logp = torch.tensor(logp, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "            # Critic loss\n",
    "            q_vals = torch.stack([\n",
    "                agent.get_q(s, a, i).squeeze() for i in range(num_options)\n",
    "            ], dim=1)\n",
    "            q = q_vals.gather(1, o)\n",
    "            with torch.no_grad():\n",
    "                a2, logp2, _, _ = agent.sample_action(s2, o[0,0].item())\n",
    "                q_target = agent.get_target_q(s2, a2, o[0,0].item())\n",
    "                target = r + gamma * (1 - d) * (q_target - alpha * logp2)\n",
    "            q_loss = F.mse_loss(q, target)\n",
    "\n",
    "            optimizers[\"q\"][o[0,0]].zero_grad()\n",
    "            q_loss.backward()\n",
    "            optimizers[\"q\"][o[0,0]].step()\n",
    "\n",
    "            # Policy loss\n",
    "            new_action, new_logp, _, _ = agent.sample_action(s, o[0,0].item())\n",
    "            q_new = agent.get_q(s, new_action, o[0,0].item())\n",
    "            policy_loss = (alpha * new_logp - q_new).mean()\n",
    "\n",
    "            optimizers[\"policy\"][o[0,0]].zero_grad()\n",
    "            policy_loss.backward()\n",
    "            optimizers[\"policy\"][o[0,0]].step()\n",
    "\n",
    "            # Soft update target networks\n",
    "            soft_update(agent.q_targets[o[0,0]], agent.q_funcs[o[0,0]])\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if episode % 25 == 0:\n",
    "        print(f\"Episode {episode} complete.\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84a630a",
   "metadata": {},
   "source": [
    "### Transformer-based Option Selection\n",
    "Replace MLP option selector with transformer or RNN that considers temporal context.\\\n",
    "In partially observable environments or those with temporally extended dynamics, it helps if option selection is based not just on the current state, but on a history of states or embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a89d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerOptionSelector(nn.Module):\n",
    "    def __init__(self, state_dim, num_options, seq_len=10, d_model=128, nhead=4):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Linear(state_dim, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        self.classifier = nn.Linear(d_model, num_options)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def forward(self, state_seq):\n",
    "        # state_seq: (batch_size, seq_len, state_dim)\n",
    "        x = self.embed(state_seq)\n",
    "        x = self.transformer(x)\n",
    "        final_token = x[:, -1]  # use final embedding\n",
    "        return F.softmax(self.classifier(final_token), dim=-1), F.log_softmax(self.classifier(final_token), dim=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a177afc9",
   "metadata": {},
   "source": [
    "### Sparse Rewards + Intrinsic Motivation\n",
    "### Sparse Rewards + Intrinsic Motivation\n",
    "\n",
    "To tackle hard-exploration tasks, intrinsic rewards can be used effectively. Examples include:\n",
    "\n",
    "- **RND (Random Network Distillation)**: Encourages exploration by rewarding states that are hard to predict.\n",
    "- **ICM (Intrinsic Curiosity Module)**: Drives exploration by rewarding transitions that are hard to model.\n",
    "- **Novelty-based rewards**: Rewards the agent for visiting novel states.\n",
    "\n",
    "#### Integration Steps:\n",
    "\n",
    "1. Compute intrinsic reward $r_{\\text{int}}$ at each timestep.\n",
    "2. Combine intrinsic reward with external reward:\n",
    "    $$r' = r_{\\text{ext}} + \\eta \\cdot r_{\\text{int}}$$\n",
    "    where $\\eta$ is a scaling factor for intrinsic rewards.\n",
    "3. Train the Soft Option-Critic using the combined reward $ r' $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c07998",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNDModel(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        self.target = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128), nn.ReLU(), nn.Linear(128, 128)\n",
    "        )\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128), nn.ReLU(), nn.Linear(128, 128)\n",
    "        )\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False  # fixed target\n",
    "\n",
    "    def forward(self, state):\n",
    "        with torch.no_grad():\n",
    "            target_feat = self.target(state)\n",
    "        pred_feat = self.predictor(state)\n",
    "        return F.mse_loss(pred_feat, target_feat, reduction='none').mean(dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49560a91",
   "metadata": {},
   "source": [
    "Use DIAYN-style skill discovery to unsupervisedly learn diverse, distinguishable options.\n",
    "\n",
    "#### Core Idea:\n",
    "Train a discriminator to classify the option ID from state embeddings:\n",
    "- The easier it is to predict the option from the state, the more distinguishable the skills.\n",
    "- Adds mutual information (MI) between states and options, $MI(s, z)$, into the reward function.\n",
    "\n",
    "#### DIAYN Components:\n",
    "- **Discriminator**: $D(o \\mid s)$\n",
    "- **Intrinsic Reward**:\n",
    "    $$r_{\\text{disc}}(s, o) = \\log D(o \\mid s) - \\log p(o)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd0f09d",
   "metadata": {},
   "source": [
    "### Mutual Information Regularization (InfoMax)\n",
    "\n",
    "Encourage options to encode distinct representations by maximizing mutual information. This ensures that each option learns a unique and disentangled representation.\n",
    "\n",
    "#### Loss Examples:\n",
    "- **InfoNCE Loss**: Measures the similarity between the option ID and the learned state embedding.\n",
    "- **Disentangled Representations**: Promotes diversity across options.\n",
    "\n",
    "#### Loss Function:\n",
    "$$\n",
    "\\mathcal{L}_{\\text{InfoNCE}} = -\\mathbb{E}_{(s, o)}\\left[\\log \\frac{\\exp(f(s)^\\top W g(o))}{\\sum_{o'} \\exp(f(s)^\\top W g(o'))}\\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( f(s) \\): State embedding function.\n",
    "- \\( g(o) \\): Option embedding function.\n",
    "- \\( W \\): Learnable weight matrix.\n",
    "\n",
    "This loss encourages the embeddings \\( f(s) \\) and \\( g(o) \\) to align for the correct option \\( o \\), while being distinct from other options \\( o' \\)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fc07f1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "procgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
