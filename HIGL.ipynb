{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Goal Latents (HIGL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In traditional RL agents struggle to explore effectively. Hierarchical RL tackles this by decomposing a task into temporally abstract sub-tasks. However a major bottleneck lies in defining what the high-level policy should output, instead of raw actions or hardcoded goals, HIGL proposes learning a continuous latent goal space, allowing flexible and reusable nehaviours.\n",
    "HIGL: Learn a goal embedding space + train a goal-conditioned low-level policy to reach high-level latent goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backgorund\n",
    "1. **Two-level HRL**:\n",
    "    -High-level policy: $\\pi_h(z_g|s_t)$, output a latent goal $z_g\\in \\mathbb{R}^d$ every k steps.\n",
    "    -Low-level policy: $\\pi_l(a_t|s_t,z_g)$, tries to reach the current goal $z_g$ from the current state $s_t$.\n",
    "\n",
    "2. **Goal Encoder $E: s \\leftarrow z$**:\n",
    "    -Learns to map state observations $s\\in\\mathcal{S}$ into latent goals $z\\in\\mathcal{Z}$.\n",
    "    -Often implemented as CNN or MLP.\n",
    "3. **Discriminator(Optional)**:\n",
    "    - Encourages the latent goals to be informative and diverse, ensuring that goals correspond to distinct, reachable behaviours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the framework as a Semi-MDP:\n",
    "- Low-level policy Objective: Train a policy $\\pi_l(a_t|s_t,z_g)$ to reach a goal $z_g$, the latent representation of a future state. We define the goal-reaching reward as:\n",
    "$$r_t^{goal} = - ||E(s_{t+1})-z_g||^2$$\n",
    "- High.level Policy Objective: Every k steps, a high-level policy selects a new latent goal:\n",
    "$$z_g \\sim \\pi_h(z_g|s_t)$$\n",
    "It is trained via policy gradient, but using a reward signal that reflects the success of reaching the goal.\n",
    "This can be Extrinsic from the env, or goal-reaching reward defined above.\n",
    "- Training the Encoder: The encoder E must ensure that latent distances reflect the true notion of reaching. There are two popular ways to train it:\n",
    "   1. Temporal Contrastive Loss: Maximizes the mutual information between current state $s_t$ and future state $s_{t+k}$:\n",
    "   $$\\mathcal{L}_{contrastive} = - \\log \\frac{\\exp(\\sim(E(s_t),E(s_{t+k}))}{\\sum_{i=1}^N \\exp(\\sim(E(s_t),E(s_i)))}$$\n",
    "   Where $\\sim$ is cosine similarity or dot product, and $s_j$ are negative samples.\n",
    "   2. Reconstruction Loss: Make the latent goal predictive of some future state or reward:\n",
    "   $$\\mathcal{L}_{reconstruction} = ||\\hat{s}_{t+k} - s_{t+k}||^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The high-level policy only makes decisions every k steps. The low-level policy continues to act, conditioned on the same latent goal $z_g$, until the next high-level decision.\n",
    "This introduces a temporal abstraction:\n",
    "- Reduces the frequency of high-level decisions.\n",
    "- Encourages the low-level policy to learn to reach the goal.\n",
    "- Improves sample efficiency and long-horizon planning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Low-level SAC or PPO Loss**:\n",
    "$$ r_t = - ||E(s_{t+k}) - z_g||^2$$\n",
    "Train using standard SAC or PPO on this reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**High-Level PPO or REINFORCE**:\n",
    "$$\\mathcal{J}(\\pi_h) = \\mathbb{E}_{s_t,z_t}[R_{t:t+k}]$$\n",
    "Optimize using the returns obtained while pursuing goal $z_g$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional Contrastive/InfoNCE Loss for Encoder**:\n",
    "$$\\mathcal{L}_{info} = - \\log \\frac{\\exp(E(s_t)^T E(s_{t+k}))}{\\sum_{j}\\exp(E(s_t)^T E(s_j))}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, input_shape=(3,96,96), latent_dim=64):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2), nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, *input_shape)\n",
    "            conv_out = self.conv(dummy)\n",
    "            self.flattened = conv_out.view(1,-1).size(1)\n",
    "        \n",
    "        self.fc = nn.Linear(self.flattened, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x/255.0\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkillEmbedding(nn.Module):\n",
    "    def __init__(self, num_skills, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_skills, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MI_Estimator(nn.Module):\n",
    "    def __init__(self, latent_dim, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.project_state = nn.Linear(latent_dim, embedding_dim)\n",
    "        self.project_skill = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, encoded_states, skill_embeddings):\n",
    "        z = self.project_state(encoded_states)\n",
    "        s = self.project_skill(skill_embeddings)\n",
    "\n",
    "        logits = torch.matmul(z, s.T)\n",
    "        labels = torch.arange(len(z), device=z.device)\n",
    "        mi_loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return -mi_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerPolicy(nn.Module):\n",
    "    def __init__(self, input_dim, skill_dim, action_dim, seq_len, hidden_dim=128, n_heads=4, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.input_proj = nn.Linear(input_dim+skill_dim, hidden_dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model = hidden_dim,\n",
    "            nhead = n_heads,\n",
    "            dim_feedforward = hidden_dim*2,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.head = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, state_seq, skill_seq):\n",
    "        x = torch.cat([state_seq, skill_seq], dim=-1)\n",
    "        x = self.input_proj(x)\n",
    "        x = self.transformer(x)\n",
    "        out = self.head(x[:, -1])\n",
    "        return torch.tanh(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim, skill_dim, action_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.q1 = nn.Sequential(\n",
    "            nn.Linear(input_dim+skill_dim+action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "        self.q2 = nn.Sequential(\n",
    "            nn.Linear(input_dim+skill_dim+action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action, skill):\n",
    "        x = torch.cat([state, action, skill], dim=-1)\n",
    "        return self.q1(x), self.q2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentGoalSACAgent(nn.Module):\n",
    "    def __init__(self, image_shape, num_skills, action_dim, latent_dim=64, seq_len=10, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.seq_len = seq_len\n",
    "        self.encoder = ConvEncoder(input_shape=image_shape, latent_dim=latent_dim).to(device)\n",
    "        self.skill_gen = SkillEmbedding(num_skills, latent_dim).to(device)\n",
    "\n",
    "        self.policy = TransformerPolicy(\n",
    "            input_dim = latent_dim,\n",
    "            skill_dim= latent_dim,\n",
    "            action_dim= action_dim,\n",
    "            seq_len = seq_len,\n",
    "        ).to(device)\n",
    "\n",
    "        self.critic = Critic(latent_dim, latent_dim, action_dim).to(device)\n",
    "        self.target_critic = Critic(latent_dim, latent_dim, action_dim).to(device)\n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "\n",
    "        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=3e-4)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=3e-4)\n",
    "\n",
    "        self.log_alpha = torch.tensor(np.lot(0.1), requires_grad=True, device=device)\n",
    "        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=3e-4)\n",
    "\n",
    "        self.target_entropy = -action_dim\n",
    "\n",
    "    @property\n",
    "    def alpha(self):\n",
    "        return self.log_alpha.exp()\n",
    "    \n",
    "    def act(self,obs_seq, skill_seq):\n",
    "        with torch.no_grad():\n",
    "            z_seq = self.encoder(obs_seq)\n",
    "            a = self.policy(z_seq, skill_seq)\n",
    "            return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(agent, replay_buffer, batch_size=128, gamma=0.99, tau=0.005):\n",
    "    obs_seq, skill_seq, actions, rewards, next_obs_seq, dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "    z_seq = agent.encoder(obs_seq)\n",
    "    next_z_seq = agent.encoder(next_obs_seq)\n",
    "\n",
    "    skill_embeddings = agent.skill_gen(skill_seq)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_action = agent.policy(next_z_seq, skill_embeddings)\n",
    "        q1_tgt, q2_tgt = agent.target_critic(next_z_seq[:,-1], next_action, skill_embeddings[:,-1])\n",
    "        min_q_tgt = torch.min(q1_tgt, q2_tgt)\n",
    "        target = rewards + (1-dones) * gamma * (min_q_tgt - agent.alpha * agent.log_prob(next_action))\n",
    "\n",
    "    q1, q2 = agent.critic(z_seq[:,-1], actions, skill_embeddings[:,-1])\n",
    "    critic_loss = F.mse_loss(q1, target) + F.mse_loss(q2, target)\n",
    "\n",
    "    agent.critic_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    agent.critic_optimizer.step()\n",
    "\n",
    "    new_action = agent.policy(z_seq, skill_embeddings)\n",
    "    q1_new, q2_new = agent.critic(z_seq[:,-1], new_action, skill_embeddings[:,-1])\n",
    "    min_q_new = torch.min(q1_new, q2_new)\n",
    "\n",
    "    log_prob = agent.log_prob(new_action)\n",
    "    policy_loss = (agent.alpha * log_prob - min_q_new).mean()\n",
    "\n",
    "    agent.policy_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    agent.policy_optimizer.step()\n",
    "\n",
    "    alpha_loss = -(agent.log_alpha * (log_prob + agent.target_entropy).detach()).mean()\n",
    "    agent.alpha_optimizer.zero_grad()\n",
    "    alpha_loss.backward()\n",
    "    agent.alpha_optimizer.step()\n",
    "\n",
    "    for target_param, param in zip(agent.target_critic.parameters(), agent.critic.parameters()):\n",
    "        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoalReplayBuffer:\n",
    "    def __init__(self, max_size, obs_shape, action_dim, latent_dim, seq_len=10):\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.obs = torch.zeros((max_size, seq_len, *obs_shape))\n",
    "        self.next_obs = torch.zeros((max_size, seq_len, *obs_shape))\n",
    "        self.actions = torch.zeros((max_size, action_dim))\n",
    "        self.rewards = torch.zeros((max_size, 1))\n",
    "        self.dones = torch.zeros((max_size, 1))\n",
    "        self.skills = torch.zeros((max_size, seq_len, latent_dim))\n",
    "\n",
    "    def add(self,obs,skill,action, reward, next_obs, done):\n",
    "        self.obs[self.ptr] = obs\n",
    "        self.skills[self.ptr] = skill\n",
    "        self.actions[self.ptr] = action\n",
    "        self.rewards[self.ptr] = reward\n",
    "        self.next_obs[self.ptr] = next_obs\n",
    "        self.dones[self.ptr] = done\n",
    "\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "    \n",
    "    def sample(self,batch_size, device=\"cuda\"):\n",
    "        idxs = np.random.choice(0, self.size, size=batch_size)\n",
    "        return (\n",
    "            self.obs[idxs].to(device),\n",
    "            self.skills[idxs].to(device),\n",
    "            self.actions[idxs].to(device),\n",
    "            self.rewards[idxs].to(device),\n",
    "            self.next_obs[idxs].to(device),\n",
    "            self.dones[idxs].to(device)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_latent_goal_agent(env, agent, replay_buffer, episodes=1000, train_after=500, seq_len=10):\n",
    "    all_rewards = []\n",
    "    for ep in range(episodes):\n",
    "        obs_seq, skill_seq = [], []\n",
    "        obs = env.reset()[0]\n",
    "        tot_reward = 0\n",
    "        done = False\n",
    "\n",
    "        for t in range(200):\n",
    "            obs_tensor = preprocess_obs(obs)\n",
    "            skill = agent.skill_gen.sample().squeeze(0)\n",
    "\n",
    "            obs_seq.append(obs_tensor)\n",
    "            skill_seq.append(skill)\n",
    "\n",
    "            if len(obs_seq) > seq_len:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                obs_seq_tensor = torch.stack(obs_seq[-seq_len:]).unsqueeze(0).to(device)\n",
    "                skill_seq_tensor = torch.stack(skill_seq[-seq_len:]).unsqueeze(0).to(device)\n",
    "                action = agent.act(obs_seq_tensor, skill_seq_tensor).cpu().numpy()\n",
    "\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            tot_reward += reward\n",
    "            done = terminated or truncated\n",
    "\n",
    "            if len(obs_seq) >= seq_len:\n",
    "                replay_buffer.add(\n",
    "                    torch.stack(obs_seq[-seq_len:]),\n",
    "                    torch.stack(skill_seq[-seq_len:]),\n",
    "                    torch.tensor(action,dtype=torch.float32),\n",
    "                    torch.tensor([reward], dtype=torch.float32),\n",
    "                    preprocess_obs(next_obs).unsqueeze(0),\n",
    "                    torch.tensor([done], dtype=torch.float32)\n",
    "                )\n",
    "\n",
    "            obs = next_obs\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            if replay_buffer.size >= train_after:\n",
    "                for _ in range(5):\n",
    "                    update(agent, replay_buffer)\n",
    "\n",
    "            if ep % 10 == 0:\n",
    "                print(f\"Episode {ep}, Step {t}, Total Reward: {tot_reward:.2f}\")\n",
    "    \n",
    "    return all_rewards\n",
    "\n",
    "def preprocess_obs(obs):\n",
    "    obs = torch.tensor(obs, dtype=torch.float32).permute(2,0,1)\n",
    "    return obs / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "procgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
