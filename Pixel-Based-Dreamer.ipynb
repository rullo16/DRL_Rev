{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pixel-Based DreamerV2: Vision Based RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many Real world applications involve visual perception rather than structured low-dimensional state inputs. However, learning from high-dimensional pixel observations is computationally expensive and sample-inefficient using traditional RL approaches.\n",
    "Pixel-Based DreamerV2 solves this by:\n",
    "- Using a Convolutinal Encoder to transform high-dimensional images into compact latent representations.\n",
    "- Performing all planning and policy optimization in the latent space, drastically reducing computational complexity\n",
    "- Learning a latent transition model to enable efficient imagination-based planning\n",
    "\n",
    "**Real-World Applications:**\n",
    "- Robotics: Manipulation, Navigation, and Control\n",
    "- Autonomous Driving: Perception, Planning, and Control\n",
    "- Games: Atari, Mujoco, and other environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Latent Representation Learning**\n",
    "CNN-based encoder maps raw images $x_t$ into a low-dimensional latent vector $z_t$. The policy is trained using latent-space trajectories, reducing the burden of raw pixel learning.\n",
    "- **Recurrent State Space Model (RSSM)**\n",
    "Uses a stichastic latent state $z_t$ and a deterministic recurrent state $h_t$ to predict future states, handles uncertainty and long-term dependencies using GRUs/LSTM.\n",
    "- **Policy Optimization in Latent Space**\n",
    "Instead of acting directly on images, the policy and value function operate in the latent state space. This significantly improves sample efficicency and reduces policy overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent-Space encoding\n",
    "At each timestep t, the image observation $x_t$ is encoded into a latent vector $z_t$:\n",
    "$$z_t = CNNEncoder(x_t)$$\n",
    "The encoder is trained to capture the relevant information in the image, such as object positions, colors, and textures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RSSM Latent Transition\n",
    "The deterministic and stochastic state updates:\n",
    "$$h_t =f(h_{t-1}, z_{t-1}, a_{t-1})$$\n",
    "$$z_t \\sim q(z_t|h_t,x_t)$$\n",
    "The deterministic state $h_t$ is updated using the previous state $h_{t-1}$, the latent vector $z_{t-1}$, and the action $a_{t-1}$. The stochastic state $z_t$ is sampled from the latent transition model $q(z_t|h_t,x_t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imagination-Based Policy Learning\n",
    "Instead of learning from real trajectories, DreamerV2 learns from imagined rollouts within the latent space. The actor-critic is optimized using imagined latent sequences:\n",
    "$$L_{actor} = - \\mathbb{E}[\\sum_t V(z_t)]$$\n",
    "$$L_{critic} = \\mathbb{E}[(V(z_t)-(r_t + \\gamma V(z_{t+1})))^2]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, image_shape, latent_dim):\n",
    "        \"\"\"\n",
    "        Convolutional encoder for pixel-based DreamerV2.\n",
    "        \n",
    "        Args:\n",
    "            image_shape (tuple): Expected image shape (Channels, Height, Width).\n",
    "            latent_dim (int): Latent space dimension.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Handle input shape (C, H, W) vs (H, W, C)\n",
    "        if len(image_shape) == 3 and image_shape[0] in [1, 3]:  \n",
    "            self.input_shape = image_shape  \n",
    "        elif len(image_shape) == 3:  \n",
    "            self.input_shape = (image_shape[2], image_shape[0], image_shape[1])\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid image_shape: {image_shape}\")\n",
    "\n",
    "        # Compute output feature size\n",
    "        dummy_input = torch.zeros(1, *self.input_shape)\n",
    "        with torch.no_grad():\n",
    "            dummy_output = self.encoder(dummy_input)\n",
    "        self.fc = nn.Linear(dummy_output.view(1, -1).shape[1], latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x / 255.0  # Normalize input images\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten feature maps\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSSM(nn.Module):\n",
    "    def __init__(self, latent_dim, action_dim):\n",
    "        \"\"\"\n",
    "        Recurrent State-Space Model (RSSM) for DreamerV2.\n",
    "\n",
    "        Args:\n",
    "            latent_dim (int): Latent state size.\n",
    "            action_dim (int): Number of action dimensions.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRUCell(latent_dim + action_dim, latent_dim)\n",
    "        self.mu_layer = nn.Linear(latent_dim, latent_dim)\n",
    "        self.logvar_layer = nn.Linear(latent_dim, latent_dim)\n",
    "\n",
    "    def forward(self, h, z, a):\n",
    "        \"\"\"\n",
    "        RSSM forward pass: Processes hidden state, latent state, and action.\n",
    "\n",
    "        Args:\n",
    "            h (torch.Tensor): Previous deterministic hidden state (batch_size, latent_dim).\n",
    "            z (torch.Tensor): Previous stochastic latent state (batch_size, latent_dim).\n",
    "            a (torch.Tensor): Action taken (batch_size, action_dim) or (action_dim,).\n",
    "\n",
    "        Returns:\n",
    "            h_next, z_next, mu, logvar\n",
    "        \"\"\"\n",
    "        # Ensure `a` has batch dimension\n",
    "        if a.dim() == 1:\n",
    "            a = a.unsqueeze(0)  # Convert from (action_dim,) → (1, action_dim)\n",
    "        \n",
    "        # Ensure a matches batch size of z\n",
    "        if a.shape[0] != z.shape[0]:\n",
    "            a = a.expand(z.shape[0], -1)  # Expand to match batch size\n",
    "        \n",
    "        h_next = self.gru(torch.cat([z, a], dim=-1), h)  # Ensure correct shape\n",
    "        mu, logvar = self.mu_layer(h_next), self.logvar_layer(h_next)\n",
    "        z_next = mu + torch.exp(0.5 * logvar) * torch.randn_like(mu)\n",
    "\n",
    "        return h_next, z_next, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, latent_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, action_dim), nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntrinsicReward(nn.Module):\n",
    "    def __init__(self, latent_dim, intrinsic_scale=0.1):\n",
    "        \"\"\"\n",
    "        Computes intrinsic rewards based on prediction error in latent space.\n",
    "\n",
    "        Args:\n",
    "            latent_dim (int): Size of latent space.\n",
    "            intrinsic_scale (float): Scale factor for intrinsic reward.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.intrinsic_scale = intrinsic_scale\n",
    "        self.fc = nn.Linear(latent_dim, 1)  # Correct input size: latent_dim → 1\n",
    "\n",
    "    def forward(self, z_pred, z_next):\n",
    "        \"\"\"\n",
    "        Computes intrinsic reward from latent state prediction error.\n",
    "\n",
    "        Args:\n",
    "            z_pred (torch.Tensor): Predicted latent state, shape (batch_size, latent_dim).\n",
    "            z_next (torch.Tensor): True latent state, shape (batch_size, latent_dim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Intrinsic reward, shape (batch_size, 1).\n",
    "        \"\"\"\n",
    "        prediction_error = ((z_next - z_pred) ** 2).mean(dim=-1, keepdim=True)  # Keep (batch_size, 1)\n",
    "        intrinsic_reward = self.fc(prediction_error)  # Ensure correct shape\n",
    "        return self.intrinsic_scale * intrinsic_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DreamerV2PixelAgent:\n",
    "    def __init__(self, image_shape, action_dim, latent_dim=32, intrinsic_scale=0.1):\n",
    "        self.encoder = ConvEncoder(image_shape, latent_dim).to(device)\n",
    "        self.rssm = RSSM(latent_dim, action_dim).to(device)\n",
    "        self.actor = Actor(latent_dim, action_dim).to(device)\n",
    "        self.critic = Critic(latent_dim).to(device)\n",
    "        self.intrinsic_reward = IntrinsicReward(latent_dim, intrinsic_scale).to(device)\n",
    "\n",
    "        self.optim_actor = optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "        self.optim_critic = optim.Adam(self.critic.parameters(), lr=3e-4)\n",
    "\n",
    "    def train(self, obs_seq, action_seq, reward_seq):\n",
    "        obs_seq = [torch.tensor(obs, dtype=torch.float32, device=device).permute(2, 0, 1).unsqueeze(0)\n",
    "               if isinstance(obs, np.ndarray) else obs for obs in obs_seq]\n",
    "\n",
    "        z = self.encoder(obs_seq[0])\n",
    "        h = torch.zeros_like(z).to(device)\n",
    "\n",
    "        zs, hs, rewards = [], [], []\n",
    "        for t in range(len(action_seq)):\n",
    "            a = action_seq[t]\n",
    "            if isinstance(a, torch.Tensor):\n",
    "                a = a.to(device)\n",
    "            else:\n",
    "                a = torch.tensor(a, dtype=torch.float32, device=device).unsqueeze(0)  # Convert and add batch dim\n",
    "            \n",
    "            h, z_pred, mu, logvar = self.rssm(h, z, a)\n",
    "            z_next = self.encoder(obs_seq[t + 1]) if isinstance(obs_seq[t + 1], torch.Tensor) else self.encoder(\n",
    "                torch.tensor(obs_seq[t + 1], dtype=torch.float32, device=device).permute(2, 0, 1).unsqueeze(0))\n",
    "            intrinsic_reward = self.intrinsic_reward(z_pred, z_next)\n",
    "\n",
    "            total_reward = reward_seq[t] + intrinsic_reward\n",
    "            zs.append(z_next)\n",
    "            hs.append(h)\n",
    "            rewards.append(total_reward)\n",
    "\n",
    "        values = [self.critic(z) for z in zs]\n",
    "        targets = [rewards[i] + 0.99 * values[i+1].detach() if i+1 < len(values) else rewards[i]\n",
    "                   for i in range(len(rewards))]\n",
    "\n",
    "        value_loss = sum((values[i] - targets[i]).pow(2).mean() for i in range(len(values)))\n",
    "        actor_loss = -sum(self.critic(z).mean() for z in zs)\n",
    "\n",
    "        self.optim_actor.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.optim_actor.step()\n",
    "\n",
    "        self.optim_critic.zero_grad()\n",
    "        value_loss.backward()\n",
    "        self.optim_critic.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x1 and 32x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m     obs \u001b[38;5;241m=\u001b[39m next_obs\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done: \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_rewards\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m rewards\u001b[38;5;241m.\u001b[39mappend(ep_reward)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[64], line 30\u001b[0m, in \u001b[0;36mDreamerV2PixelAgent.train\u001b[1;34m(self, obs_seq, action_seq, reward_seq)\u001b[0m\n\u001b[0;32m     27\u001b[0m h, z_pred, mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrssm(h, z, a)\n\u001b[0;32m     28\u001b[0m z_next \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(obs_seq[t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obs_seq[t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m], torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m     29\u001b[0m     torch\u001b[38;5;241m.\u001b[39mtensor(obs_seq[t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m---> 30\u001b[0m intrinsic_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintrinsic_reward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_next\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m reward_seq[t] \u001b[38;5;241m+\u001b[39m intrinsic_reward\n\u001b[0;32m     33\u001b[0m zs\u001b[38;5;241m.\u001b[39mappend(z_next)\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\mlagents\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\mlagents\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[63], line 26\u001b[0m, in \u001b[0;36mIntrinsicReward.forward\u001b[1;34m(self, z_pred, z_next)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03mComputes intrinsic reward from latent state prediction error.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m    torch.Tensor: Intrinsic reward, shape (batch_size, 1).\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     25\u001b[0m prediction_error \u001b[38;5;241m=\u001b[39m ((z_next \u001b[38;5;241m-\u001b[39m z_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Keep (batch_size, 1)\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m intrinsic_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction_error\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Ensure correct shape\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintrinsic_scale \u001b[38;5;241m*\u001b[39m intrinsic_reward\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\mlagents\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\mlagents\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\mlagents\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x1 and 32x1)"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CarRacing-v3\")\n",
    "obs_dim = env.observation_space.shape\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "agent = DreamerV2PixelAgent(obs_dim, action_dim)\n",
    "\n",
    "episodes = 200\n",
    "rewards = []\n",
    "\n",
    "for ep in range(episodes):\n",
    "    obs, _ = env.reset()\n",
    "    ep_reward = 0\n",
    "    real_obs, real_actions, real_rewards = [], [], []\n",
    "\n",
    "    for step in range(1000):\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32).permute(2,0,1).unsqueeze(0).to(device)\n",
    "\n",
    "        action = agent.actor(agent.encoder(obs_tensor)).cpu().detach().numpy()[0]\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        real_obs.append(torch.tensor(next_obs, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0).to(device))\n",
    "        real_actions.append(torch.tensor(action, dtype=torch.float32).to(device))\n",
    "        real_rewards.append(torch.tensor([reward], dtype=torch.float32).to(device))\n",
    "\n",
    "        obs = next_obs\n",
    "        if done: break\n",
    "\n",
    "    agent.train(real_obs, real_actions, real_rewards)\n",
    "    rewards.append(ep_reward)\n",
    "    print(f\"Episode: {ep}, Reward: {ep_reward}\")\n",
    "\n",
    "plt.plot(rewards)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
