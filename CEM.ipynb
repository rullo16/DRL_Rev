{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e212555",
   "metadata": {},
   "source": [
    "# Cross-Entropy Method (CEM) for Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe83654",
   "metadata": {},
   "source": [
    "Gradient-free, population-based optimization algorithm used widely in:\n",
    "- planning in latent dynamics models\n",
    "- continuous control without differentiable models\n",
    "- Hig-dimensional policy search\n",
    "CEM is useful in model-based RL:\n",
    "- It works directly with learned models\n",
    "- it optimizes action sequences over a planning horizon\n",
    "- It's simple, parallelizable and highly effective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2751f9f4",
   "metadata": {},
   "source": [
    "The fundamental goal in many control and decision-making problems is:\\\n",
    "Find an action sequence $a_{1:H}$ that maximizes expected return: $a_1{:H}^* = \\arg\\max_{a_{1:H}} \\mathbb{E}[\\sum_{t=1}^H r(s_t, a_t)]$\\\n",
    "This is often non-convex, non-differentiable, or model-based."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4249cd0",
   "metadata": {},
   "source": [
    "Instead of searching for a solution directly, CEM transforms the optimization into a probabilitty distribution optimization problem.\\\n",
    "\"Search over distributions rather than solutions\"\\\n",
    "CEM maintains a parameterized distribution over candidate solutions, samples from it, evaluates those samples, and refines the distribution to increase the likelihood of generating good solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f977cf",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a613ea",
   "metadata": {},
   "source": [
    "Let $p(x;\\theta)$ be a distribution over solutions $x\\in\\mathbb{R}^d$, parameterized by $\\theta$.\\\n",
    "The Cross-Entropy Method solves:\n",
    "$$\\theta^* = \\arg\\max_{\\theta} D_{KL}(\\pi^*(x)||p(x;\\theta))$$\n",
    "Where:\n",
    "- $\\pi^*(x)$ is the target distribution (e.g., the distribution of good solutions)\n",
    "- $D_{KL}$ is the Kullback-Leibler divergence, which measures how one probability distribution diverges from a second expected probability distribution.\n",
    "Since $\\pi^*(x)$ is unknown, we approximate it using elite samples (top K) from the current distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd90e79",
   "metadata": {},
   "source": [
    "## Math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da8ceba",
   "metadata": {},
   "source": [
    "1. Sampling: Draw N samples $x_1, \\ldots, x_N \\sim p(\\cdot;\\theta)$\n",
    "2. Scoring: Evaluate each sample under the objective function: $R_i = f(x_i) = cumulative predicted reward$\n",
    "3. Elite Selection: Let $\\mathcal{E} = {x_i, \\ldots, x_{i_K}}$ be the elite set, top K samples with highest reward.\n",
    "4. Update Distribution Parameters: For Gaussian $\\mathcal{N}(\\mu, \\sigma^2)$, use the elite set to update the mean and variance:\n",
    "$$\\mu' = \\frac{1}{K}\\sum_{i=1}^K x_i$$\n",
    "$$\\sigma'^2 = \\frac{1}{K}\\sum_{i=1}^K(x_i - \\mu')^2$$\n",
    "This update minimizes the KL divergence between the new distribution and the one induced by elites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8230c7a2",
   "metadata": {},
   "source": [
    "CEM originates from rare simulation in Monte Carlo methods. In its original form, it was used to optimize the probability of hitting rare events:\n",
    "$$\\max_x \\mathbb{P}(f(x) > \\gamma)$$\n",
    "It was later repurposed for general optimization, especially for control."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ba836b",
   "metadata": {},
   "source": [
    "In model-based RL planning, we interpret x as an action sequence:\n",
    "$$x=(a_1, a_2, \\ldots, a_H)$$\n",
    "CEM searches over action sequences to maximize predicted future rewards using a learned mdoel $\\hat{T}(s,a)$ and reward predictor $\\hat{r}(s,a)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7d5fff",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "007e4f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best action: tensor([-0.0028,  0.0378])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class DummyModel:\n",
    "    def evaluate_rollout(self, h, s, actions):\n",
    "        return - (actions ** 2).sum(dim=(1,2))\n",
    "\n",
    "class CEMPlanner:\n",
    "    def __init__(self, model, plan_horizon=10, action_dim=2, num_candidates=1000, top_k=100, num_iters=5, action_bounds=(-1.0,1.0)):\n",
    "        self.model = model\n",
    "        self.plan_horizon = plan_horizon\n",
    "        self.action_dim = action_dim\n",
    "        self.num_candidates = num_candidates\n",
    "        self.top_k = top_k\n",
    "        self.num_iters = num_iters\n",
    "        self.action_bounds = action_bounds\n",
    "\n",
    "    def plan(self, h, s):\n",
    "        mean = torch.zeros(self.plan_horizon, self.action_dim)\n",
    "        std = torch.ones_like(mean)*0.5\n",
    "        for _ in range(self.num_iters):\n",
    "            actions = torch.normal(mean.unsqueeze(0).expand(self.num_candidates, -1,-1),\n",
    "                                std.unsqueeze(0).expand(self.num_candidates, -1,-1))\n",
    "            actions = actions.clamp(*self.action_bounds)\n",
    "\n",
    "            returns = self.model.evaluate_rollout(h, s, actions)\n",
    "\n",
    "            elite_inds = returns.topk(self.top_k).indices\n",
    "            elites = actions[elite_inds]\n",
    "\n",
    "            mean = elites.mean(dim=0)\n",
    "            std = elites.std(dim=0) + 1e-5\n",
    "\n",
    "        return mean[0]\n",
    "\n",
    "model = DummyModel()        \n",
    "planner = CEMPlanner(model, plan_horizon=10, action_dim=2)\n",
    "\n",
    "h = torch.randn(1, 200)\n",
    "s = torch.randn(1, 30)\n",
    "\n",
    "best_action = planner.plan(h, s)\n",
    "print(\"Best action:\", best_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f3e7ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "procgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
