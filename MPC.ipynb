{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Predictive Control (MPC) with RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Predictive Control is a planning-based approach where at each time step an agent optimizes a sequence of future actions (over a finite horizon) to maximize expected reward, then executes only the first action of that sequence before re-planning at the next step. This \"plan-at-every-step\" strategy allows the agent to use new state feedback at each step, mitigating errors from imperfect models. MPC requires a dynamic model of the environment to predict how actions will affect future states and rewards.\n",
    "Instead of learning a policy directly, MPC optimizes actions online using a learned dynamics model. It is used in self-driving cars, robots, and drones. It solves the long-term planning problems of RL by actively searching for optimal actions at each timestep.\n",
    "Instead of training an actor network, MPC searches for the best action at each step by solving:\n",
    "$$a_t^*,a_{t+1}^*,...,a_{t+H}^* = \\arg\\max_{a_t,a_{t+1},...,a_{t+H}} \\sum_{i=0}^{H}\\gamma^i R(s_{t+i},a_{t+i})$$\n",
    "where:\n",
    " - H is the planning horizon\n",
    " - $R(s,a)$ is the reward function\n",
    " - $\\gamma$ is the discount factor\n",
    "\n",
    "MPC differs from MBPO in that it optimizes at each step instead of using a policy network, predicts future states online instead of generic synthetic rollouts, runs online at each step instead of training offline, and works best in structured environments instead of high-dimensional action spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Shooting\n",
    "Simplest MPC planning method, samples many random action sequences, simulates each sequence with the model to compute total reward, and chooses the best sequence's first action. This is easy to implement and parallelize, but can be inefficientâ€”wastes samples on poor sequences and may require a very large number of samples to find a good plan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class MPCRandomShooting:\n",
    "    def __init__(self, model, action_dim, horizon=5, num_samples=500, discount=0.99):\n",
    "        self.model = model\n",
    "        self.action_dim = action_dim\n",
    "        self.horizon = horizon\n",
    "        self.num_samples = num_samples\n",
    "        self.discount = discount\n",
    "\n",
    "    def predict_trajectory(self, state, actions):\n",
    "        state = state.clone()\n",
    "        total_rewards = np.zeros(self.num_samples)\n",
    "\n",
    "        for t in range(self.horizon):\n",
    "            action = actions[:, t, :]\n",
    "            next_state, reward = self.model.predict(state, action)\n",
    "            total_rewards += (self.discount ** t) * reward.squeeze().detach().cpu().numpy()\n",
    "            state = next_state\n",
    "\n",
    "        return total_rewards\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = state.unsqueeze(0).repeat(self.num_samples,1)\n",
    "        actions = torch.rand(self.num_samples, self.horizon, self.action_dim) * 2 - 1\n",
    "        rewards = self.predict_trajectory(state, actions)\n",
    "        best_idx = np.argmax(rewards)\n",
    "        return actions[best_idx, 0, :].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class EnsembleDynamicsModel(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, num_models=5, hidden_dim=256):\n",
    "        super(EnsembleDynamicsModel, self).__init__();\n",
    "        self.models = nn.ModuleList([nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, state_dim + 1)\n",
    "        ) for _ in range(num_models)]);\n",
    "        self.num_models = num_models;\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=-1);\n",
    "        outputs = [model(x) for model in self.models];\n",
    "        next_states, rewards = zip(*[(out[..., :-1], out[..., -1:]) for out in outputs]);\n",
    "        return next_states, rewards;\n",
    "\n",
    "    def predict(self, state, action):\n",
    "        model_idx = np.random.randint(self.num_models);\n",
    "        x = torch.cat([state, action], dim=-1);\n",
    "        output = self.models[model_idx](x);\n",
    "        next_state, reward = output[..., :-1], output[..., -1:];\n",
    "        return next_state, reward;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make('HalfCheetah-v5')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "dynamics_model = EnsembleDynamicsModel(state_dim, action_dim)\n",
    "mpc_controller = MPCRandomShooting(dynamics_model, action_dim)\n",
    "\n",
    "num_episodes = 100\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "\n",
    "    for step in range(1000):\n",
    "        action = mpc_controller.select_action(torch.tensor(state, dtype=torch.float32)).squeeze(0)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        done = terminated or truncated\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(f\"Episode {episode+1}, Reward: {episode_reward}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Entropy Method (CEM)\n",
    "Improvement over random shooting, uses a form of iterative refinement. Instead of sampling action sequences from a fixed distribution, CEM updates the sampling distribution to focus on high-performing sequences. In practice, CEM often assumes a parameterized distribution over action sequences. The process works as follows:\n",
    "1. Sample a population of $M$ random action sequences from a proposal distribution $p(A)$\n",
    "2. Evaluate each action sequence by simulating it on the model from the current state and summing the rewards (compute $J(A_i)$ for each sequence $A_i$)\n",
    "3. Select elites, the top $K$ sequences with highest returns.\n",
    "4. Update distribution $p(A)$ by fitting it to these elite samples\n",
    "5. Repeat steps 1-4 until convergence or a fixed number of iterations.\n",
    "6. Output the best sequence found (or the mean of the final elite distribution) as the plan, and execute the first action of this plan.\n",
    "\n",
    "This algorithm concentrates search around promising regions of the action space, making planning more efficient and effective than one-shot random shooting. The advantage of CEM is that it requires far fewer samples to find good action sequences because it iteratively zooms in on high-reward areas rather than repeatedly sampling uniformly at random. CEM is still simple and highly parallelizable, and it has been used successfully in MPC-based RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CEMPlanner:\n",
    "    def __init__(self, act_dim, horizon, pop_size=500, elite_frac=0.1, cem_iterations=4, action_bounds=None, init_mean=None, init_std=None):\n",
    "        self.act_dim = act_dim\n",
    "        self.horizon = horizon\n",
    "        self.pop_size = pop_size\n",
    "        self.elite_num = int(pop_size * elite_frac)\n",
    "        self.cem_iterations = cem_iterations\n",
    "        self.action_bounds = action_bounds\n",
    "\n",
    "        self.init_mean = init_mean if init_mean else np.zeros((horizon, act_dim))\n",
    "        self.init_std = init_std if init_std else np.ones((horizon, act_dim))\n",
    "\n",
    "        self.curr_mean = self.init_mean.copy()\n",
    "        self.curr_std = self.init_std.copy()\n",
    "    \n",
    "    def plan(self, initial_state, model):\n",
    "        mean = self.curr_mean.copy()\n",
    "        std = self.curr_std.copy()\n",
    "        best_return = -np.inf\n",
    "        best_sequence = None\n",
    "\n",
    "        for iter in range(self.cem_iterations):\n",
    "            action_sequences = np.random.normal(loc=mean, scale=std, size=(self.pop_size, self.horizon, self.act_dim))\n",
    "            if self.action_bounds:\n",
    "                low, high = self.action_bounds\n",
    "                action_sequences = np.clip(action_sequences, low, high)\n",
    "\n",
    "            returns = np.zeros(self.pop_size)\n",
    "            for i in range(self.pop_size):\n",
    "                seq = action_sequences[i]\n",
    "                total_reward = 0.0\n",
    "                state = torch.tensor(initial_state, dtype=torch.float32)\n",
    "                for a in seq:\n",
    "                    action = torch.tensor(a, dtype=torch.float32)\n",
    "                    next_state, reward = model.predict(state, action)\n",
    "                    total_reward += reward.item()\n",
    "                    state = next_state\n",
    "\n",
    "                returns[i] = total_reward\n",
    "\n",
    "            elite_indices = returns.argsort()[-self.elite_num:]\n",
    "            elite_seqs = action_sequences[elite_indices]\n",
    "\n",
    "            mean = np.mean(elite_seqs, axis=0)\n",
    "            std = np.std(elite_seqs, axis=0)\n",
    "\n",
    "            max_idx = elite_indices[np.argmax(returns[elite_indices])]\n",
    "            if returns[max_idx] > best_return:\n",
    "                best_return = returns[max_idx]\n",
    "                best_sequence = action_sequences[max_idx]\n",
    "\n",
    "        return best_sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "obs, _ = env.reset()\n",
    "done = False\n",
    "\n",
    "act_dim = env.action_space.shape[0]\n",
    "horizon = 20\n",
    "planner = CEMPlanner(act_dim, horizon, pop_size=500, elite_frac=0.1, cem_iterations=4, action_bounds=(env.action_space.low, env.action_space.high))\n",
    "total_reward = 0.0\n",
    "t = 0\n",
    "while not done:\n",
    "    current_state = obs\n",
    "    plan_sequence = planner.plan(current_state, dynamics_model)\n",
    "    action = plan_sequence[0]\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    done = terminated or truncated\n",
    "    t += 1\n",
    "    if t % horizon == 0:\n",
    "        planner.curr_mean = planner.init_mean.copy()\n",
    "        planner.curr_std = planner.init_std.copy()\n",
    "\n",
    "print(\"Episode finished with total reward: \", total_reward)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison MPC vs MBPO vs SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\" Stores transitions for model training and policy optimization \"\"\"\n",
    "    def __init__(self, max_size=int(1e6)):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        state = np.array(state, dtype=np.float32).flatten()\n",
    "        action = np.array(action, dtype=np.float32).flatten()\n",
    "        next_state = np.array(next_state, dtype=np.float32).flatten()\n",
    "        reward = np.float32(reward)\n",
    "        done = np.float32(done)\n",
    "        \n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        return (torch.tensor(np.stack(states), dtype=torch.float32),\n",
    "                torch.tensor(np.stack(actions), dtype=torch.float32),\n",
    "                torch.tensor(rewards, dtype=torch.float32).unsqueeze(1),\n",
    "                torch.tensor(np.stack(next_states), dtype=torch.float32),\n",
    "                torch.tensor(dones, dtype=torch.float32).unsqueeze(1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MPC with CEM\n",
    "class CEMPlanner:\n",
    "    def __init__(self, act_dim, horizon=5, pop_size=500, elite_frac=0.1, cem_iters=4, action_bounds=None):\n",
    "        self.act_dim = act_dim\n",
    "        self.horizon = horizon\n",
    "        self.pop_size = pop_size\n",
    "        self.elite_num = int(pop_size * elite_frac)\n",
    "        self.cem_iters = cem_iters\n",
    "        self.action_bounds = action_bounds\n",
    "\n",
    "    def plan(self, initial_state, model):\n",
    "        mean = np.zeros((self.horizon, self.act_dim))\n",
    "        std = np.ones((self.horizon, self.act_dim))\n",
    "\n",
    "        for _ in range(self.cem_iters):\n",
    "            actions = np.random.normal(mean, std, (self.pop_size, self.horizon, self.act_dim))\n",
    "            if self.action_bounds:\n",
    "                actions = np.clip(actions, self.action_bounds[0], self.action_bounds[1])\n",
    "\n",
    "            returns = np.zeros(self.pop_size)\n",
    "            for i in range(self.pop_size):\n",
    "                returns[i] = self.evaluate_sequence(initial_state, actions[i], model)\n",
    "            \n",
    "            elite_indices = returns.argsort()[-self.elite_num:]\n",
    "            elite_actions = actions[elite_indices]\n",
    "\n",
    "            mean = elite_actions.mean(axis=0)\n",
    "            std = elite_actions.std(axis=0)\n",
    "\n",
    "        return mean[0]\n",
    "    \n",
    "    def evaluate_sequence(self, state, actions, model):\n",
    "        total_reward = 0.0\n",
    "        for action in actions:\n",
    "            state, reward = model.predict(state, action)\n",
    "            total_reward += reward\n",
    "        return total_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MBPO\n",
    "class DynamicsModel(nn.Module):\n",
    "    def __init___(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(DynamicsModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, state_dim + 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        next_state, reward = x[..., :-1], x[..., -1:]\n",
    "        return next_state, reward\n",
    "\n",
    "    def predict(self, state, action):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "        action_tensor = torch.tensor(action, dtype=torch.float32)\n",
    "        next_state, reward = self(state_tensor, action_tensor)\n",
    "        return next_state.detach().numpy().squeeze(0), reward.detach().numpy().squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_rollouts(model, policy, real_buffer, model_buffer, rollout_length=5, num_rollouts=500):\n",
    "    model.eval()\n",
    "    policy.eval()\n",
    "\n",
    "    states, _,_,_,_ = real_buffer.sample(num_rollouts)\n",
    "    with torch.no_grad():\n",
    "        for state in states:\n",
    "            for _ in range(rollout_length):\n",
    "                action, _ = policy.sample_action(torch.tensor(state, dtype=torch.float32).unsqueeze(0))\n",
    "                next_state, reward = model.predict(state, action)\n",
    "                model_buffer.add(state, action, reward, next_state, False)\n",
    "                state = next_state\n",
    "\n",
    "    model.train()\n",
    "    policy.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action, hidden_dim=256):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.mean = nn.Linear(hidden_dim, action_dim)\n",
    "        self.log_std = nn.Linear(hidden_dim, action_dim)\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mean = self.mean(x)\n",
    "        log_std = torch.clamp(self.log_std(x), -20, 2)\n",
    "        return mean, log_std\n",
    "    \n",
    "    def sample_action(self, state):\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        dist = torch.distributions.Normal(mean, std)\n",
    "        z = dist.rsample()\n",
    "        action = torch.tanh(z) * self.max_action\n",
    "        log_prob = dist.log_prob(z) - torch.log(1 - action.pow(2) + 1e-6)\n",
    "        return action, log_prob.sum(-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MBPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, max_action, model_rollouts=1000):\n",
    "        self.actor = Actor(state_dim, action_dim, max_action)\n",
    "        self.critic = Critic(state_dim, action_dim)\n",
    "        self.critic2 = Critic(state_dim, action_dim)\n",
    "        self.target_critic = Critic(state_dim, action_dim)\n",
    "        self.target_critic2 = Critic(state_dim, action_dim)\n",
    "\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
    "\n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "        self.target_critic2.load_state_dict(self.critic2.state_dict())\n",
    "\n",
    "        self.gamma = 0.99\n",
    "        self.tau = 0.005\n",
    "        self.alpha = 0.2\n",
    "        self.model_rollouts = model_rollouts\n",
    "        self.model = DynamicsModel(state_dim, action_dim)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        action, _ = self.actor.sample_action(state_tensor)\n",
    "        return action.detach().numpy().squeeze(0)\n",
    "    \n",
    "    def train_model(self, real_buffer, model_buffer):\n",
    "        batch_size=256\n",
    "        states, actions, rewards, next_states, dones = model_buffer.sample(batch_size)\n",
    "\n",
    "        predicted_next_states, predicted_rewards = self.model(states, actions)\n",
    "        model_loss = F.mse_loss(predicted_next_states, next_states) + F.mse_loss(predicted_rewards, rewards)\n",
    "\n",
    "        model_optimizer = optim.Adam(self.model.parameters(), lr=3e-4)\n",
    "        model_optimizer.zero_grad()\n",
    "        model_loss.backward()\n",
    "        model_optimizer.step()\n",
    "\n",
    "        generate_model_rollouts(self.model, self.actor, real_buffer, model_buffer, rollout_length=5, num_rollouts=self.model_rollouts)\n",
    "    \n",
    "    def train_sac(self, replay_buffer):\n",
    "        batch_size = 256\n",
    "        states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "        #Compute Q-values:\n",
    "        with torch.no_grad():\n",
    "            next_actions, next_log_probs = self.actor.sample_action(next_states)\n",
    "            q1_next, q2_next = self.target_critic(next_states, next_actions), self.target_critic2(next_states, next_actions)\n",
    "            min_q_next = torch.min(q1_next, q2_next) - self.alpha * next_log_probs\n",
    "            target_q = rewards + self.gamma * (1-dones) * min_q_next\n",
    "\n",
    "        q1, q2 = self.critic(states, actions), self.critic2(states, actions)\n",
    "        critic_loss = F.mse_loss(q1, target_q) + F.mse_loss(q2, target_q)\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        #Update actor\n",
    "        actions, log_probs = self.actor.sample_action(states)\n",
    "        q1_new = self.critic(states, actions)\n",
    "        actor_loss = (self.alpha * log_probs - q1_new).mean()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        for param, target_param in zip(self.critic.parameters(), self.target_critic.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "        for param, target_param in zip(self.critic2.parameters(), self.target_critic2.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAC\n",
    "\n",
    "class SACAgent:\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        self.actor = Actor(state_dim, action_dim, max_action);\n",
    "        self.critic = Critic(state_dim, action_dim);\n",
    "        self.critic2 = Critic(state_dim, action_dim);\n",
    "        self.target_critic = Critic(state_dim, action_dim);\n",
    "        self.target_critic2 = Critic(state_dim, action_dim);\n",
    "\n",
    "        self.target_critic.load_state_dict(self.critic.state_dict());\n",
    "        self.target_critic2.load_state_dict(self.critic2.state_dict());\n",
    "\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4);\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4);\n",
    "\n",
    "        self.gamma = 0.99;\n",
    "        self.tau = 0.005;\n",
    "        self.alpha = 0.2;\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0);\n",
    "        action, _ = self.actor.sample_action(state_tensor);\n",
    "        return action.detach().numpy().squeeze(0);\n",
    "\n",
    "    def train(self, replay_buffer):\n",
    "        batch_size = 256;\n",
    "        states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size);\n",
    "\n",
    "        # Compute Q-values:\n",
    "        with torch.no_grad():\n",
    "            next_actions, next_log_probs = self.actor.sample_action(next_states);\n",
    "            q1_next, q2_next = self.target_critic(next_states, next_actions), self.target_critic2(next_states, next_actions);\n",
    "            min_q_next = torch.min(q1_next, q2_next) - self.alpha * next_log_probs;\n",
    "            target_q = rewards + self.gamma * (1 - dones) * min_q_next;\n",
    "\n",
    "        q1, q2 = self.critic(states, actions), self.critic2(states, actions);\n",
    "        critic_loss = F.mse_loss(q1, target_q) + F.mse_loss(q2, target_q);\n",
    "\n",
    "        self.critic_optimizer.zero_grad();\n",
    "        critic_loss.backward();\n",
    "        self.critic_optimizer.step();\n",
    "\n",
    "        # Update actor\n",
    "        actions, log_probs = self.actor.sample_action(states);\n",
    "        q1_new = self.critic(states, actions);\n",
    "        actor_loss = (self.alpha * log_probs - q1_new).mean();\n",
    "\n",
    "        self.actor_optimizer.zero_grad();\n",
    "        actor_loss.backward();\n",
    "        self.actor_optimizer.step();\n",
    "\n",
    "        for param, target_param in zip(self.critic.parameters(), self.target_critic.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data);\n",
    "\n",
    "        for param, target_param in zip(self.critic2.parameters(), self.target_critic2.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "## Comparison\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('HalfCheetah-v5')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "mpc_controller = CEMPlanner(action_dim, horizon=20, pop_size=500)\n",
    "mbpo_agent = MBPOAgent(state_dim, action_dim, max_action)\n",
    "sac_agent = SACAgent(state_dim, action_dim, max_action)\n",
    "\n",
    "real_buffer = ReplayBuffer(max_size=int(1e6))\n",
    "model_buffer = ReplayBuffer(max_size=int(1e6))\n",
    "\n",
    "num_episodes = 500\n",
    "mpc_rewards, mbpo_rewards, sac_rewards = [], [], []\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    mpc_total, mbpo_total, sac_total = 0.0, 0.0, 0.0\n",
    "\n",
    "    for step in range(1000):\n",
    "        # MPC Action\n",
    "        action_mpc = mpc_controller.plan(state, mbpo_agent.model)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action_mpc)\n",
    "        done = terminated or truncated\n",
    "        mpc_total += reward\n",
    "        state = next_state if not done else env.reset()\n",
    "\n",
    "        #MBPO Action\n",
    "        action_mbpo = mbpo_agent.select_action(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action_mbpo)\n",
    "        done = terminated or truncated\n",
    "        real_buffer.add(state, action_mbpo, reward, next_state, done)\n",
    "        state = next_state if not done else env.reset()\n",
    "\n",
    "        #SAC Action\n",
    "        action_sac = sac_agent.select_action(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action_sac)\n",
    "        done = terminated or truncated\n",
    "        real_buffer.add(state, action_sac, reward, next_state, done)\n",
    "        sac_total += reward\n",
    "        state = next_state if not done else env.reset()\n",
    "\n",
    "        #Train MBPO\n",
    "        if len(real_buffer) > 10000:\n",
    "            mbpo_agent.train_model(real_buffer, model_buffer)\n",
    "            mbpo_agent.train_sac(real_buffer)\n",
    "\n",
    "        #Train SAC\n",
    "        if len(real_buffer) > 10000:\n",
    "            sac_agent.train(real_buffer)\n",
    "\n",
    "    mpc_rewards.append(mpc_total)\n",
    "    mbpo_rewards.append(mbpo_total)\n",
    "    sac_rewards.append(sac_total)\n",
    "\n",
    "    print(f\"Episode {episode+1}, MPC Reward: {mpc_total}, MBPO Reward: {mbpo_total}, SAC Reward: {sac_total}\")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(mpc_rewards, label='MPC')\n",
    "plt.plot(mbpo_rewards, label='MBPO')\n",
    "plt.plot(sac_rewards, label='SAC')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title(\"MPC vs MBPO vs SAC on HalfCheetah-v5\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
