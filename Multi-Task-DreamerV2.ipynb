{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Task DreamerV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional RL models learn one task at a time and struggle to transfer knowledge across different tasks. Multi-Task RL (MTRL) enables a single model to learn and adapt to multiple environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Shared and task-specific components**\n",
    "    Shared models where a common encoder extracts general features across tasks, each task has its own policy and dynamics and knowledge is shared across tasks via latent space representations.\n",
    "- **Multi-Task RL**\n",
    "    Shared encoder learns a task-agnostic latent representation and each task has its own RSSM and actor-critic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Shared Encoder:\n",
    "$$z_t=ConvEncoder(x_t)$$\n",
    "Extract agnostic latent features from the input.\n",
    "- Task-Specific RSSM:\n",
    "$$h_t = f(h_{t-1}, z_{t-1},a_{t-1}, task_id)$$\n",
    "Each task gets its own recurrent dynamics model.\n",
    "- Task-Specific Actor-Critic:\n",
    "$$L_{actor}^{task} = - \\mathbb{E}[V_{task}(z_t)]$$\n",
    "$$L_{critic}^{task} = \\mathbb{E}[(V_{task}(z_t) -(r_t + \\gamma V_{task}(z_{t+1})))^2]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskConvEncoder(nn.Module):\n",
    "    def __init__(self, image_shape, latent_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2), nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        dummy_input = torch.zeros(1, *image_shape).to(device)\n",
    "        with torch.no_grad():\n",
    "            conv_out_size = self.encoder(dummy_input).view(1, -1).shape[1]\n",
    "\n",
    "        self.fc = nn.Linear(conv_out_size, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x / 255.0\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskRSSM(nn.Module):\n",
    "    def __init__(self, latent_dim, action_dim, num_tasks):\n",
    "        super().__init__()\n",
    "        self.task_specific_rssms = nn.ModuleList([RSSM(latent_dim, action_dim) for _ in range(num_tasks)])\n",
    "    def forward(self, h,z,a,task_id):\n",
    "        return self.task_specific_rssms[task_id](h,z,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskActor(nn.Module):\n",
    "    def __init__(self, latent_dim, action_dim, num_tasks):\n",
    "        super().__init__()\n",
    "        self.task_specific_actors = nn.ModuleList([Actor(latent_dim, action_dim) for _ in range(num_tasks)])\n",
    "\n",
    "    def forward(self, z, task_id):\n",
    "        return self.task_specific_actors[task_id](z)\n",
    "\n",
    "class MultiTaskCritic(nn.Module):\n",
    "    def __init__(self, latent_dim, num_tasks):\n",
    "        super().__init__()\n",
    "        self.task_specific_critics = nn.ModuleList([Critic(latent_dim) for _ in range(num_tasks)])\n",
    "\n",
    "    def forward(self, z, task_id):\n",
    "        return self.task_specific_critics[task_id](z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskDreamerV2Agent:\n",
    "    def __init__(self, image_shape, action_dim, num_tasks, latent_dim=32):\n",
    "        self.encoder = MultiTaskConvEncoder(image_shape, latent_dim).to(device)\n",
    "        self.rssm = MultiTaskRSSM(latent_dim, action_dim, num_tasks).to(device)\n",
    "        self.actor = MultiTaskActor(latent_dim, action_dim, num_tasks).to(device)\n",
    "        self.critic = MultiTaskCritic(latent_dim, num_tasks).to(device)\n",
    "\n",
    "        self.optim_actor = optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "        self.optim_critic = optim.Adam(self.critic.parameters(), lr=3e-4)\n",
    "\n",
    "    def train(self, obs_seq, action_seq, reward_seq, task_id):\n",
    "        z = self.encoder(obs_seq[0])\n",
    "        h = torch.zeros_like(z).to(device)\n",
    "\n",
    "        zs, hs, rewards = [], [], []\n",
    "        for t in range(len(action_seq)):\n",
    "            h, z_pred, mu, logvar = self.rssm(h, z, action_seq[t], task_id)\n",
    "            z_next = self.encoder(obs_seq[t + 1])\n",
    "            zs.append(z_next)\n",
    "            hs.append(h)\n",
    "            rewards.append(reward_seq[t])\n",
    "\n",
    "        values = [self.critic(z, task_id) for z in zs]\n",
    "        targets = [rewards[i] + 0.99 * values[i+1].detach() if i+1 < len(values) else rewards[i]\n",
    "                   for i in range(len(rewards))]\n",
    "\n",
    "        value_loss = sum((values[i] - targets[i]).pow(2).mean() for i in range(len(values)))\n",
    "        actor_loss = -sum(self.critic(z, task_id).mean() for z in zs)\n",
    "\n",
    "        self.optim_actor.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.optim_actor.step()\n",
    "\n",
    "        self.optim_critic.zero_grad()\n",
    "        value_loss.backward()\n",
    "        self.optim_critic.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\"CarRacing-v2\", \"Walker2d-v4\"]  # Example tasks\n",
    "num_tasks = len(tasks)\n",
    "\n",
    "envs = [gym.make(task) for task in tasks]\n",
    "image_shapes = [env.observation_space.shape for env in envs]\n",
    "\n",
    "image_shapes = [(shape[2], shape[0], shape[1]) for shape in image_shapes]  # Convert to (C,H,W)\n",
    "\n",
    "agent = MultiTaskDreamerV2Agent(image_shapes[0], action_dim=envs[0].action_space.shape[0], num_tasks=num_tasks)\n",
    "\n",
    "for ep in range(200):\n",
    "    for task_id, env in enumerate(envs):\n",
    "        obs, _ = env.reset()\n",
    "        ep_reward = 0\n",
    "\n",
    "        for step in range(1000):\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32).permute(2,0,1).unsqueeze(0).to(device)\n",
    "            action = agent.actor(agent.encoder(obs_tensor), task_id).cpu().detach().numpy()[0]\n",
    "            next_obs, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            obs = next_obs\n",
    "            ep_reward += reward\n",
    "            if done: break\n",
    "\n",
    "        print(f\"Task {tasks[task_id]} | Episode {ep} | Reward: {ep_reward:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve RSSM using Transformer based models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional GRU are limited in handling long-horizon dependencies, transformers provide better sequence modeling allowing:\n",
    "-Cross-task generalization\n",
    "-More stable latent representations\n",
    "-Better memory for long-term planning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h_t = Transformer(h_{t-1}, z_{t-1},a_{t-1}, task_id)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerRSSM(nn.Module):\n",
    "    def __init__(self, latent_dim, action_dim, num_tasks, num_layers=4, num_heads = 4):\n",
    "        super().__init__()\n",
    "        self.taks_specific_embeddings = nn.Embedding(num_tasks, latent_dim)\n",
    "        self.action_proj = nn.Linear(action_dim, latent_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=latent_dim, nhead=num_heads)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.mu_layer = nn.Linear(latent_dim, latent_dim)\n",
    "        self.logvar_layer = nn.Linear(latent_dim, latent_dim)\n",
    "\n",
    "    def forward(self, h,z,a,task_id):\n",
    "        task_embedding=self.taks_specific_embeddings(torch.tensor([task_id], device=z.device))\n",
    "        a_proj = self.action_proj(a)\n",
    "\n",
    "        transformer_input = torch.cat([z,h,a_proj,task_embedding], dim=-1).unsqueeze(0)\n",
    "        h_next = self.transformer_encoder(transformer_input).squeeze(0)\n",
    "\n",
    "        mu, logvar = self.mu_layer(h_next), self.logvar_layer(h_next)\n",
    "        z_next = mu +torch.exp(0.5*logvar)*torch.randn_like(mu)\n",
    "\n",
    "        return h_next, z_next, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
