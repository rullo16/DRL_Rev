{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Actor-Critic with Autoencoders (SAC-AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network that compresses high-dimensional data into a lower-dimensional latent representation and reconstructs the original data from this compressed form.\n",
    "Components:\n",
    "-Encoder: maps input $x$ to a latent code $z=E_{\\phi}(x)$\n",
    "-Decoder: Reconstruct input $\\hat{x}=D_{\\theta}(z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical formulation\n",
    "Training objective minimizes reconstruction error, typically Mean Squared Error (MSE):\n",
    "$$min_{\\phi,\\theta} E_{x\\sim D}[||x-D_{\\theta}(E_{\\phi}(x))||^2]$$\n",
    "Regularization terms can be added:\n",
    "$$L_{AE}(\\phi,\\theta)=||x-\\hat{x}||^2+\\lambda_{z}||z||^1+\\lambda_{\\theta}||\\theta||^2$$\n",
    "\n",
    "1.**Latent Feature Extraction**\n",
    "Autoencoders learn meaningful, compressed representations, capturing essential data features. High-dimensional inputs (images) er encoded into lower-dimensional latent vectors, preserving essential information.\n",
    "2.**Why Autoencoders help in RL?**\n",
    "RL from raw pixels is sample-inefficient, reward signals alone are sparse. Autoencoders provide rich, dense signals via reconstructions, guiding the network toward stable and meaningful features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretcial Background of SAC-AE\n",
    "Standard SAC directly on pixels is inefficient, requiring massive data. Sparse rewards poorly shape CNN feature extraction, causing slow and unstable training.\n",
    "SAC-AE integrates an autoencoder into SAC, Images ($s_t$) are encoded into latent states ($z_t = E_{\\phi}(s_t)$), which actor/critic networks use. The autoencoder provides additional supervision via reconstruction loss, stabilizing and accelerating training.\n",
    "SAC-AE combines SAC's objective with AE reconstruction loss:\n",
    "$$J_{SAC-AE} = J_{SAC}(\\phi,\\psi,\\theta)+\\beta L_{AE}(\\phi,\\theta_{dec})$$\n",
    "-Critic Loss: minimizes Bellman error using encoded states.\n",
    "-Actor loss: maximizes Q-value of actions and entropy.\n",
    "-Entropy temperature loss: tunes exploration via entropy.\n",
    "-Autoencoder Loss: reconstruction loss ensures latent representation preserves image info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim=50):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3,32,3,2)\n",
    "        self.conv2 = nn.Conv2d(32,32,3,1)\n",
    "        self.conv3 = nn.Conv2d(32,32,3,1)\n",
    "        self.conv4 = nn.Conv2d(32,32,3,1)\n",
    "\n",
    "        self.fc = nn.Linear(32*36*36, latent_dim)\n",
    "        self.ln = nn.LayerNorm(latent_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = self.ln(x)\n",
    "        x = torch.tanh(x)\n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=50):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.fc = nn.Linear(latent_dim, 32*36*36)\n",
    "        \n",
    "        self.deconv1 = nn.ConvTranspose2d(32,32,3,1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(32,32,3,1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(32,32,3,1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(32,3,3,2,output_padding=1)\n",
    "\n",
    "    def forward(self,z):\n",
    "        z = F.relu(self.fc(z))\n",
    "        z = z.view(z.size(0), 32, 36, 36)\n",
    "        z = F.relu(self.deconv1(z))\n",
    "        z = F.relu(self.deconv2(z))\n",
    "        z = F.relu(self.deconv3(z))\n",
    "        z = self.deconv4(z)\n",
    "\n",
    "        recon= torch.tanh(z)\n",
    "        return recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, latent_dim, action_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 1024), nn.ReLU(),\n",
    "            nn.Linear(1024, 1024), nn.ReLU(),\n",
    "            nn.Linear(1024, 2*action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.net(z)\n",
    "        mu, log_std = out.chunk(2, dim=-1) #split the output into two halves\n",
    "        log_std = torch.clamp(log_std, -10, 2)\n",
    "        std = torch.exp(log_std)\n",
    "        return mu, std\n",
    "    \n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, latent_dim, action_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim+action_dim, 1024), nn.ReLU(),\n",
    "            nn.Linear(1024, 1024), nn.ReLU(),\n",
    "            nn.Linear(1024, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, z, a):\n",
    "        z_a = torch.cat([z,a], dim=-1)\n",
    "        q = self.net(z_a)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, image_shape, action_dim):\n",
    "        self.capacity = capacity\n",
    "        self.ptr = 0\n",
    "        self.size =0\n",
    "\n",
    "        self.obs_buf = np.zeros((capacity, *image_shape), dtype=np.float32)\n",
    "        self.next_obs_buf = np.zeros((capacity, *image_shape), dtype=np.float32)\n",
    "        self.acts_buf = np.zeros((capacity, action_dim), dtype=np.float32)\n",
    "        self.rews_buf = np.zeros(capacity, dtype=np.float32)\n",
    "        self.done_buf = np.zeros(capacity, dtype=np.float32)\n",
    "\n",
    "    def add(self, obs, action, reward, next_obs, done):\n",
    "        idx = self.ptr\n",
    "        self.obs_buf[idx] = obs\n",
    "        self.next_obs_buf[idx] = next_obs\n",
    "        self.acts_buf[idx] = action\n",
    "        self.rews_buf[idx] = reward\n",
    "        self.done_buf[idx] = done\n",
    "\n",
    "        self.ptr = (self.ptr+1) % self.capacity\n",
    "        self.size = min(self.size+1, self.capacity)\n",
    "\n",
    "    def sample(self, batch_size=32):\n",
    "        idxs = np.random.choice(self.size, batch_size, replace=False)\n",
    "        return dict(obs=self.obs_buf[idxs],\n",
    "                    next_obs=self.next_obs_buf[idxs],\n",
    "                    acts=self.acts_buf[idxs],\n",
    "                    rews=self.rews_buf[idxs],\n",
    "                    done=self.done_buf[idxs])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x53792 and 41472x50)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[1;32m---> 18\u001b[0m     _, z \u001b[38;5;241m=\u001b[39m \u001b[43mae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     action, _ \u001b[38;5;241m=\u001b[39m actor\u001b[38;5;241m.\u001b[39msample(z)\n\u001b[0;32m     20\u001b[0m     next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\mlagents\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\mlagents\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 24\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv4(x))\n\u001b[0;32m     23\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln(x)\n\u001b[0;32m     26\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(x)\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\mlagents\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\mlagents\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\mlagents\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x53792 and 41472x50)"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CarRacing-v3\")\n",
    "ae = Encoder(50)\n",
    "decoder = Decoder(50)\n",
    "actor = PolicyNetwork(50, env.action_space.shape[0])\n",
    "critic = QNetwork(50, env.action_space.shape[0])\n",
    "buffer = ReplayBuffer(100000, (96,96,3), env.action_space.shape[0])\n",
    "\n",
    "ae_optim = optim.Adam(ae.parameters(), lr=1e-3)\n",
    "actor_optim = optim.Adam(actor.parameters(), lr=3e-4)\n",
    "critic_optim = optim.Adam(critic.parameters(), lr=3e-4)\n",
    "\n",
    "for ep in range(500):\n",
    "    state, _ = env.reset()\n",
    "    state = torch.FloatTensor(state).permute(2,0,1).unsqueeze(0) / 255.\n",
    "    total_reward = 0\n",
    "\n",
    "    for t in range(1000):\n",
    "        _, z = ae(state)\n",
    "        action, _ = actor.sample(z)\n",
    "        next_state, reward, done, _, _ = env.step(action.detach().numpy())\n",
    "        buffer.add(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "\n",
    "        # Train autoencoder\n",
    "        s_batch, _, _, _, _ = buffer.sample(32)\n",
    "        recon, _ = ae(s_batch)\n",
    "        loss_ae = nn.MSELoss()(recon, s_batch)\n",
    "        ae_optim.zero_grad()\n",
    "        loss_ae.backward()\n",
    "        ae_optim.step()\n",
    "\n",
    "        # Train SAC\n",
    "        # (same as standard SAC but using latent states)\n",
    "    \n",
    "    print(f\"Episode {ep+1}: {total_reward}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
