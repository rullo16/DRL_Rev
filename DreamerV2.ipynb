{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DreamerV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced, Model- ased RL algorithm designed for efficient learning by leveraging an internal latent-space world model. Unlike traditional RL methods, which require extensive interactions with real environment, it reduces substantially sample complexity by performing most learning within imagined trajectories.\n",
    "DreamerV2 is then:\n",
    "- **Sample Efficient**: Drastically reduces the required umber of interactions with the real environment.\n",
    "- **High-dimensional**: Can efficiently process raw pixel observations.\n",
    "- **Long-term Planning**: Performs effective long-horizon decision-making by modeling uncertainty explicitly through latent imagination.\n",
    "DreamerV2 addresses Model bias and compounding predicion errors, which are errors that accumulate over time due to the model's inability to perfectly predict the environment, by using a robust latent representation and advanced recurrent architectures to stabilize long-term imagination rollouts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Latent-Space Representations:\n",
    "Compresses high dimensional, redundant observations (like images) into low-dimensional embedding, capturing only essential features necessary for decision-making.\n",
    "\n",
    "2. Recurrent State-Space Models (RSSM):\n",
    "Combine deterministic and stochastic latent variables with recurrent neural networks (GRU, LSTM) to predict future states, thereby handling uncertainty and temporal dependencies effectively.\n",
    "\n",
    "3. Imagination-based RL:\n",
    "Rather than relying exclusively on real interactions, these methods generate synthetic trajectories within a learned model to efficienlty optimize the policy, drastically reducing sample complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DreamerV2 consists of several interacting NN components:\n",
    "1. **Representation Model**(Encoder): Encodes observations $x_t$ into latent variables $z_t$.\n",
    "2. Transition Model (RSSM): Predicts future latent states, consisting of:\n",
    "    - **Deterministic hidden state**: $h_t$.\n",
    "    - **Stochastic latent variable**: $z_t$.\n",
    "3. **Reward Model**: Predicts immediate rewards from latent states.\n",
    "4. **Policy Network (Actor)**: Optimizes actions within the latent imagination.\n",
    "5. **Value Network (Critic)**: Predicts expected cumulative returns from latent states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main training loop includes:\n",
    "1. World Model Learning: Encoding observations and learning latent dynbamics and rewards.\n",
    "2. Imagination-based Planning: Generating imagined future rollouts and optimizing the policy network based\n",
    "3. Real Environment Interaction: Deploying the policy to collect real observations to update the world model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Observations: $x_t$, Actions: $a_t$, Rewards: $r_t$,\n",
    "- Latent states:\n",
    "    - Deterministic hidden state: $h_t$.\n",
    "    - Stochastic latent state: $z_t$.\n",
    "- RSSM parameters: $\\theta$, Encoder parameters: $\\phi$, Actor parameters: $\\psi$, Critic parameters: $\\xi$.\n",
    "- Discount Factor: $\\gamma$, Learning Rate: $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RSSM Dynamics\n",
    "Explicitly models uncertainty via stochastic latent variables and maintains temporal context via a recurrent hidden state:\n",
    "$$h_t = f_\\theta(h_{t-1}, z_{t-1}, a_{t-1})$$\n",
    "Stochastic latent variables are inferred and sampled as:\n",
    "- Prior $p_\\theta(z_t|h_t)$\n",
    "- Posterior(approx.): $q_\\phi(z_t|h_t, x_t)$\n",
    "Thus, the latent state at timestep t is:\n",
    "$$s_t = (h_t, z_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### World Model Loss (Variational Lower Bound)\n",
    "\n",
    "The world model is optimized by maximizing the **Evidence Lower Bound** (ELBO) of observations:\n",
    "$$L_{model}(\\theta, \\phi) = \\mathbb{E}_{q_\\phi(z_{\\leq T}| x_{\\leq T},a_{\\leq T})} [\\sum_{t=1}^{T} \\log p_\\theta(x_t| h_t,z_t) - KL(q_\\pi(z_t|h_t, x_t)||p_\\theta(z_t|h_t))]$$\n",
    "Where:\n",
    "- $log p_\\theta(x_t| h_t,z_t)$: Reconstruction likelihood encourages accurate latent representation.\n",
    "- KL term: Regularizes posterior to remain close to prior, balancing representation accuracy with regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Value Prediction\n",
    "DreamerV2 learns separate networks for reward and value estimation:\n",
    "- Reward Model: Predicts immediate rewards from latent states.\n",
    "$$r_t \\sim \\hat{r}_t = r_\\eta(h_t, z_t)$$\n",
    "- Value Model(Critic): estimates expected cumulative returns:\n",
    "$$V_\\xi(h_t, z_t) \\approx \\mathbb{E}[\\sum_{\\tau = t}^{T} \\gamma^{\\tau - t} \\hat{r}_\\tau]$$\n",
    "The critic is trained using temporal difference learning within imagined trajectories:\n",
    "$$L_{value}(\\xi) = \\mathbb{E}_{p_\\theta(z_{\\leq T}| x_{\\leq T},a_{\\leq T})} [\\sum_{t}(V_\\xi(s_t) -(r_\\eta(s_t,a_t) + \\gamma V_\\xi(s_{t+1}))^2]$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Optimization in Imagination (Actor Loss)\n",
    "The policy is optimized entirely via imagined rollouts. The objective is maximising the rpedicted cumulative rewards:\n",
    "$$ L_{actor}(\\phi) = - \\mathbb{E}_{p_\\theta(s_{> T}| s_t, pi_\\psi)} [\\sum_{t} V_\\xi(s_t)]$$\n",
    "the maximizes expected future returns in the imagined trajectories, leading to improved real-world policy performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representation Model\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, obs_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 128), nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RSSM Dynamics (Transition Model)\n",
    "class RSSM(nn.Module):\n",
    "    def __init__(self, latent_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRUCell(latent_dim+action_dim, latent_dim)\n",
    "        self.latent_mu = nn.Linear(latent_dim, latent_dim)\n",
    "        self.latent_logvar = nn.Linear(latent_dim, latent_dim)\n",
    "\n",
    "    def forward(self, h,z,a):\n",
    "        x = torch.cat([z,a],dim=-1)\n",
    "        h_next = self.rnn(x,h)\n",
    "        mu, logvar = self.latent_mu(h_next), self.latent_logvar(h_next)\n",
    "        z_next = mu + torch.exp(logvar * 0.5) * torch.randn_like(mu)\n",
    "        return h_next, z_next, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor, Critic, Reward Nets\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, latent_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 128), nn.ReLU(),\n",
    "            nn.Linear(128, action_dim), nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "    \n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DreamerV2 Agent\n",
    "\n",
    "class DreamerAgent:\n",
    "    def __init__(self, obs_dim, action_dim, latent_dim = 32):\n",
    "        self.encoder = Encoder(obs_dim, latent_dim).to(device)\n",
    "        self.rssm = RSSM(latent_dim, action_dim).to(device)\n",
    "        self.actor = Actor(latent_dim, action_dim).to(device)\n",
    "        self.critic = Critic(latent_dim).to(device)\n",
    "        self.reward_model = RewardModel(latent_dim).to(device)\n",
    "        \n",
    "        self.optim_model = optim.Adam(list(self.encoder.parameters()) + list(self.rssm.parameters()) + list(self.reward_model.parameters()), lr=3e-4)\n",
    "        self.optim_actor = optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "        self.optim_critic = optim.Adam(self.critic.parameters(), lr=3e-4)\n",
    "\n",
    "    def imagine(self, z,h, horizon=15):\n",
    "        zs, hs, rewards = [], [], []\n",
    "        for _ in range(horizon):\n",
    "            a = self.actor(z)\n",
    "            h,z, _,_ = self.rssm(h,z,a)\n",
    "            r = self.reward_model(z)\n",
    "            zs.append(z), hs.append(h), rewards.append(r)\n",
    "        return zs, hs, rewards\n",
    "    \n",
    "    def train(self, real_obs, real_actions, real_rewards):\n",
    "        z = self.encoder(real_obs[0])\n",
    "        h = torch.zeros_like(z)\n",
    "\n",
    "        # World Model Learning\n",
    "        recon_loss = 0\n",
    "        for t in range(len(real_actions)):\n",
    "            h, z, mu, logvar = self.rssm(h, z, real_actions[t])\n",
    "            reward_pred = self.reward_model(z)\n",
    "            recon_loss += (reward_pred - real_rewards[t]).pow(2).mean() + -0.5 * (1 + logvar - mu.pow(2) - logvar.exp()).mean()\n",
    "\n",
    "        self.optim_model.zero_grad()\n",
    "        recon_loss.backward()\n",
    "        self.optim_model.step()\n",
    "\n",
    "        # Imagined Trajectories for policy and value\n",
    "        with torch.no_grad():\n",
    "            zs, hs, rewards = self.imagine(z.detach(), h.detach())\n",
    "        \n",
    "        values = [self.critic(z.detach()) for z in zs]\n",
    "\n",
    "        targets = []\n",
    "        for i in range(len(rewards)):\n",
    "            reward = rewards[i].detach()\n",
    "            if i+1 < len(values):\n",
    "                target = reward + 0.99 * values[i+1].detach()\n",
    "            else:\n",
    "                target = reward\n",
    "            targets.append(target)\n",
    "\n",
    "        #Critic Update\n",
    "        value_loss = sum((values[i] - targets[i]).pow(2).mean() for i in range(len(values)))\n",
    "        self.optim_critic.zero_grad()\n",
    "        value_loss.backward()\n",
    "        self.optim_critic.step()\n",
    "\n",
    "        #Actor Update\n",
    "        zs_actor, _, _ = self.imagine(z.detach(), h.detach())\n",
    "        actor_loss = -sum(self.critic(z).mean() for z in zs_actor)\n",
    "        self.optim_actor.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.optim_actor.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Reward: -2.0546528784648523\n",
      "Episode: 1, Reward: -3.7437222324707076\n",
      "Episode: 2, Reward: -1.1145175284066375\n",
      "Episode: 3, Reward: -2.5700051245964874\n",
      "Episode: 4, Reward: -3.649885728377622\n",
      "Episode: 5, Reward: -2.655493199143356\n",
      "Episode: 6, Reward: -2.174071936989553\n",
      "Episode: 7, Reward: -1.7237313311098417\n",
      "Episode: 8, Reward: -4.548262665787252\n",
      "Episode: 9, Reward: -3.2693187720011814\n",
      "Episode: 10, Reward: -3.641746772295978\n",
      "Episode: 11, Reward: -2.614426024539506\n",
      "Episode: 12, Reward: -2.885255983690158\n",
      "Episode: 13, Reward: -4.007152626706676\n",
      "Episode: 14, Reward: -4.578455800204929\n",
      "Episode: 15, Reward: -4.039986062940572\n",
      "Episode: 16, Reward: -4.548049311238123\n",
      "Episode: 17, Reward: -4.616050120208955\n",
      "Episode: 18, Reward: -5.253468909287032\n",
      "Episode: 19, Reward: -6.533259316608662\n",
      "Episode: 20, Reward: -5.333492748171143\n",
      "Episode: 21, Reward: -5.516887088015152\n",
      "Episode: 22, Reward: -5.79349307084449\n",
      "Episode: 23, Reward: -5.799521595055809\n",
      "Episode: 24, Reward: -5.621189570891531\n",
      "Episode: 25, Reward: -7.485862245706652\n",
      "Episode: 26, Reward: -7.6231742210020705\n",
      "Episode: 27, Reward: -7.7490232162737325\n",
      "Episode: 28, Reward: -7.2109685390000156\n",
      "Episode: 29, Reward: -9.140238901261636\n",
      "Episode: 30, Reward: -8.92618663199691\n",
      "Episode: 31, Reward: -8.217835481352024\n",
      "Episode: 32, Reward: -9.424664838838373\n",
      "Episode: 33, Reward: -11.594452740504586\n",
      "Episode: 34, Reward: -10.627826091399085\n",
      "Episode: 35, Reward: -11.711627692721692\n",
      "Episode: 36, Reward: -10.774827333496946\n",
      "Episode: 37, Reward: -11.746177279584899\n",
      "Episode: 38, Reward: -11.40518528447108\n",
      "Episode: 39, Reward: -12.628005743490425\n",
      "Episode: 40, Reward: -13.238579997822074\n",
      "Episode: 41, Reward: -12.709261904485201\n",
      "Episode: 42, Reward: -13.582366896009107\n",
      "Episode: 43, Reward: -13.238350006163593\n",
      "Episode: 44, Reward: -14.49784828889299\n",
      "Episode: 45, Reward: -14.450112160156998\n",
      "Episode: 46, Reward: -14.726773535895328\n",
      "Episode: 47, Reward: -16.410127295668126\n",
      "Episode: 48, Reward: -16.338236062887656\n",
      "Episode: 49, Reward: -15.346756434944474\n",
      "Episode: 50, Reward: -15.311882136011064\n",
      "Episode: 51, Reward: -16.277242489955334\n",
      "Episode: 52, Reward: -16.89321632652777\n",
      "Episode: 53, Reward: -17.156641187337666\n",
      "Episode: 54, Reward: -17.45405048554136\n",
      "Episode: 55, Reward: -17.401303955141643\n",
      "Episode: 56, Reward: -18.82162924836919\n",
      "Episode: 57, Reward: -20.564096273321375\n",
      "Episode: 58, Reward: -20.169774597232113\n",
      "Episode: 59, Reward: -19.8823081393109\n",
      "Episode: 60, Reward: -20.269320868719017\n",
      "Episode: 61, Reward: -21.560373961411155\n",
      "Episode: 62, Reward: -22.14533641617584\n",
      "Episode: 63, Reward: -22.33006903690722\n",
      "Episode: 64, Reward: -23.621851197452294\n",
      "Episode: 65, Reward: -22.507683693344585\n",
      "Episode: 66, Reward: -23.901174794383895\n",
      "Episode: 67, Reward: -23.824759197711515\n",
      "Episode: 68, Reward: -24.026799268306586\n",
      "Episode: 69, Reward: -23.52388390392701\n",
      "Episode: 70, Reward: -25.275193857368258\n",
      "Episode: 71, Reward: -25.37397431095204\n",
      "Episode: 72, Reward: -26.83825583069104\n",
      "Episode: 73, Reward: -27.214183920036284\n",
      "Episode: 74, Reward: -26.523156970930746\n",
      "Episode: 75, Reward: -28.193087226608885\n",
      "Episode: 76, Reward: -29.067534564677164\n",
      "Episode: 77, Reward: -28.495489598542328\n",
      "Episode: 78, Reward: -29.11997549390519\n",
      "Episode: 79, Reward: -30.437638105583755\n",
      "Episode: 80, Reward: -29.92780748020606\n",
      "Episode: 81, Reward: -30.28163871597292\n",
      "Episode: 82, Reward: -30.823914725163057\n",
      "Episode: 83, Reward: -32.249338043173026\n",
      "Episode: 84, Reward: -32.9221602421407\n",
      "Episode: 85, Reward: -32.56580651648526\n",
      "Episode: 86, Reward: -33.3776082855555\n",
      "Episode: 87, Reward: -33.186037919311815\n",
      "Episode: 88, Reward: -36.96258137707692\n",
      "Episode: 89, Reward: -36.40076738399228\n",
      "Episode: 90, Reward: -35.91012113044008\n",
      "Episode: 91, Reward: -36.41328962444294\n",
      "Episode: 92, Reward: -36.12502531023328\n",
      "Episode: 93, Reward: -37.51550481011095\n",
      "Episode: 94, Reward: -38.336714359359355\n",
      "Episode: 95, Reward: -40.8426253528594\n",
      "Episode: 96, Reward: -41.45837772245047\n",
      "Episode: 97, Reward: -40.18538800380779\n",
      "Episode: 98, Reward: -41.228365884860324\n",
      "Episode: 99, Reward: -42.19084867408178\n",
      "Episode: 100, Reward: -41.986614362939484\n",
      "Episode: 101, Reward: -42.650162529211414\n",
      "Episode: 102, Reward: -47.89435659856169\n",
      "Episode: 103, Reward: -45.02838946877505\n",
      "Episode: 104, Reward: -48.55881168802588\n",
      "Episode: 105, Reward: -45.61167003414258\n",
      "Episode: 106, Reward: -48.96476381448851\n",
      "Episode: 107, Reward: -51.15275901580244\n",
      "Episode: 108, Reward: -51.92565051838987\n",
      "Episode: 109, Reward: -50.91951166470509\n",
      "Episode: 110, Reward: -54.09428471365921\n",
      "Episode: 111, Reward: -48.70904774803955\n",
      "Episode: 112, Reward: -49.11110140327748\n",
      "Episode: 113, Reward: -55.58215599675155\n",
      "Episode: 114, Reward: -55.127824939136886\n",
      "Episode: 115, Reward: -53.65340567529609\n",
      "Episode: 116, Reward: -57.23916063916613\n",
      "Episode: 117, Reward: -57.86241205293129\n",
      "Episode: 118, Reward: -58.65995426020034\n",
      "Episode: 119, Reward: -59.370750753746265\n",
      "Episode: 120, Reward: -60.03124908196894\n",
      "Episode: 121, Reward: -64.46861753855032\n",
      "Episode: 122, Reward: -62.14805781191364\n",
      "Episode: 123, Reward: -63.676821327681715\n",
      "Episode: 124, Reward: -67.08232617662813\n",
      "Episode: 125, Reward: -69.09714893062292\n",
      "Episode: 126, Reward: -70.98185092034898\n",
      "Episode: 127, Reward: -72.66539477751203\n",
      "Episode: 128, Reward: -74.32758001111996\n",
      "Episode: 129, Reward: -76.46733103750422\n",
      "Episode: 130, Reward: -75.68383903913258\n",
      "Episode: 131, Reward: -82.21025615424132\n",
      "Episode: 132, Reward: -85.00028364830361\n",
      "Episode: 133, Reward: -85.86983384524066\n",
      "Episode: 134, Reward: -88.97617132233792\n",
      "Episode: 135, Reward: -91.9325357850493\n",
      "Episode: 136, Reward: -93.97460730321791\n",
      "Episode: 137, Reward: -97.1091419658026\n",
      "Episode: 138, Reward: -101.66472288106083\n",
      "Episode: 139, Reward: -104.85778895576455\n",
      "Episode: 140, Reward: -107.01864228077865\n",
      "Episode: 141, Reward: -111.1062855571215\n",
      "Episode: 142, Reward: -115.96156081662348\n",
      "Episode: 143, Reward: -118.63846872014169\n",
      "Episode: 144, Reward: -121.92386301338496\n",
      "Episode: 145, Reward: -126.65622575687327\n",
      "Episode: 146, Reward: -129.80104484248184\n",
      "Episode: 147, Reward: -131.7747662719496\n",
      "Episode: 148, Reward: -137.69581927014713\n",
      "Episode: 149, Reward: -141.84964632227087\n",
      "Episode: 150, Reward: -144.99286944621218\n",
      "Episode: 151, Reward: -150.8629708362251\n",
      "Episode: 152, Reward: -157.6102889185405\n",
      "Episode: 153, Reward: -161.81349819169426\n",
      "Episode: 154, Reward: -166.67807107143653\n",
      "Episode: 155, Reward: -171.63799915452304\n",
      "Episode: 156, Reward: -174.9301207076663\n",
      "Episode: 157, Reward: -174.72925302706517\n",
      "Episode: 158, Reward: -179.19967489320317\n",
      "Episode: 159, Reward: -184.85481937172887\n",
      "Episode: 160, Reward: -188.05919279231819\n",
      "Episode: 161, Reward: -189.06444422216202\n",
      "Episode: 162, Reward: -191.9167713590834\n",
      "Episode: 163, Reward: -191.98056881825286\n",
      "Episode: 164, Reward: -195.53554372411688\n",
      "Episode: 165, Reward: -197.96539735308724\n",
      "Episode: 166, Reward: -199.22330585513598\n",
      "Episode: 167, Reward: -203.80011838490861\n",
      "Episode: 168, Reward: -207.80182547967513\n",
      "Episode: 169, Reward: -210.91372778085943\n",
      "Episode: 170, Reward: -213.06823925369608\n",
      "Episode: 171, Reward: -215.04723515068042\n",
      "Episode: 172, Reward: -217.13714892119154\n",
      "Episode: 173, Reward: -219.8413107510864\n",
      "Episode: 174, Reward: -221.55349223247725\n",
      "Episode: 175, Reward: -225.59394418806994\n",
      "Episode: 176, Reward: -227.51075581279554\n",
      "Episode: 177, Reward: -231.5103009386392\n",
      "Episode: 178, Reward: -234.52728589047402\n",
      "Episode: 179, Reward: -237.3869868007843\n",
      "Episode: 180, Reward: -238.9312606998921\n",
      "Episode: 181, Reward: -242.428700888318\n",
      "Episode: 182, Reward: -244.56983831804658\n",
      "Episode: 183, Reward: -249.63200261016533\n",
      "Episode: 184, Reward: -253.0536714917927\n",
      "Episode: 185, Reward: -255.14620105040757\n",
      "Episode: 186, Reward: -261.5052499559431\n",
      "Episode: 187, Reward: -266.9480592067962\n",
      "Episode: 188, Reward: -273.09205008722233\n",
      "Episode: 189, Reward: -278.0009033226587\n",
      "Episode: 190, Reward: -283.8775453176132\n",
      "Episode: 191, Reward: -287.83219333775565\n",
      "Episode: 192, Reward: -292.9735220886848\n",
      "Episode: 193, Reward: -298.4344352278066\n",
      "Episode: 194, Reward: -302.8644619590941\n",
      "Episode: 195, Reward: -309.93926541865136\n",
      "Episode: 196, Reward: -320.8127524682137\n",
      "Episode: 197, Reward: -326.9436282657998\n",
      "Episode: 198, Reward: -339.8865051794222\n",
      "Episode: 199, Reward: -350.2051181046843\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('HalfCheetah-v5')\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "agent = DreamerAgent(obs_dim, action_dim)\n",
    "episodes = 200\n",
    "rewards = []\n",
    "\n",
    "for ep in range(episodes):\n",
    "    obs, _ = env.reset()\n",
    "    ep_reward = 0\n",
    "    real_obs, real_actions, real_rewards = [], [], []\n",
    "\n",
    "    for step in range(1000):\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        z = agent.encoder(obs_tensor)\n",
    "        action = agent.actor(z).detach().cpu().numpy().squeeze(0)\n",
    "        next_obs, reward, truncated, terminated,_ = env.step(action)\n",
    "        done = truncated or terminated\n",
    "\n",
    "        real_obs.append(obs_tensor.squeeze(0))\n",
    "        real_actions.append(torch.tensor(action, dtype=torch.float32).to(device))\n",
    "        real_rewards.append(torch.tensor(reward, dtype=torch.float32).unsqueeze(0).to(device))\n",
    "\n",
    "        obs = next_obs\n",
    "        ep_reward += reward\n",
    "\n",
    "        if done:break\n",
    "    agent.train(real_obs, real_actions, real_rewards)\n",
    "    rewards.append(ep_reward)\n",
    "    print(f\"Episode: {ep}, Reward: {ep_reward}\")\n",
    "\n",
    "plt.plot(rewards)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Rewards')\n",
    "plt.title('DreamerV2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intrinsic Motivation For Enhanced Exploration\n",
    "Traditional RL relies primarily on extrinsic rewards, but many environments provide sparse or delayed rewards, Instrinsic motivation adds internal rewards based on the novelty or uncertainty of observatons or latent states, encouraging the agent to explore new regions in the state space proactively.\n",
    "DreamerV2 naturally integrates intrinsic motivation by using model uncertainty (from RSSM) to compute intrinsic rewards.\n",
    "This improves performance in environments with sparse external rewards, accellerates learning by encouraging exploration of uncertain states and enhances robustness against suboptimal local solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardModelWithIntrinsic(nn.Module):\n",
    "    def __init__(self, latent_dim, intrinsic_scale=0.1):\n",
    "        super().__init__()\n",
    "        self.extrinsic = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "        self.intrinsic = intrinsic_scale\n",
    "\n",
    "    def forward(self, z, predition_error):\n",
    "        extrinsic_reward = self.extrinsic(z)\n",
    "        intrinsic_reward = predition_error.unsqueeze(-1) * self.intrinsic\n",
    "        total_reward = extrinsic_reward + intrinsic_reward\n",
    "        return total_reward, extrinsic_reward, intrinsic_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSSMWithIntrinsic(nn.Module):\n",
    "    def __init__(self, latent_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRUCell(latent_dim+action_dim, latent_dim)\n",
    "        self.latent_mu = nn.Linear(latent_dim, latent_dim)\n",
    "        self.latent_logvar = nn.Linear(latent_dim, latent_dim)\n",
    "\n",
    "    def forward(self, h,z,a):\n",
    "        x = torch.cat([z,a],dim=-1)\n",
    "        h_next = self.rnn(x,h)\n",
    "        mu, logvar = self.latent_mu(h_next), self.latent_logvar(h_next)\n",
    "        z_next = mu + torch.exp(logvar * 0.5) * torch.randn_like(mu)\n",
    "        prediction_error = torch.mean((z_next - mu).pow(2), dim=-1)\n",
    "        return h_next, z_next, mu, logvar, prediction_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DreamerV2IntrinsicRew( DreamerAgent):\n",
    "    def __init__(self, obs_dim, action_dim, latent_dim = 32):\n",
    "        super(DreamerV2IntrinsicRew, self).__init__(obs_dim, action_dim, latent_dim)\n",
    "        self.reward_model = RewardModelWithIntrinsic(latent_dim).to(device)\n",
    "        self.rssm = RSSMWithIntrinsic(latent_dim, action_dim).to(device)\n",
    "\n",
    "    def train(self, real_obs, real_actions, real_rewards):\n",
    "        z = self.encoder(real_obs[0])\n",
    "        h = torch.zeros_like(z)\n",
    "\n",
    "        # World Model Learning, with simple intrinsic reward based on prediction uncertainty in RSSM\n",
    "        recon_loss = 0\n",
    "        intrinsic_rewards = []\n",
    "        for t in range(len(real_actions)):\n",
    "            h, z, mu, logvar, prediction_error = self.rssm(h, z, real_actions[t])\n",
    "            reward_pred, _, intrinsic_reward = self.reward_model(z, prediction_error)\n",
    "            intrinsic_rewards.append(intrinsic_reward.detach())\n",
    "\n",
    "            recon_loss += (reward_pred - real_rewards[t]).pow(2)\n",
    "            kl_loss = -0.5 * (1 + logvar - mu.pow(2) - logvar.exp()).mean()\n",
    "\n",
    "            recon_loss += recon_loss.mean() + kl_loss\n",
    "\n",
    "        self.optim_model.zero_grad()\n",
    "        recon_loss.backward()\n",
    "        self.optim_model.step()\n",
    "\n",
    "        # Imagined Trajectories for policy and value\n",
    "        zs, hs, imagined_rewards = [], [], []\n",
    "        z_imagine, h_imagine = z.detach(), h.detach()\n",
    "        \n",
    "        #Horizon range\n",
    "        for _ in range(15):\n",
    "            a = self.actor(z_imagine)\n",
    "            h_imagine, z_imagine, _, _, prediction_error = self.rssm(h_imagine, z_imagine, a)\n",
    "            r_total, _,_ = self.reward_model(z_imagine, prediction_error)\n",
    "            imagined_rewards.append(r_total)\n",
    "            zs.append(z_imagine)\n",
    "\n",
    "        \n",
    "        values = [self.critic(z) for z in zs]\n",
    "\n",
    "        targets = []\n",
    "        for i in range(len(imagined_rewards)):\n",
    "            reward = imagined_rewards[i]\n",
    "            if i+1 < len(values):\n",
    "                target = reward + 0.99 * values[i+1].detach()\n",
    "            else:\n",
    "                target = reward\n",
    "            targets.append(target)\n",
    "\n",
    "        #Critic Update\n",
    "        value_loss = sum((values[i] - targets[i]).pow(2).mean() for i in range(len(values)))\n",
    "        self.optim_critic.zero_grad()\n",
    "        value_loss.backward()\n",
    "        self.optim_critic.step()\n",
    "\n",
    "        #Actor Update\n",
    "        actor_loss = -sum(self.critic(z).mean() for z in zs)\n",
    "        self.optim_actor.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.optim_actor.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DreamerV2IntrinsicRew() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mDreamerV2IntrinsicRew\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[0;32m      3\u001b[0m rewards \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mTypeError\u001b[0m: DreamerV2IntrinsicRew() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "agent = DreamerV2IntrinsicRew(obs_dim, action_dim)\n",
    "episodes = 200\n",
    "rewards = []\n",
    "\n",
    "for ep in range(episodes):\n",
    "    obs, _ = env.reset()\n",
    "    ep_reward = 0\n",
    "    real_obs, real_actions, real_rewards = [], [], []\n",
    "\n",
    "    for step in range(1000):\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        z = agent.encoder(obs_tensor)\n",
    "        action = agent.actor(z).detach().cpu().numpy().squeeze(0)\n",
    "        next_obs, reward, truncated, terminated,_ = env.step(action)\n",
    "        done = truncated or terminated\n",
    "\n",
    "        real_obs.append(obs_tensor.squeeze(0))\n",
    "        real_actions.append(torch.tensor(action, dtype=torch.float32).to(device))\n",
    "        real_rewards.append(torch.tensor(reward, dtype=torch.float32).unsqueeze(0).to(device))\n",
    "\n",
    "        obs = next_obs\n",
    "        ep_reward += reward\n",
    "\n",
    "        if done:break\n",
    "    agent.train(real_obs, real_actions, real_rewards)\n",
    "    rewards.append(ep_reward)\n",
    "    print(f\"Episode: {ep}, Reward: {ep_reward}\")\n",
    "\n",
    "plt.plot(rewards)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Rewards')\n",
    "plt.title('DreamerV2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention-Based Recurrent State-Space Models\n",
    "Standard RSSM use RNN(GRU, LSTM) limiting the ability to acpture long-range dependencies efficiently, attention mechanisms allow latent state represetnations to attend selectively to past states, improving modeling of complex temporal relationships.\n",
    "This improves handling of environments with long-horizon dependencies, Enhances modeling accuracy and robustness, Potentially improves long-term planning and decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionRSSM(nn.Module):\n",
    "    def __init__(self, latent_dim, action_dim, attention_heads=4):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=latent_dim, num_heads=attention_heads)\n",
    "\n",
    "        self.gru_cell = nn.GRUCell(latent_dim+action_dim, latent_dim)\n",
    "\n",
    "        self.mu_layer = nn.Linear(latent_dim, latent_dim)\n",
    "        self.logvar_layer = nn.Linear(latent_dim, latent_dim)\n",
    "\n",
    "    def forward(self, past_latents, h, z, a):\n",
    "        query = (z+h).unsqueeze(0)\n",
    "        att_output, _ = self.attention(query, past_latents, past_latents)\n",
    "        att_output = att_output.squeeze(0)\n",
    "\n",
    "        gru_input = torch.cat([att_output, a], dim=-1)\n",
    "        h_next = self.gru_cell(gru_input, h)\n",
    "\n",
    "        mu = self.mu_layer(h_next)\n",
    "        logvar = self.logvar_layer(h_next)\n",
    "        z_next = mu + torch.exp(logvar * 0.5) * torch.randn_like(mu)\n",
    "\n",
    "        return h_next, z_next, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DreamerV2AttentionAgent(DreamerAgent):\n",
    "    def __init__(self, obs_dim, action_dim, latent_dim=32):\n",
    "        super(DreamerV2AttentionAgent, self).__init__(obs_dim, action_dim, latent_dim)\n",
    "        self.rssm = AttentionRSSM(latent_dim, action_dim).to(device)\n",
    "\n",
    "        params = list(self.encoder.parameters()) + list(self.rssm.parameters()) + list(self.reward_model.parameters())\n",
    "        self.optim_model = optim.Adam(params, lr=3e-4)\n",
    "\n",
    "    def train(self, real_obs, real_actions, real_rewards):\n",
    "        z = self.encoder(real_obs[0])\n",
    "        h = torch.zeros_like(z)\n",
    "\n",
    "        # World Model Learning\n",
    "        recon_loss = 0\n",
    "        for t in range(len(real_actions)):\n",
    "            past_latents = torch.stack(real_obs[:t+1])\n",
    "            h, z, mu, logvar = self.rssm(h, z, real_actions[t], past_latents)\n",
    "            reward_pred = self.reward_model(z)\n",
    "            recon_loss += (reward_pred - real_rewards[t]).pow(2).mean() + -0.5 * (1 + logvar - mu.pow(2) - logvar.exp()).mean()\n",
    "\n",
    "        self.optim_model.zero_grad()\n",
    "        recon_loss.backward()\n",
    "        self.optim_model.step()\n",
    "\n",
    "        # Imagined Trajectories for policy and value\n",
    "        with torch.no_grad():\n",
    "            zs, hs, rewards = self.imagine(z.detach(), h.detach())\n",
    "        \n",
    "        values = [self.critic(z.detach()) for z in zs]\n",
    "\n",
    "        targets = []\n",
    "        for i in range(len(rewards)):\n",
    "            reward = rewards[i].detach()\n",
    "            if i+1 < len(values):\n",
    "                target = reward + 0.99 * values[i+1].detach()\n",
    "            else:\n",
    "                target = reward\n",
    "            targets.append(target)\n",
    "\n",
    "        #Critic Update\n",
    "        value_loss = sum((values[i] - targets[i]).pow(2).mean() for i in range(len(values)))\n",
    "        self.optim_critic.zero_grad()\n",
    "        value_loss.backward()\n",
    "        self.optim_critic.step()\n",
    "\n",
    "        #Actor Update\n",
    "        zs_actor, _, _ = self.imagine(z.detach(), h.detach())\n",
    "        actor_loss = -sum(self.critic(z).mean() for z in zs_actor)\n",
    "        self.optim_actor.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.optim_actor.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pixel Based DreamerV2 (Vison-Based RL)\n",
    "DreamerV2 can compress visual observations into a low-dimensional latent representation enabling effective policy optimization directly from pixels, a convolutional encoder maps images to latent states, enabling end-to-end learning from visual inputs.\n",
    "This allows DreamerV2 to handle high-dimensional observations, such as images, effectively, and improves performance in environments with complex visual features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.conv_net = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=8, stride=4), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1), nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*7*7, latent_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv_net(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
