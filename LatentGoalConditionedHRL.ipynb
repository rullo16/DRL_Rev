{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Goal-Conditioned Hierarchical Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional skill-learning methods like DIAYN or VALOR allow agents to learn diverse behaviors without external reward signals. However, these skills are often passive and lack direction towards solving meaningful long-term tasks. That's where goal-conditioned learning comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In latent goal-conditioned HRL, the agent does not just execute skills it learns to reach goals in a learned latent space. The idea is to:\n",
    "- Learn compact represetnations of goals\n",
    "- Condition the policy on a goal vector g, instead of just a skill id\n",
    "- Learn a goal-transition model that predicts how goals evolve\n",
    "- Plan or adapt dynamically using these learned latent subgoals.\n",
    "This is useful for:\n",
    "- Compositional behavior\n",
    "- Skill planning\n",
    "- Multi-task learning\n",
    "- Hierarchical exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-**Goal-conditioned Policy**: A policy $\\pi(a|s,g)$ takes a latent goal as input and produces an action to reach it.\n",
    "-**Goal Space**: Instead of using raw coordinates or pixel targetsm we learn a goal space $g\\in\\mathcal{Z}$, via an encoder. This makes the representation compact and transferable.\n",
    "- **Latent goal Transitions**: We model how latent goals evolve, using a simple feedforward network:\n",
    "$$g_{t+1} = f(g_t, s_t, a_t)$$\n",
    "- **Reward Signal**: Instead of hand-crafted rewards, we can use:\n",
    "    - Distance in latent space between achieved and target goal\n",
    "    - Discriminator scores\n",
    "    - Contrastive learning signals\n",
    "- **Hierarchy**: The high-level policy picks or predicts a new latent goal every k steps. The low-level policy is trained to reach this goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Latent Goal Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WE learn a represetnation $g = E(s)$ from a goal encoder $E$, where:\n",
    "- s is an observation\n",
    "- $g \\in \\mathbb{R}^d$ is a vector in goal space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal-Conditioned Policy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent maximizes the probability of achieving a goal $g$, optionally using entropy regularization (SAC-style):\n",
    "$$\\max_\\pi \\mathbb{E}_{s,a,g}[r(s,g)+\\alpha \\mathcal{H}(\\pi(\\cdot|s,g))]$$\n",
    "Where:\n",
    "- $r(s,g) = - ||E(s)-g||^2$ is a distance-based reward\n",
    "- $\\mathcal{H}$ is the entropy of the policy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $s_t$ environment state\n",
    "- $g_t$ latent goal\n",
    "- $E(s)$ encoder mapping obeservations to latent goal space\n",
    "- $\\pi(a|s,g)$ goal-conditioned policy\n",
    "- $f(g_t,s_t,a_t)$ goal transition function\n",
    "- $Q(s,a,g)$ goal-conditioned critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal-conditioned Reward**:\n",
    "$$r(s_t,g) - ||E(s_t)-g||^2_2$$\n",
    "**High-level Transition**:\n",
    "$$g_{t+1} = f(g_t,s_t,a_t)$$\n",
    "**SAC-style critic loss**:\n",
    "$$\\mathcal{L}_{Q} = (Q(s_t,g,a_t) - [r(s_t,g) + \\gamma \\mathbb{E}_{a'}Q(s_{t+1},g,a')-\\alpha \\log \\pi(a'|s_{t+1},g)])^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoalEncoder(nn.Module):\n",
    "    def __init__(self, input_shape=(3, 96, 96), latent_dim=64):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, *input_shape)\n",
    "            conv_out = self.conv(dummy).view(1, -1).shape[1]\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out, latent_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "        if x.ndim == 3:\n",
    "            x = x.permute(2, 0, 1).unsqueeze(0)\n",
    "        elif x.ndim == 4 and x.shape[-1] == 3:\n",
    "            x = x.permute(0, 3, 1, 2)\n",
    "        x = x / 255.0\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoalConditionedPolicy(nn.Module):\n",
    "    def __init__(self, obs_dim, goal_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128, 256), nn.ReLU(),\n",
    "            nn.Linear(256, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, state, goal):\n",
    "        if goal.ndim == 3:\n",
    "            goal = goal.squeeze(1)\n",
    "        x = torch.cat([state, goal], dim=-1)\n",
    "        return torch.tanh(self.fc(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoalConditionedCritic(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim, goal_dim):\n",
    "        super().__init__()\n",
    "        self.q1 = nn.Sequential(\n",
    "            nn.Linear(obs_dim + action_dim + goal_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        self.q2 = nn.Sequential(\n",
    "            nn.Linear(obs_dim + action_dim + goal_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action, goal):\n",
    "        x = torch.cat([state, action, goal], dim=-1)\n",
    "        return self.q1(x), self.q2(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoalCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, goal_dim):\n",
    "        super().__init__()\n",
    "        self.q1 = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim + goal_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        self.q2 = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim + goal_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action, goal):\n",
    "        if state.ndim == 4:\n",
    "            state = state.view(state.size(0), -1)\n",
    "        if goal.ndim == 3:\n",
    "            goal = goal.squeeze(1)\n",
    "        x = torch.cat([state, action, goal], dim=-1)\n",
    "        return self.q1(x), self.q2(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done, goal):\n",
    "        self.buffer.append((state, action, reward, next_state, done, goal))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done, goal = zip(*batch)\n",
    "\n",
    "        def process(x):\n",
    "            x = torch.tensor(np.array(x), dtype=torch.float32)\n",
    "            if x.ndim == 4 and x.shape[-1] == 3:\n",
    "                x = x.permute(0, 3, 1, 2)\n",
    "            return x / 255.0\n",
    "\n",
    "        return (process(state), torch.tensor(action, dtype=torch.float32),\n",
    "                torch.tensor(reward, dtype=torch.float32).unsqueeze(1),\n",
    "                process(next_state),\n",
    "                torch.tensor(done, dtype=torch.float32).unsqueeze(1),\n",
    "                process(goal))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_goal_reward(achieved, desired):\n",
    "    return -((achieved-desired)**2).sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, buffer, encoder, critic, target_critic, policy, critic_optimizer, policy_optimizer, alpha=0.2, gamma=0.99, tau=0.005, batch_size=128):\n",
    "    if len(buffer) < batch_size:\n",
    "        return 0\n",
    "    \n",
    "    state, action, _, next_state, done, goal = buffer.sample(batch_size)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if next_state.ndim == 4 and next_state.shape[1] != 3:\n",
    "            next_state = next_state.permute(0, 3, 1, 2)  # Fix channel order\n",
    "        next_goal = encoder(next_state)\n",
    "        next_action = policy(next_state, next_goal)\n",
    "        target_q1, target_q2 = target_critic(next_state, next_action, next_goal)\n",
    "        target_q = torch.min(target_q1, target_q2)\n",
    "        target_val = latent_goal_reward(encoder(state), goal)+(1-done)*gamma*target_q\n",
    "\n",
    "    q1, q2 = critic(state, action, goal)\n",
    "    critic_loss = F.mse_loss(q1, target_val) + F.mse_loss(q2, target_val)\n",
    "\n",
    "    critic_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    critic_optimizer.step()\n",
    "\n",
    "    new_action = policy(state, goal)\n",
    "    new_q1, new_q2 = critic(state, new_action, goal)\n",
    "    new_q = torch.min(new_q1, new_q2)\n",
    "\n",
    "    policy_loss = (alpha * torch.log(torch.clamp(torch.ones_like(new_q) - new_q, min=1e-6)) - new_q).mean()\n",
    "\n",
    "    policy_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    policy_optimizer.step()\n",
    "\n",
    "    for param, target_param in zip(critic.parameters(), target_critic.parameters()):\n",
    "        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_goal_from_obs(obs, encoder):\n",
    "    obs = torch.tensor(obs, dtype=torch.float32).permute(2,0,1).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        goal = encoder(obs).squeeze(0)\n",
    "    return goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_episodes(env, agent, encoder, buffer, max_steps=200):\n",
    "    obs, _ = env.reset()\n",
    "    goal = sample_goal_from_obs(obs, encoder)\n",
    "    tot_reward = 0\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            goal_tensor = goal.unsqueeze(0)\n",
    "            action = agent.act(obs_tensor, goal_tensor)\n",
    "\n",
    "        next_obs, _, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        buffer.push(obs, action, 0.0, next_obs, done, goal)\n",
    "        tot_reward += -np.linalg.norm(encoder(torch.tensor(next_obs, dtype=torch.float32).permute(2,0,1).unsqueeze(0)).detach().numpy() - goal.numpy())\n",
    "        \n",
    "        obs = next_obs\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return tot_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoalAgent:\n",
    "    def __init__(self, encoder, policy):\n",
    "        self.encoder = encoder\n",
    "        self.policy = policy\n",
    "\n",
    "    def act(self, obs, goal):\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32)\n",
    "        if obs_tensor.ndim == 3:\n",
    "            obs_tensor = obs_tensor.permute(2, 0, 1).unsqueeze(0)\n",
    "        elif obs_tensor.ndim == 4 and obs_tensor.shape[-1] == 3:\n",
    "            obs_tensor = obs_tensor.permute(0, 3, 1, 2)\n",
    "\n",
    "        obs_tensor = obs_tensor / 255.0\n",
    "        latent_obs = self.encoder(obs_tensor)\n",
    "        action = self.policy(latent_obs, goal)\n",
    "        return action.squeeze(0).detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v2', render_mode='rgb_array')\n",
    "obs_shape = (3, 96, 96)\n",
    "action_dim = env.action_space.shape[0]\n",
    "encoder = GoalEncoder(input_shape=obs_shape)\n",
    "policy = GoalConditionedPolicy(obs_shape[0], 64, action_dim)\n",
    "agent = GoalAgent(encoder, policy)\n",
    "buffer = ReplayBuffer()\n",
    "critic = GoalCritic(obs_shape[0], action_dim, 64)\n",
    "target_critic = GoalCritic(obs_shape[0], action_dim, 64)\n",
    "target_critic.load_state_dict(critic.state_dict())\n",
    "critc_optimizer = optim.Adam(critic.parameters(), lr=3e-4)\n",
    "policy_optimizer = optim.Adam(policy.parameters(), lr=3e-4)\n",
    "\n",
    "rewards = []\n",
    "\n",
    "episodes = 200\n",
    "for ep in range(episodes):\n",
    "    ep_reward = collect_episodes(env, agent, agent.encoder, buffer)\n",
    "    rewards.append(ep_reward)\n",
    "\n",
    "    for _ in range(5):\n",
    "        train(agent, buffer, encoder, critic, target_critic, policy, critc_optimizer, policy_optimizer)\n",
    "    if ep % 10 == 0:\n",
    "        print(f\"Episode {ep}, Reward: {ep_reward:.2f}\")\n",
    "\n",
    "plt.plot(rewards)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Training Rewards')\n",
    "plt.show()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "procgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
