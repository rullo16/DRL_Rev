{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory-Advanced RL (MARL):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In traditional RL, policies are often memoryless (Markovian), making decisions based solely on the current state. However, manu real-world tasks require remembering past observations to act effectively. Memory-Augmented RL introduces explicit memory mechanisms withing RL agents, allowing them to capture temporal dependencies and make more informed decisionas based on past experiences.\n",
    "- **Long-term dependencies**: Better handles tasks requiring remembering events from the distant past.\n",
    "- **Partial observability**: Deals effectively with partially observable environments.\n",
    "- **Generalization**: Enhances the agent's capability to adapt to complex, dynamic, and structured tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Markovian vs Non-Markovian**: Memory-Aug RL explicitly addresses non-Markovian environments by introducing memory structures into the agent's policy.\n",
    "- **RNNs**\n",
    "- **Extenral Memory (Neural Turing Machines, Memory Networks)**: Networks that explicitly read/write to external memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory-Aug RL agetns parameterize a policy $\\pi$ as:\n",
    "$$a_t \\sim \\pi(a_t|s_t,h_T)$$\n",
    "where $h_T$ is a memory state that symmarizes past interactions:\n",
    "- Implicit memory (RNN/LSTM):\n",
    "$$h_{t+1} = f_{\\theta}(h_t,s_t,a_t,r_t)$$\n",
    "- Explicit memory (Memory Networks/NTM):\n",
    "Memory is a separate storage structure explicitly written to and read from.\n",
    "Aims to optimize policy paramters $\\theta$ while incorporating past experiences effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Policy (RNN/LSTM)\n",
    "A recurrent policy integrates past experienves into a hidden memory state:\n",
    "$$h_{t+1} = LSTM(h_t,s_t,a_t,r_t;\\theta_h)$$\n",
    "Policy output is conditioned on hidden state:\n",
    "$$\\pi(a_t|s_t,h_t;\\theta_{\\pi})$$ \n",
    "$$V(s_t,h_t;\\theta_V)$$\n",
    "\n",
    "### External Memory\n",
    "Involve explicit read/write operations:\n",
    "- Write:\n",
    "$$M_t = W(M_{t-1},s_t,a_t,r_t)$$\n",
    "- Read:\n",
    "$$m_t = R(M_t,s_t)$$\n",
    "Then, policy decisions use both current state and retrieved memory $m_t$:\n",
    "$$a_t \\sim \\pi(a_t|s_t,m_t;\\theta_{\\pi})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMActorCritic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(obs_dim, hidden_size, batch_first=True)\n",
    "        self.actor = nn.Linear(hidden_size, act_dim)\n",
    "        self.critic = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, obs, hidden):\n",
    "        obs = obs.view(1, 1, -1)  # Correct shape: (seq_len=1, batch=1, features)\n",
    "        lstm_out, hidden = self.lstm(obs, hidden)\n",
    "        action_mean = torch.tanh(self.actor(lstm_out[:, -1]))\n",
    "        value = self.critic(lstm_out[:, -1])\n",
    "        return action_mean, value, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(1, 1, 128).to(device), \n",
    "                torch.zeros(1, 1, 128).to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rullo\\anaconda3\\envs\\mlagents\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "c:\\Users\\rullo\\anaconda3\\envs\\mlagents\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:245: UserWarning: \u001b[33mWARN: The reward returned by `step()` must be a float, int, np.integer or np.floating, actual type: <class 'numpy.ndarray'>\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: [-890.9534]\n",
      "Episode 10: [-1374.8759]\n",
      "Episode 20: [-747.6549]\n",
      "Episode 30: [-1120.7549]\n",
      "Episode 40: [-1889.1458]\n",
      "Episode 50: [-1095.8055]\n",
      "Episode 60: [-1864.7867]\n",
      "Episode 70: [-906.3905]\n",
      "Episode 80: [-1055.7147]\n",
      "Episode 90: [-1070.7229]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pendulum-v1\")\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "\n",
    "agent = LSTMActorCritic(obs_dim, act_dim).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=3e-4)\n",
    "\n",
    "def run_episode(env, agent, optimizer):\n",
    "    obs, _ = env.reset()\n",
    "    hidden = agent.init_hidden()\n",
    "\n",
    "    log_probs, values, rewards = [], [], []\n",
    "    ep_reward = 0\n",
    "\n",
    "    for _ in range(200):\n",
    "        obs_tensor = torch.tensor(obs, dtype = torch.float32).to(device)\n",
    "        action_mean, value, hidden = agent(obs_tensor, hidden)\n",
    "\n",
    "        dist = torch.distributions.Normal(action_mean, 0.1)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action).sum()\n",
    "\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action.cpu().numpy())\n",
    "        done = terminated or truncated\n",
    "\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value.squeeze(0))\n",
    "        rewards.append(torch.tensor(reward, dtype=torch.float32).to(device))\n",
    "\n",
    "        obs = next_obs\n",
    "        ep_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    returns, R = [], 0\n",
    "    gamma = 0.99\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "\n",
    "    returns = torch.stack(returns)\n",
    "    log_probs = torch.stack(log_probs)\n",
    "    values = torch.stack(values)\n",
    "\n",
    "    advantage = returns - values\n",
    "\n",
    "    actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "    critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "    loss = actor_loss + critic_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return ep_reward\n",
    "\n",
    "for ep in range(100):\n",
    "    ep_reward = run_episode(env, agent, optimizer)\n",
    "    if ep % 10 == 0:\n",
    "        print(f\"Episode {ep}: {ep_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next-Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer-Based Memory-Aug RL\n",
    "\n",
    "Introduce Transformer-based policy to enhance memory capabilities via attention mechanisms, explicitly capturing long-term dependencies in the environment.\n",
    "Transformers utilize self-attention mechanisms, allowing the agent to selectively focus on relevant past experiences, significantly improving long-range memory ober traditional recurrent models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerActorCritic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, seq_len=10, hidden_dim=128, n_heads=4, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "\n",
    "        self.input_proj = nn.Linear(obs_dim, hidden_dim)\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(seq_len, hidden_dim))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=n_heads, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "        self.actor = nn.Linear(hidden_dim, act_dim)\n",
    "        self.critic = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, obs_seq):\n",
    "        batch_size = obs_seq.shape[0]\n",
    "        x = self.input_proj(obs_seq) + self.pos_encoding[:obs_seq.size(1),:]\n",
    "\n",
    "        memory = self.transformer_encoder(x)\n",
    "        latest_memory = memory[:, -1]\n",
    "\n",
    "        action = torch.tanh(self.actor(latest_memory))\n",
    "        value = self.critic(latest_memory)\n",
    "        return action, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Reward: -7.75\n",
      "Episode 10, Reward: -25.13\n",
      "Episode 20, Reward: -56.99\n",
      "Episode 30, Reward: -1.89\n",
      "Episode 40, Reward: -43.98\n",
      "Episode 50, Reward: -69.21\n",
      "Episode 60, Reward: -36.11\n",
      "Episode 70, Reward: -88.89\n",
      "Episode 80, Reward: -83.48\n",
      "Episode 90, Reward: -55.05\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pendulum-v1\")\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "\n",
    "agent = TransformerActorCritic(obs_dim, act_dim).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=3e-4)\n",
    "\n",
    "def collect_episode(env, policy, seq_len):\n",
    "    obs_seq, action_seq, reward_seq = [], [], []\n",
    "    obs, _ = env.reset()\n",
    "    ep_reward = 0\n",
    "\n",
    "    for step in range(seq_len):\n",
    "        obs_seq.append(obs)\n",
    "        if len(obs_seq) < seq_len:\n",
    "            # Before having enough observations, take random actions\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            obs_tensor = torch.tensor(obs_seq[-seq_len:], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                action_pred, _ = policy(obs_tensor)\n",
    "\n",
    "            action = action_pred.cpu().numpy().flatten()\n",
    "\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        reward_seq.append(reward)\n",
    "        action_seq.append(action)\n",
    "        obs = next_obs\n",
    "        ep_reward += reward\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    return obs_seq, action_seq, reward_seq, ep_reward\n",
    "\n",
    "\n",
    "\n",
    "def train_step(policy, optimizer, obs_seq, action_seq, reward_seq, seq_len):\n",
    "    if len(obs_seq) < seq_len:\n",
    "        return  # Skip if insufficient length\n",
    "\n",
    "    obs_tensor = torch.tensor(obs_seq[-seq_len:], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    action_tensor = torch.tensor(action_seq[-seq_len:], dtype=torch.float32).to(device)\n",
    "    rewards = torch.tensor(reward_seq[-seq_len:], dtype=torch.float32).to(device)\n",
    "\n",
    "    action_preds, values = policy(obs_tensor)\n",
    "    values = values.squeeze()\n",
    "\n",
    "    actor_loss = ((action_preds - action_tensor)**2).mean()\n",
    "\n",
    "    returns, R = [], 0\n",
    "    gamma = 0.99\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns).to(device)\n",
    "\n",
    "    critic_loss = ((returns - values)**2).mean()\n",
    "\n",
    "    loss = actor_loss + critic_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "seq_len = 10\n",
    "episodes = 100\n",
    "\n",
    "for ep in range(episodes):\n",
    "    obs_seq, act_seq, rew_seq, ep_reward = collect_episode(env, agent, seq_len)\n",
    "    train_step(agent, optimizer, obs_seq, act_seq, rew_seq, seq_len)\n",
    "\n",
    "    if ep % 10 == 0:\n",
    "        print(f\"Episode {ep}, Reward: {ep_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Episode 0, Reward: -67.22\n",
      "Test Episode 1, Reward: -87.23\n",
      "Test Episode 2, Reward: -45.17\n",
      "Test Episode 3, Reward: -67.43\n",
      "Test Episode 4, Reward: -43.21\n",
      "Test Episode 5, Reward: -82.03\n",
      "Test Episode 6, Reward: -48.55\n",
      "Test Episode 7, Reward: -25.49\n",
      "Test Episode 8, Reward: -18.65\n",
      "Test Episode 9, Reward: -81.23\n",
      "Average Test Reward: -56.62\n"
     ]
    }
   ],
   "source": [
    "test_rewards = []\n",
    "for ep in range(10):\n",
    "    _, _, _, ep_reward = collect_episode(env, agent, seq_len=10)\n",
    "    test_rewards.append(ep_reward)\n",
    "    print(f\"Test Episode {ep}, Reward: {ep_reward:.2f}\")\n",
    "\n",
    "print(f\"Average Test Reward: {np.mean(test_rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicit Memory-Aug RL\n",
    "\n",
    "While RNN implicitly store past information in their hidden states, explicit memory methods (Neural Turing Machines (NTM)) allow an agent to clearly and explicitly read from and write to a memory buffer, perform more structured memory operations and capture long-range dependencies better than typical RNNs or LSTMs.\n",
    "An explicit memory model typically consists of:\n",
    "- Memory matrix $M\\in R^{N\\times M}$ with N memory slots, each of dimension M.\n",
    "- Read and write heads, controlling what info is written/read from memory.\n",
    "- Controller (NN) deciding read/write actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplicitMemoryAgent(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, memory_slots=20, memory_dim=32, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.memory_slots = memory_slots\n",
    "        self.memory_dim = memory_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.controller = nn.GRUCell(obs_dim+memory_dim, hidden_dim)\n",
    "\n",
    "        self.memory = torch.zeros(memory_slots, memory_dim).to(device)\n",
    "\n",
    "        self.read_head = nn.Linear(hidden_dim, memory_slots)\n",
    "        self.write_head = nn.Linear(hidden_dim, memory_slots)\n",
    "\n",
    "        self.write_projection= nn.Linear(hidden_dim, memory_dim)\n",
    "\n",
    "        self.actor = nn.Linear(hidden_dim+memory_dim, act_dim)\n",
    "        self.critic = nn.Linear(hidden_dim+memory_dim, 1)\n",
    "\n",
    "    def reset_memory(self):\n",
    "        self.memory = torch.zeros(self.memory_slots, self.memory_dim).to(device)\n",
    "\n",
    "    def forward(self, obs, hidden):\n",
    "        obs = obs.unsqueeze(0)  # shape: (1, obs_dim)\n",
    "        \n",
    "        # Read from memory\n",
    "        read_weights = torch.softmax(self.read_head(hidden), dim=-1)  # (1, memory_slots)\n",
    "        read_vector = torch.matmul(read_weights, self.memory)         # (1, memory_dim)\n",
    "        \n",
    "        # Controller update (ensure matching dimensions)\n",
    "        controller_input = torch.cat([obs, read_vector], dim=-1)      # (1, obs_dim + memory_dim)\n",
    "        hidden = self.controller(controller_input, hidden)            # (1, hidden_dim)\n",
    "        \n",
    "        # Write to memory\n",
    "        write_weights = torch.softmax(self.write_head(hidden), dim=-1)  # (1, memory_slots)\n",
    "        write_content = torch.tanh(self.write_projection(hidden))       # (1, memory_dim)\n",
    "\n",
    "        # update memory explicitly\n",
    "        self.memory = self.memory + write_weights.squeeze(0).unsqueeze(-1) * write_content\n",
    "\n",
    "        # Actor and Critic\n",
    "        actor_input = torch.cat([hidden, read_vector], dim=-1)\n",
    "        action = torch.tanh(self.actor(actor_input))\n",
    "        value = self.critic(actor_input)\n",
    "\n",
    "        return action.squeeze(0), value.squeeze(0), hidden\n",
    "\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: -889.102543802381\n",
      "Episode 10: -1921.4417541711236\n",
      "Episode 20: -907.8367765463909\n",
      "Episode 30: -968.2965106261779\n",
      "Episode 40: -1663.0584427264087\n",
      "Episode 50: -1074.7426779459788\n",
      "Episode 60: -897.1711201635997\n",
      "Episode 70: -1759.5716529330393\n",
      "Episode 80: -1170.2580476895098\n",
      "Episode 90: -1803.3554597443747\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pendulum-v1\")\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "\n",
    "agent = ExplicitMemoryAgent(obs_dim, act_dim).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=1e-4)\n",
    "\n",
    "def run_episode(env, agent, optimizer):\n",
    "    obs, _ = env.reset()\n",
    "    agent.reset_memory()\n",
    "    hidden = agent.init_hidden()\n",
    "\n",
    "    log_probs, values, rewards = [], [], []\n",
    "    ep_reward = 0\n",
    "\n",
    "    for step in range(200):\n",
    "        obs_tensor = torch.tensor(obs, dtype = torch.float32).to(device)\n",
    "        action, value, hidden = agent(obs_tensor, hidden)\n",
    "\n",
    "        dist = torch.distributions.Normal(action, 0.1)\n",
    "        sampled_action = dist.sample()\n",
    "        log_prob = dist.log_prob(sampled_action).sum()\n",
    "\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(sampled_action.cpu().numpy())\n",
    "        done = terminated or truncated\n",
    "\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value.squeeze())\n",
    "        rewards.append(torch.tensor(reward, dtype=torch.float32).to(device))\n",
    "\n",
    "        obs = next_obs\n",
    "        ep_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    returns, R = [], 0\n",
    "    gamma = 0.99\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "\n",
    "    returns = torch.stack(returns)\n",
    "    log_probs = torch.stack(log_probs)\n",
    "    values = torch.stack(values)\n",
    "\n",
    "    advantage = returns - values\n",
    "\n",
    "    actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "    critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "    loss = actor_loss + critic_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return ep_reward\n",
    "\n",
    "for ep in range(100):\n",
    "    ep_reward = run_episode(env, agent, optimizer)\n",
    "    if ep % 10 == 0:\n",
    "        print(f\"Episode {ep}: {ep_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
