{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid MPC + SAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MPC is a great short-term optimization but is not very good at long-term strategies, SAC is gread at long-term strategies but requires a lot of environment interactions.\n",
    "The solution is to combine both MPC and SAC, MPC will handle short-term planning and ensures good actions early in training, SAC learns a policy that improves overtime and generaluzes to new situations.\n",
    "The MPC-generated rollouts are used to train SAC more efficiently, reducing sample complexity.\n",
    "**Early Training**-> MPC helps explore good actions while SAC is still learning.\n",
    "**Later Training**-> SAC learns a long-term strategy to replace MPC.\n",
    "**Faster Convergence**-> MPC rollouts speed up sac training, reducing sample complexity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MPC solves an optimization problem at every step to find the best action sequence:\n",
    "$$\\min_{a_{t:t+H}}\\sum_{k=t}^{t+H}C(s_k,a_k)+\\lambda||a_k||$$\n",
    "subject to:\n",
    "$$s_{k+1}=f(s_k,a_k)$$\n",
    "where $H$ is the planning horizon, $C(s_k,a_k)$ is the cost function and $f(s_k,a_k)$ is the dynamics function.\n",
    "However MPC soes not learn from past experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAC is an off-policy actor-critic algorithm that learns a stochastic policy by maximizing:\n",
    "$$J(\\pi)=\\sum_{t}\\mathbb(E)_{(s_t,a_t)\\sim\\pi}[r(s_t,a_t)+\\alpha H(\\pi(\\cdot |s_t))]$$\n",
    "where $H(\\pi)$ is the policy entropy ensuring exploration, Critic Networks $Q_1,Q_2$ are estimated Q-value functions and Actor Network $\\pi$ samples continuous actions from a Gaussian Policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the hybrid approach we define a weighting function $w(t)$ that determines whether to use MPC or SAC:\n",
    "$$a_t = w(t) \\cdot \\alpha_{MPC} + (1 - W(t)) \\cdot \\alpha_{SAC}$$\n",
    "$w(t)$ starts high, favoring MPC early on. $w(t)$ decreases, letting SAC take control over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hybrid approach works as follows:\n",
    "1. At the beginning of training, the policy learned by SAC is random and unreliable. MPC, using a dynamics model, provides high-quality actions through direct optimization.\n",
    "2. Over time, SAC learns from both real and MPC-generated experiences, gradually improving and eventually outperforming MPC due to its long-term optimziation capability.\n",
    "3. A blending coefficient $w(t)$ controls the balance between MPC and SAC actions. Initially, $w(t) \\approx 1$, relying on MPC. Gradually, $w(t) \\rightarrow 0$, transitioning entirely to the learned SAC policy.\n",
    "\n",
    "At timestep t, the selected action $a_t$ is given by:\n",
    "$$a_t = w(t) \\cdot a_{MPC,t} + (1-w(t)) \\cdot a_{SAC}(s_t)$$\n",
    "where $a_{MPC,t}$ is the action optimized by MPC, $a_{SAC} = \\pi(s_t)$ is the action suggested by SAC policy and $w(t)$ is the time-dependent weight that decreases from 1 to 0 over training.\n",
    "To choose $w(t)$, there are common strategies:\n",
    "1. Linear Decay: $w(t) = \\max(0, 1-\\lambda\\cdot t)$, with $\\lambda$ small.\n",
    "2. Exponential Decay: $w(t) = \\exp^{-\\lambda \\cdot t}$, for some positive parameter $\\lambda.$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MPC solves an optimization problem at each timestep:\n",
    "$$\\min_{a_t^{t+H-1}} \\mathbb{E}_{\\hat{s}_t^t+H}[\\sum_{k=0}^{H-1} C(s_{t+k}, a_{t+k})]$$\n",
    "subject to:\n",
    "$$ s_{k+1} = f(s_k, a_k)$$\n",
    "Here $f(s,a)$ is a learned dynamics model.\n",
    "We measure uncertainty using an enseble of predictive models\n",
    "$$Uncertainty(s_t,a_t) = \\frac{1}{N} \\sum_{i=1}^{N} ||f_{\\theta_i}(s_t,a_t) - \\bar{f}(s_t,a_t)||^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = zip(*batch)\n",
    "        return (torch.tensor(np.array(state), dtype=torch.float),\n",
    "                torch.tensor(np.array(action), dtype=torch.float),\n",
    "                torch.tensor(np.array(reward), dtype=torch.float).unsqueeze(1),\n",
    "                torch.tensor(np.array(next_state), dtype=torch.float),\n",
    "                torch.tensor(np.array(done, dtype=int)).unsqueeze(1))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.max_action * torch.tanh(self.model(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.q_net = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        return self.q_net(torch.cat([state, action], dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleDynamicsModel(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, num_models=5):\n",
    "        super().__init__()\n",
    "        self.models = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(state_dim + action_dim, 256), nn.ReLU(),\n",
    "                nn.Linear(256, 256), nn.ReLU(),\n",
    "                nn.Linear(256, state_dim)\n",
    "            ) for _ in range(num_models)\n",
    "        ])\n",
    "\n",
    "    def predict(self, state, action):\n",
    "        sa = torch.cat([state, action], dim=-1)\n",
    "        preds = [m(sa) for m in self.models]\n",
    "        preds = torch.stack(preds)\n",
    "        mean_pred = preds.mean(0)\n",
    "        uncertainty = preds.var(0).mean().item()\n",
    "        return mean_pred, uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridMPCSACAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.shape[0]\n",
    "        self.max_action = env.action_space.high[0]\n",
    "\n",
    "        self.actor = Actor(self.state_dim, action_dim, max_action)\n",
    "        self.critic = Critic(self.state_dim, self.action_dim)\n",
    "        self.dynamics_model = EnsembleDynamicsModel(state_dim, action_dim)\n",
    "        self.buffer = ReplayBuffer()\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=3e-4)\n",
    "\n",
    "        self.horizon = 5\n",
    "        self.mpc_weight = 1.0\n",
    "        self.mpc_decay = 0.001\n",
    "\n",
    "    def mpc_action(self, state):\n",
    "        best_action = torch.zeros(self.action_dim)\n",
    "        best_reward = -np.inf\n",
    "        for _ in range(100):\n",
    "            candidate_action = np.random.uniform(-1,1, self.action_dim)\n",
    "            next_state_pred, uncertainty = self.dynamics_model.predict(torch.tensor(state, dtype=torch.float), torch.tensor(candidate_action, dtype=torch.float))\n",
    "            reward_pred = self.critic(torch.tensor(next_state_pred, dtype=torch.float), torch.tensor(candidate_action, dtype=torch.float))\n",
    "            if reward_pred.item() > best_reward:\n",
    "                best_reward = reward_pred.item()\n",
    "                best_action = candidate_action\n",
    "        return best_action\n",
    "    \n",
    "    def select_action(self, state, t):\n",
    "        sac_action = self.actor(torch.tensor(state, dtype=torch.float)).detach().numpy()\n",
    "        mpc_action = self.mpc_action(state)\n",
    "        w = max(0, 1-t/100000)\n",
    "        return w*mpc_action + (1-w)*sac_action\n",
    "    \n",
    "    def train(self):\n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample(128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
