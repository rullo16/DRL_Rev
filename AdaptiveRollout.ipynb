{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model-based RL builds a predicrive model of the environment and uses it to generate synthetic experience, which significantly improves sample efficiency. One of the most well known model-based RL algorithm is MBPO. MBPO enhances sample efficiency by training a dynamics model of the environment and using it to create rollouts, simulated sequences of state transitions. These rollouts are then used to train a policy, typically through an actor-critic algorithm.\n",
    "However, MBPO suffers from an important limitation: fixed rollout lengths. This means that ince the learned dynamics model is sufficiently trained, it is used to simulate future transitions for a fixed number of steps. But this approach is not adaptive to the accuracy of the model:\n",
    "- If rollouts are too short, the model is not utilized effectively, reducing the benefit of MBRL.\n",
    "- If rollouts are too long, compounding model errors degrade the training data, harming policy learning.\n",
    "\n",
    "To address this problem, we introduce Adaptive Rollout Lengths, where the number of steps simulated by the model is dynamically adjusted based on model uncertainty.\n",
    "this approach aims to balance:\n",
    "- Early-stage training, where short rollouts prevent excessive model bias.\n",
    "- Late-stage training, where longer rollouts maximize sample efficiency when the model is accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A rollout refers to a sequence of state transitions generated by a learned environment model. Instead of interacting with the real environment, the RL agent queries the model for predictions of what would happen given a state and action.\n",
    "In standard MBPO, rollouts are fixed at a predetermined lenght. But this introduces a fundamental problem: the model is not equally accurate at all stages of training. Early on, the model is poorly trained, so long rollouts introduce high errors. Later, as the model improves, using short rollouts wastes valuable synthetic data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind adaptive rollouts is that the rollout lenght should be proportional to model confidence. If the model is accurate, we can trust it to generate longer sequences. If it is uncertain, shorter rollouts are safer.\n",
    "This means that the rollout length, $H_t$, should be a function of model uncertainty:\n",
    "$$H_t = \\max(H_{min}, \\min(H_{max}, H_{base}\\cdot \\exp^{-\\beta \\cdot Var(s_t)}))$$\n",
    "where:\n",
    "- $H_{min}$ and $H_{max}$ are the minimum and maximum rollout lengths.\n",
    "- $H_{base}$ is the base rollout length.\n",
    "- $\\beta$ is a sensitivity parameter that controls how aggressively the rollout length changes\n",
    "- $Var(s_t)$ is the variance of the model, the model uncertainty.\n",
    "\n",
    "### Estimating Model Uncertainty\n",
    "One way to estimate model uncertainty is to train an ensemble of N models, each predicting the next state:\n",
    "$$\\hat{s}_{t+1}^{(i)} = f_{\\theta_i}(s_t, a_t), i \\in {1,2,...,N}$$\n",
    "We then compute the variance of these predictions:\n",
    "$$Var(\\hat{s}_{t+1}) = \\frac{1}{N-1} \\sum_{i=1}^{N} (\\hat{s}_{t+1}^{(i)} - \\bar{\\hat{s}}_{t+1})^2$$\n",
    "where $\\bar{\\hat{s}}_{t+1}$ is the mean of the predictions.\n",
    "The variance serves as a measure of model confidence, if predictions vary widely, it indicates high uncertainty, meaning shorter rollouts should be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\" Stores transitions for real and model rollouts \"\"\"\n",
    "    def __init__(self, max_size=int(1e6)):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        state = np.array(state, dtype=np.float32).flatten()\n",
    "        action = np.array(action, dtype=np.float32).flatten()\n",
    "        next_state = np.array(next_state, dtype=np.float32).flatten()\n",
    "        reward = np.float32(reward)\n",
    "        done = np.float32(done)\n",
    "        \n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        return (torch.tensor(np.stack(states), dtype=torch.float32),\n",
    "                torch.tensor(np.stack(actions), dtype=torch.float32),\n",
    "                torch.tensor(rewards, dtype=torch.float32).unsqueeze(1),\n",
    "                torch.tensor(np.stack(next_states), dtype=torch.float32),\n",
    "                torch.tensor(dones, dtype=torch.float32).unsqueeze(1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EnsembleDynamicsModel:\n",
    "    \"\"\" An ensemble of neural networks for uncertainty-aware model-based rollouts \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, num_models=5):\n",
    "        super(EnsembleDynamicsModel, self).__init__()\n",
    "        self.models = [self._build_model(state_dim, action_dim) for _ in range(num_models)]\n",
    "        self.num_models = num_models\n",
    "\n",
    "    def _build_model(self, state_dim, action_dim):\n",
    "        \"\"\" Create a single dynamics model \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, state_dim + 1)  # Predict next state and reward\n",
    "        )\n",
    "\n",
    "    def predict(self, state, action):\n",
    "        \"\"\" Predict next state and reward with model ensemble \"\"\"\n",
    "        inputs = torch.cat([state, action], dim=-1)\n",
    "        predictions = [model(inputs) for model in self.models]\n",
    "        \n",
    "        predictions = torch.stack(predictions)  # Shape: (num_models, batch, output_dim)\n",
    "        mean_prediction = predictions.mean(dim=0)  # Mean over ensemble models\n",
    "        uncertainty = predictions.var(dim=0).mean().item()  # Variance as uncertainty\n",
    "        \n",
    "        return mean_prediction[..., :-1], mean_prediction[..., -1], uncertainty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_adaptive_model_rollouts(model, policy, real_buffer, model_buffer, adaptive_rollout):\n",
    "    \"\"\" Generate synthetic rollouts with dynamically adjusted rollout length \"\"\"\n",
    "    for model_instance in model.models:\n",
    "        model_instance.eval()\n",
    "    policy.eval()\n",
    "\n",
    "    states, _, _, _, _ = real_buffer.sample(500)  # Sample states from real buffer\n",
    "    states = states.to(torch.float32)\n",
    "\n",
    "    for state in states:\n",
    "        uncertainty_sum = 0  # Track uncertainty to adjust rollout length\n",
    "        for _ in range(adaptive_rollout.rollout_length):\n",
    "            action, _ = policy.sample_action(state.unsqueeze(0))\n",
    "            action = torch.tensor(action, dtype=torch.float32).unsqueeze(0)  # Ensure action has the same dimension as state\n",
    "            next_state, reward, uncertainty = model.predict(state.unsqueeze(0), action)\n",
    "            model_buffer.add(state.numpy(), action.squeeze(0).numpy(), reward.numpy(), next_state.numpy(), False)\n",
    "            state = next_state.squeeze(0)  # Remove batch dimension for the next iteration\n",
    "            uncertainty_sum += uncertainty\n",
    "        \n",
    "        avg_uncertainty = uncertainty_sum / adaptive_rollout.rollout_length\n",
    "        adaptive_rollout.update(avg_uncertainty)  # Adjust rollout length\n",
    "\n",
    "    model.train()\n",
    "    policy.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveRollout:\n",
    "    \"\"\" Manages the adaptive rollout length based on model uncertainty \"\"\"\n",
    "    def __init__(self, rollout_min=1, rollout_max=10, beta=0.1):\n",
    "        self.rollout_min = rollout_min\n",
    "        self.rollout_max = rollout_max\n",
    "        self.beta = beta\n",
    "        self.rollout_length = rollout_min\n",
    "\n",
    "    def update(self, uncertainty):\n",
    "        \"\"\" Adjust rollout length based on uncertainty \"\"\"\n",
    "        self.rollout_length = int(max(self.rollout_min, min(self.rollout_max, \n",
    "                               self.rollout_max * np.exp(-self.beta * uncertainty))))\n",
    "        return self.rollout_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\" Stochastic Policy Network for SAC \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, max_action, hidden_dim=256):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.mean = nn.Linear(hidden_dim, action_dim)  # Mean of action distribution\n",
    "        self.log_std = nn.Linear(hidden_dim, action_dim)  # Log standard deviation\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        mean = self.mean(x)\n",
    "        log_std = torch.clamp(self.log_std(x), -20, 2)  # Keep std in reasonable range\n",
    "        return mean, log_std\n",
    "\n",
    "    def sample_action(self, state):\n",
    "        \"\"\" Samples action using reparameterization trick (for SAC) \"\"\"\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        dist = torch.distributions.Normal(mean, std)\n",
    "        z = dist.rsample()  # Reparameterization trick\n",
    "        action = torch.tanh(z) * self.max_action  # Squash output to action space\n",
    "        log_prob = dist.log_prob(z) - torch.log(1 - action.pow(2) + 1e-6)\n",
    "        return action.detach().numpy(), log_prob.sum(dim=-1, keepdim=True)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\" Twin Q-Value Network for SAC \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(Critic, self).__init__()\n",
    "        # First Q-Network\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)  # Q-value output\n",
    "\n",
    "        # Second Q-Network\n",
    "        self.fc4 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc5 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc6 = nn.Linear(hidden_dim, 1)  # Second Q-value output\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\" Forward pass for both Q-networks \"\"\"\n",
    "        x1 = torch.cat([state, action], dim=-1)\n",
    "        x1 = torch.relu(self.fc1(x1))\n",
    "        x1 = torch.relu(self.fc2(x1))\n",
    "        q1 = self.fc3(x1)\n",
    "\n",
    "        x2 = torch.cat([state, action], dim=-1)\n",
    "        x2 = torch.relu(self.fc4(x2))\n",
    "        x2 = torch.relu(self.fc5(x2))\n",
    "        q2 = self.fc6(x2)\n",
    "\n",
    "        return q1, q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveMBPO:\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        self.actor = Actor(state_dim, action_dim, max_action)\n",
    "        self.critic1 = Critic(state_dim, action_dim)\n",
    "        self.critic2 = Critic(state_dim, action_dim)\n",
    "        self.target_critic1 = Critic(state_dim, action_dim)\n",
    "        self.target_critic2 = Critic(state_dim, action_dim)\n",
    "        self.dynamics_model = EnsembleDynamicsModel(state_dim, action_dim)\n",
    "        self.adaptive_rollout = AdaptiveRollout(rollout_min=1, rollout_max=10, beta=0.1)\n",
    "        \n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "        self.critic_optimizer = torch.optim.Adam(list(self.critic1.parameters()) + list(self.critic2.parameters()), lr=3e-4)\n",
    "\n",
    "        self.gamma = 0.99\n",
    "        self.tau = 0.005\n",
    "        self.alpha = 0.2\n",
    "\n",
    "    def train_model(self, real_buffer, model_buffer):\n",
    "        \"\"\" Train dynamics model and generate adaptive-length rollouts \"\"\"\n",
    "        batch_size = 256\n",
    "        states, actions, rewards, next_states, dones = real_buffer.sample(batch_size)\n",
    "\n",
    "        predicted_next_states, predicted_rewards, _ = self.dynamics_model.predict(states, actions)\n",
    "        model_loss = nn.MSELoss()(predicted_next_states, next_states) + nn.MSELoss()(predicted_rewards, rewards)\n",
    "\n",
    "        optimizer = torch.optim.Adam(\n",
    "            [param for model in self.dynamics_model.models for param in model.parameters()], \n",
    "            lr=1e-3\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        model_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        generate_adaptive_model_rollouts(self.dynamics_model, self.actor, real_buffer, model_buffer, self.adaptive_rollout)\n",
    "\n",
    "    def train_sac(self, replay_buffer):\n",
    "        \"\"\" Train SAC policy using both real and synthetic experiences \"\"\"\n",
    "        states, actions, rewards, next_states, dones = replay_buffer.sample(256)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_actions, next_log_probs = self.actor.sample_action(next_states)\n",
    "            q1_next, q2_next = self.target_critic1(next_states, next_actions), self.target_critic2(next_states, next_actions)\n",
    "            min_q_next = torch.min(q1_next, q2_next) - self.alpha * next_log_probs\n",
    "            target_q = rewards + self.gamma * (1 - dones) * min_q_next\n",
    "\n",
    "        q1, q2 = self.critic1(states, actions), self.critic2(states, actions)\n",
    "        critic_loss = nn.MSELoss()(q1, target_q) + nn.MSELoss()(q2, target_q)\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: Total Reward = -312.44342318839784\n",
      "Episode 1: Total Reward = -344.8832943453535\n",
      "Episode 2: Total Reward = -274.44077136748234\n",
      "Episode 3: Total Reward = -301.9231116595517\n",
      "Episode 4: Total Reward = -275.7856033364171\n",
      "Episode 5: Total Reward = -471.3448935713538\n",
      "Episode 6: Total Reward = -238.518042016767\n",
      "Episode 7: Total Reward = -338.72639772565344\n",
      "Episode 8: Total Reward = -300.12976323873215\n",
      "Episode 9: Total Reward = -323.26175059823134\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# Train adaptive MBPO once enough real data is collected\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(real_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m10000\u001b[39m:\n\u001b[1;32m---> 32\u001b[0m         \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m         agent\u001b[38;5;241m.\u001b[39mtrain_sac(real_buffer)\n\u001b[0;32m     35\u001b[0m adaptive_mbpo_rewards\u001b[38;5;241m.\u001b[39mappend(total_reward)\n",
      "Cell \u001b[1;32mIn[39], line 34\u001b[0m, in \u001b[0;36mAdaptiveMBPO.train_model\u001b[1;34m(self, real_buffer, model_buffer)\u001b[0m\n\u001b[0;32m     31\u001b[0m model_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 34\u001b[0m \u001b[43mgenerate_adaptive_model_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdynamics_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madaptive_rollout\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[36], line 15\u001b[0m, in \u001b[0;36mgenerate_adaptive_model_rollouts\u001b[1;34m(model, policy, real_buffer, model_buffer, adaptive_rollout)\u001b[0m\n\u001b[0;32m     13\u001b[0m action, _ \u001b[38;5;241m=\u001b[39m policy\u001b[38;5;241m.\u001b[39msample_action(state\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m     14\u001b[0m action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(action, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Ensure action has the same dimension as state\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m next_state, reward, uncertainty \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m model_buffer\u001b[38;5;241m.\u001b[39madd(state\u001b[38;5;241m.\u001b[39mnumpy(), action\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy(), reward\u001b[38;5;241m.\u001b[39mnumpy(), next_state\u001b[38;5;241m.\u001b[39mnumpy(), \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Remove batch dimension for the next iteration\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 22\u001b[0m, in \u001b[0;36mEnsembleDynamicsModel.predict\u001b[1;34m(self, state, action)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, action):\n\u001b[0;32m     21\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Predict next state and reward with model ensemble \"\"\"\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m [model(inputs) \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels]\n\u001b[0;32m     25\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(predictions)  \u001b[38;5;66;03m# Shape: (num_models, batch, output_dim)\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 3"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize environment\n",
    "env = gym.make(\"HalfCheetah-v5\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "# Initialize agent, replay buffers\n",
    "agent = AdaptiveMBPO(state_dim, action_dim, max_action)\n",
    "real_buffer = ReplayBuffer(max_size=int(1e6))\n",
    "model_buffer = ReplayBuffer(max_size=int(1e6))\n",
    "\n",
    "num_episodes = 500\n",
    "adaptive_mbpo_rewards = []\n",
    "\n",
    "# Training loop\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    for step in range(1000):  # Maximum steps per episode\n",
    "        action, _ = agent.actor.sample_action(torch.tensor(state, dtype=torch.float32))\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        real_buffer.add(state, action, reward, next_state, done)\n",
    "        total_reward += reward\n",
    "        state = next_state if not done else env.reset()\n",
    "\n",
    "        # Train adaptive MBPO once enough real data is collected\n",
    "        if len(real_buffer) > 10000:\n",
    "            agent.train_model(real_buffer, model_buffer)\n",
    "            agent.train_sac(real_buffer)\n",
    "\n",
    "    adaptive_mbpo_rewards.append(total_reward)\n",
    "    print(f\"Episode {episode}: Total Reward = {total_reward}\")\n",
    "\n",
    "# Plot results\n",
    "plt.plot(adaptive_mbpo_rewards, label=\"Adaptive Rollout MBPO\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"Adaptive Rollout MBPO on HalfCheetah-v4\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
