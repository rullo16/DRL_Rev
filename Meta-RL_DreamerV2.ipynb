{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-RL with DreamerV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meta-Reinforcement Learning (Meta-RL), also known as \"learning to learn\", addresses the fundamental challenge in traditional RL: the fifficulty of efficiently adapting to new, unseen tasks after training. While traditional RL typically requires extensive training for each new task, Meta-RL algorithms learn general-purpose policies or learning rules capable of rapid adaptation.\n",
    "Integrating Meta-RL allows to rapidly adpat to new tasks from very few interactions(few-shot adaptation), efficiently generalize learned policies to previously unseen scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meta-Learning involves two nested levels of learning:\n",
    "- Inner-loop (Adaptation): Fast learning that occurs within a new task, typically using a small amount of data.\n",
    "- Outer-loop (Meta-learning): Slower learning process that trains a meta-learner to generalize well and adapt quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meta-RK aims to find parameters $\\theta$ (for model or policy) that minimize expected loss after adaptation steps on task T:\n",
    "$$\\min_\\theta \\mathbb{E}_{T \\sim p(T)} [L_T(\\theta - \\alpha \\nabla_\\theta L_T(\\theta))]$$\n",
    "Where:\n",
    "- $T \\sim p(T)$: Task distribution\n",
    "- $L_T$: Loss on task T\n",
    "- $\\alpha$: Step size for adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner-Loop AdaptationStep (Task-Specific)\n",
    "Given task-specific data $D_T$, compute adapted parameters $\\theta'_T$:\n",
    "$$\\theta'_T = \\theta - \\alpha \\nabla_\\theta L_T(D_T, \\theta)$$\n",
    "\n",
    "### Outer-Loop Meta-Update (Meta-Learning)\n",
    "Update meta-parameters $\\theta$ using adapted task parameters $\\theta'_T$:\n",
    "$$\\theta \\leftarrow \\theta - \\beta \\nabla_\\theta \\sum_{T \\sim p(T)} L_T(D^{val}_T, \\theta'_T)$$\n",
    "Where:\n",
    "- $D^{val}_T$: Validation data for task T\n",
    "- $\\beta$: Meta-learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode\n",
    "``` pseudocode\n",
    "for each meta-training epoch:\n",
    "    for each task T sampled from task distribution:\n",
    "        1. Collect a small adaptation set D_train for task T.\n",
    "        2. compute adapted parameters \\theta'_T from D_train (inner-loop adaptation)\n",
    "        3. Evaluated adapted parameters on D_val (task validation set)\n",
    "    4. Update meta-parameters \\theta using aggregated gradient from all tasks.\n",
    "```\n",
    "**Inner-Loop Adaptation**\n",
    "\n",
    "``` python\n",
    "def adapt(theta, D_train, alpha):\n",
    "    for step in range(adapt_steps):\n",
    "        # Compute loss L_train(theta, D_train)\n",
    "        theta = theta - alpha * grad(L_train(theta, D_train))\n",
    "    return theta\n",
    "```\n",
    "**Outer-Loop Optimization**\n",
    "\n",
    "```python\n",
    "for epoch in range(meta_learning_epochs):\n",
    "    meta_loss = 0\n",
    "    for task in tasks:\n",
    "        theta_task = adapt(theta, D_train[taks], alpha)\n",
    "        meta_loss += L_val(theta_task, D_val[task])\n",
    "    theta = theta - beta * grad(meta_loss)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, image_shape, latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2), nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        dummy_input = torch.zeros(1, *image_shape)\n",
    "        with torch.no_grad():\n",
    "            flat_dim = self.encoder(dummy_input).view(1, -1).shape[1]\n",
    "\n",
    "        # This line fixes the error:\n",
    "        self.fc = nn.Linear(flat_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x / 255.0\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSSM(nn.Module):\n",
    "    def __init__(self, latent_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRUCell(latent_dim + action_dim, latent_dim)\n",
    "        self.mu_layer = nn.Linear(latent_dim, latent_dim)\n",
    "        self.logvar_layer = nn.Linear(latent_dim, latent_dim)\n",
    "\n",
    "    def forward(self, h, z, a):\n",
    "        if a.dim() == 1:\n",
    "            a = a.unsqueeze(0)\n",
    "        if h.dim() == 1:\n",
    "            h = h.unsqueeze(0)\n",
    "\n",
    "        x = torch.cat([z, a], dim=-1)\n",
    "        h_next = self.gru(x, h)\n",
    "        mu, logvar = self.mu_layer(h_next), self.logvar_layer(h_next)\n",
    "        z_next = mu + torch.exp(0.5 * logvar) * torch.randn_like(mu)\n",
    "\n",
    "        return h_next, z_next, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, latent_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, action_dim), nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntrinsicReward(nn.Module):\n",
    "    def __init__(self, latent_dim, intrinsic_scale=0.1):\n",
    "        super().__init__()\n",
    "        self.intrinsic_scale = intrinsic_scale\n",
    "        self.fc = nn.Linear(latent_dim, 1)\n",
    "\n",
    "    def forward(self, z_pred, z_next):\n",
    "        error = (z_next - z_pred)**2  # shape: [batch_size, latent_dim]\n",
    "        intrinsic_reward = self.fc(error)  # Now correctly shaped\n",
    "        return self.intrinsic_scale * intrinsic_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaDreamerV2Agent:\n",
    "\n",
    "    def __init__(self, image_shape, action_dim, latent_dim=32):\n",
    "        self.image_shape = image_shape\n",
    "        self.action_dim = action_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = ConvEncoder(image_shape, latent_dim).to(device)\n",
    "        self.rssm = RSSM(latent_dim, action_dim).to(device)\n",
    "        self.actor = Actor(latent_dim, action_dim).to(device)\n",
    "        self.critic = Critic(latent_dim).to(device)\n",
    "        self.intrinsic_reward = IntrinsicReward(latent_dim).to(device)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=1e-4)\n",
    "\n",
    "    def parameters(self):\n",
    "        return list(self.encoder.parameters()) + list(self.rssm.parameters()) + list(self.actor.parameters()) + list(self.critic.parameters()) + list(self.intrinsic_reward.parameters())\n",
    "    \n",
    "    def clone(self):\n",
    "        clone = MetaDreamerV2Agent(self.image_shape, self.action_dim, self.latent_dim)\n",
    "        clone.load_state_dict(self.state_dict())\n",
    "        return clone\n",
    "    \n",
    "    def state_dict(self):\n",
    "        return {\n",
    "            'encoder': self.encoder.state_dict(),\n",
    "            'rssm': self.rssm.state_dict(),\n",
    "            'actor': self.actor.state_dict(),\n",
    "            'critic': self.critic.state_dict(),\n",
    "            'intrinsic_reward': self.intrinsic_reward.state_dict()\n",
    "        }\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.encoder.load_state_dict(state_dict['encoder'])\n",
    "        self.rssm.load_state_dict(state_dict['rssm'])\n",
    "        self.actor.load_state_dict(state_dict['actor'])\n",
    "        self.critic.load_state_dict(state_dict['critic'])\n",
    "        self.intrinsic_reward.load_state_dict(state_dict['intrinsic_reward'])\n",
    "\n",
    "    def adapt(self, obs_seq, action_seq, reward_seq, steps=1, lr=1e-3):\n",
    "        adapted = self.clone()\n",
    "        optimizer = optim.Adam(adapted.parameters(), lr=lr)\n",
    "\n",
    "        for _ in range(steps):\n",
    "            loss = adapted.compute_loss(obs_seq, action_seq, reward_seq)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        return adapted\n",
    "    \n",
    "    def compute_loss(self, obs_seq, action_seq, reward_seq):\n",
    "        z = self.encoder(obs_seq[0])\n",
    "        h = torch.zeros_like(z).to(device)\n",
    "\n",
    "        zs, rewards = [], []\n",
    "        total_loss = 0.0\n",
    "        for t in range(len(action_seq)):\n",
    "            h,z_pred, _,_ = self.rssm(h, z, action_seq[t])\n",
    "            z_next = self.encoder(obs_seq[t+1])\n",
    "\n",
    "            intrinsic_reward = self.intrinsic_reward(z_pred, z_next)\n",
    "\n",
    "            combined_reward = reward_seq[t] + intrinsic_reward\n",
    "\n",
    "            zs.append(z_pred)\n",
    "            rewards.append(combined_reward)\n",
    "\n",
    "            z = z_next\n",
    "\n",
    "        values = [self.critic(z) for z in zs]\n",
    "        targets = [rewards[i] + 0.99 * values[i+1].detach() if i+1 < len(values) else rewards[i] for i in range(len(values))]\n",
    "        value_loss = sum((values[i]-targets[i]).pow(2).mean() for i in range(len(values)))\n",
    "\n",
    "        actor_loss = -sum(self.critic(z).mean() for z in zs)\n",
    "        total_loss = value_loss + actor_loss\n",
    "        return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Meta Loss: -0.270\n",
      "Epoch: 1, Meta Loss: -0.425\n",
      "Epoch: 2, Meta Loss: -0.761\n",
      "Epoch: 3, Meta Loss: 0.974\n",
      "Epoch: 4, Meta Loss: 0.349\n",
      "Epoch: 5, Meta Loss: -0.163\n",
      "Epoch: 6, Meta Loss: 0.318\n",
      "Epoch: 7, Meta Loss: -0.780\n",
      "Epoch: 8, Meta Loss: 0.735\n",
      "Epoch: 9, Meta Loss: -0.160\n",
      "Epoch: 10, Meta Loss: -0.022\n",
      "Epoch: 11, Meta Loss: 1.300\n",
      "Epoch: 12, Meta Loss: 0.190\n",
      "Epoch: 13, Meta Loss: -0.108\n",
      "Epoch: 14, Meta Loss: 0.009\n",
      "Epoch: 15, Meta Loss: 0.150\n",
      "Epoch: 16, Meta Loss: 0.104\n",
      "Epoch: 17, Meta Loss: 0.233\n",
      "Epoch: 18, Meta Loss: 0.151\n",
      "Epoch: 19, Meta Loss: -0.303\n",
      "Epoch: 20, Meta Loss: 1.548\n",
      "Epoch: 21, Meta Loss: -0.566\n",
      "Epoch: 22, Meta Loss: -0.096\n",
      "Epoch: 23, Meta Loss: -0.097\n",
      "Epoch: 24, Meta Loss: -0.074\n",
      "Epoch: 25, Meta Loss: -0.195\n",
      "Epoch: 26, Meta Loss: -0.021\n",
      "Epoch: 27, Meta Loss: 0.000\n",
      "Epoch: 28, Meta Loss: -0.024\n",
      "Epoch: 29, Meta Loss: 0.356\n",
      "Epoch: 30, Meta Loss: 0.170\n",
      "Epoch: 31, Meta Loss: 0.394\n",
      "Epoch: 32, Meta Loss: -0.133\n",
      "Epoch: 33, Meta Loss: -0.725\n",
      "Epoch: 34, Meta Loss: 0.005\n",
      "Epoch: 35, Meta Loss: 0.485\n",
      "Epoch: 36, Meta Loss: 0.028\n",
      "Epoch: 37, Meta Loss: -0.185\n",
      "Epoch: 38, Meta Loss: -0.371\n",
      "Epoch: 39, Meta Loss: -0.513\n",
      "Epoch: 40, Meta Loss: 0.084\n",
      "Epoch: 41, Meta Loss: 0.071\n",
      "Epoch: 42, Meta Loss: -0.398\n",
      "Epoch: 43, Meta Loss: 0.507\n",
      "Epoch: 44, Meta Loss: -0.249\n",
      "Epoch: 45, Meta Loss: -0.210\n",
      "Epoch: 46, Meta Loss: 0.206\n",
      "Epoch: 47, Meta Loss: 0.093\n",
      "Epoch: 48, Meta Loss: -0.177\n",
      "Epoch: 49, Meta Loss: -0.201\n",
      "Epoch: 50, Meta Loss: 0.435\n",
      "Epoch: 51, Meta Loss: -0.047\n",
      "Epoch: 52, Meta Loss: 0.173\n",
      "Epoch: 53, Meta Loss: -0.530\n",
      "Epoch: 54, Meta Loss: 0.282\n",
      "Epoch: 55, Meta Loss: 0.029\n",
      "Epoch: 56, Meta Loss: -0.196\n",
      "Epoch: 57, Meta Loss: 0.306\n",
      "Epoch: 58, Meta Loss: 0.572\n",
      "Epoch: 59, Meta Loss: 0.312\n",
      "Epoch: 60, Meta Loss: -0.418\n",
      "Epoch: 61, Meta Loss: 0.135\n",
      "Epoch: 62, Meta Loss: -0.038\n",
      "Epoch: 63, Meta Loss: -0.013\n",
      "Epoch: 64, Meta Loss: 0.174\n",
      "Epoch: 65, Meta Loss: 0.049\n",
      "Epoch: 66, Meta Loss: 0.194\n",
      "Epoch: 67, Meta Loss: -0.131\n",
      "Epoch: 68, Meta Loss: 0.354\n",
      "Epoch: 69, Meta Loss: -0.118\n",
      "Epoch: 70, Meta Loss: -0.080\n",
      "Epoch: 71, Meta Loss: -0.059\n",
      "Epoch: 72, Meta Loss: -0.118\n",
      "Epoch: 73, Meta Loss: 0.054\n",
      "Epoch: 74, Meta Loss: 0.283\n",
      "Epoch: 75, Meta Loss: 0.441\n",
      "Epoch: 76, Meta Loss: 0.267\n",
      "Epoch: 77, Meta Loss: 0.140\n",
      "Epoch: 78, Meta Loss: 0.720\n",
      "Epoch: 79, Meta Loss: -0.097\n",
      "Epoch: 80, Meta Loss: -0.234\n",
      "Epoch: 81, Meta Loss: 0.211\n",
      "Epoch: 82, Meta Loss: 0.543\n",
      "Epoch: 83, Meta Loss: 0.275\n",
      "Epoch: 84, Meta Loss: -0.240\n",
      "Epoch: 85, Meta Loss: 0.054\n",
      "Epoch: 86, Meta Loss: -0.053\n",
      "Epoch: 87, Meta Loss: -0.024\n",
      "Epoch: 88, Meta Loss: -1.029\n",
      "Epoch: 89, Meta Loss: 0.125\n",
      "Epoch: 90, Meta Loss: -0.140\n",
      "Epoch: 91, Meta Loss: 0.058\n",
      "Epoch: 92, Meta Loss: -0.061\n",
      "Epoch: 93, Meta Loss: 0.410\n",
      "Epoch: 94, Meta Loss: 0.193\n",
      "Epoch: 95, Meta Loss: -0.037\n",
      "Epoch: 96, Meta Loss: -0.074\n",
      "Epoch: 97, Meta Loss: -0.077\n",
      "Epoch: 98, Meta Loss: 1.813\n",
      "Epoch: 99, Meta Loss: -0.272\n"
     ]
    }
   ],
   "source": [
    "tasks = [\"CarRacing-v3\"]\n",
    "\n",
    "\n",
    "agent = MetaDreamerV2Agent((3,96,96),3)\n",
    "\n",
    "meta_epoch = 100\n",
    "\n",
    "for epoch in range(meta_epoch):\n",
    "    meta_loss = 0\n",
    "    for task in tasks:\n",
    "        env = gym.make(task)\n",
    "\n",
    "        obs, _ = env.reset()\n",
    "        obs_seq = [torch.tensor(obs, dtype=torch.float32).permute(2,0,1).unsqueeze(0).to(device)]\n",
    "        action_seq, reward_seq = [], []\n",
    "\n",
    "        for step in range(10):\n",
    "            action = env.action_space.sample()\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Append next_obs here directly after each step\n",
    "            obs_seq.append(torch.tensor(next_obs, dtype=torch.float32).permute(2,0,1).unsqueeze(0).to(device))\n",
    "            action_seq.append(torch.tensor(action, dtype=torch.float32).unsqueeze(0).to(device))\n",
    "            reward_seq.append(torch.tensor([reward], dtype=torch.float32).to(device))\n",
    "\n",
    "            if done: break\n",
    "\n",
    "        adapted = agent.adapt(obs_seq, action_seq, reward_seq, steps=5, lr=1e-3)\n",
    "\n",
    "        # Reset environment for evaluation\n",
    "        eval_obs, _ = env.reset()\n",
    "        eval_obs_tensor = torch.tensor(eval_obs, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "\n",
    "        # Obtain latent embedding using the adapted agent's encoder\n",
    "        z = adapted.encoder(eval_obs_tensor)\n",
    "\n",
    "        # Evaluate using the critic network to ensure differentiable meta-loss\n",
    "        value_estimate = adapted.critic(z)\n",
    "\n",
    "        # Compute differentiable meta-loss for gradient updates\n",
    "        meta_loss += -value_estimate.mean()\n",
    "\n",
    "    meta_loss /= len(tasks)\n",
    "    agent.optimizer.zero_grad()\n",
    "    meta_loss.backward()\n",
    "    agent.optimizer.step()\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Meta Loss: {meta_loss.item():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
