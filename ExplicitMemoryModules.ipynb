{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e48d3ab5",
   "metadata": {},
   "source": [
    "# Explicit Memory Modules in RL: Neural Turing Machines (NTMs) and Differentiable Neural Computers (DNCs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777d5f15",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "Problems with RNNs and LSTMs:\n",
    "- Hidden state has fixed size\n",
    "- Memory is implicitly stored - hard to query or manipulate\n",
    "- Long-term memory is fragile and vanishes during backpropagation\n",
    "We need:\n",
    "- A system that can store, access, and update information over long time spans\n",
    "- External memory that grows in capacity\n",
    "- Differentiable operations for end-to-end training\n",
    "Leads to architectures like:\n",
    "- Neural Turing Machines (NTMs)\n",
    "- Differentiable Neural Computers (DNCs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aabb41",
   "metadata": {},
   "source": [
    "## Neural Turing Machine (NTM)\n",
    "### Architecture\n",
    "- Controller: Neural Network, usually RNN/LSTM that drives memory operations\n",
    "- Memory Matrix: External memory $M \\in \\mathbb{R}^{N \\times W}$\n",
    "- Read/Write heads: Intergace to memory - emit weights to access locations\n",
    "key idea is to parameterize a neural network with differentiable read/write access to memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3722bc9",
   "metadata": {},
   "source": [
    "### Math\n",
    "**Memory Matrix**:\\\n",
    "At time t, memory is:\n",
    "$$M_t \\in \\mathbb{R}^{N \\times W}$$\n",
    "- N: number of memory slots\n",
    "- W: width (size of each memory vector)\n",
    "\n",
    "**Read Head**\\\n",
    "Produces a read weight vector $w_t^r \\in \\Delta^N$ (a probability distribution over N locations).\n",
    "The read vector is:\n",
    "$$r_t = \\sum_{i=1}^N w_t^r(i)M_t(i)$$\n",
    "So the read operation is a weighted sum of memory rows.\\\n",
    "**Write Head**\\\n",
    "The write process has two steps:\n",
    "1. Erase:\\\n",
    "Apply an erase vector $e_t \\in [0,1]^W$ and write weights $w_t^w \\in \\Delta^N$:\n",
    "$$M'_t(i) = M_{t-1}(i) \\cdot (1-w_t^w(i)e_t)$$\n",
    "2. Add:\\\n",
    "Apply an add vector $a_t \\in \\mathbb{R}^W$:\n",
    "$$M_t(i) = M'_t(i) + w_t^w(i)a_t$$\n",
    "Together, they enable seletive overwriting of memory.\\\n",
    "**Addressing Mechanism**\\\n",
    "Heads emit a key $k_t$ and strength $\\beta_t$, and compute similarity with memory:\n",
    "$$w_t^e(i)=\\frac{\\exp(\\beta_t \\cdot (k_t, M(i)))}{\\sum_{j=1}\\exp(\\beta_t \\cdot (k_t, M(j)))}$$\n",
    "(similarity is usually cosine similarity)\\\n",
    "The content-based addressing can be enhanced with:\n",
    "- Location-based shifts: shift the read/write heads to nearby locations\n",
    "- Sharpening: Make soft weights sharper (more deterministic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b60495",
   "metadata": {},
   "source": [
    "## Differentiable Neural Computers (DNC)\n",
    "DNC = NTM + Improvements\\\n",
    "Key Improvements:\n",
    "- Multiple read heads\n",
    "- Temporal memory linkage matrix $L_t$: tracks order of writes\n",
    "- Usage vector: tracks which memory locations are free\n",
    "- Dynamic memory allocation via least-used slot selection\n",
    "- Better gradient flow and scalability\n",
    "### Temporal Link Matrix\n",
    "$L_t(i,j) \\in [0,1]$ tracks that memory location i was written aften j.\\\n",
    "Allow backeard and forward traversal through memory, crucial for reasoing over sequences.\\\n",
    "This update is:\n",
    "$$L_t(i,j) = (1-w_t^w(i)-w_t^w(j))\\cdot L_{t-1}(i,j)+w_t^w(i) \\cdot prev_t(j)$$\n",
    "Where $prev_t$ is the previously written location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3389db3d",
   "metadata": {},
   "source": [
    "## Applications in RL\n",
    "Explicit memory modules can imrpve RL agents by enabling:\n",
    "- One-shot learning\n",
    "- Reasoning over temporal events\n",
    "- Learning algorithms, not just reactive policies\n",
    "- Handling partial observability and long-term dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec54f7c5",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2f870d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6e16000",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTMReadHead(nn.Module):\n",
    "    def __init__(self, memory_units, memory_unit_size, controller_dim):\n",
    "        super().__init__()\n",
    "        self.key_proj = nn.Linear(controller_dim, memory_unit_size)\n",
    "        self.beta_proj = nn.Linear(controller_dim, 1)\n",
    "\n",
    "    def forward(self, memory, controller_state):\n",
    "        key = torch.tanh(self.key_proj(controller_state))\n",
    "        beta = F.softplus(self.beta_proj(controller_state))\n",
    "\n",
    "        norm_mem = F.normalize(memory, dim=1)\n",
    "        norm_key = F.normalize(key.unsqueeze(1), dim=1)\n",
    "        sim = torch.sum(norm_mem * norm_key, dim=-1)\n",
    "\n",
    "        weights = F.softmax(beta*sim, dim=-1)\n",
    "        read_vec = torch.bmm(weights.unsqueeze(1), memory).squeeze(1)\n",
    "        return read_vec, weights\n",
    "\n",
    "class NTMWriteHead(nn.Module):\n",
    "    def __init__(self, memory_units, memory_unit_size, controller_dim):\n",
    "        super().__init__()\n",
    "        self.key_proj = nn.Linear(controller_dim, memory_unit_size)\n",
    "        self.beta_proj = nn.Linear(controller_dim, 1)\n",
    "        self.erase_proj = nn.Linear(controller_dim, memory_unit_size)\n",
    "        self.add_proj = nn.Linear(controller_dim, memory_unit_size)\n",
    "\n",
    "    def forward(self, memory, controller_state):\n",
    "        key = torch.tanh(self.key_proj(controller_state))\n",
    "        beta = F.softplus(self.beta_proj(controller_state))\n",
    "        erase = torch.sigmoid(self.erase_proj(controller_state))\n",
    "        add = torch.tanh(self.add_proj(controller_state))\n",
    "\n",
    "        norm_mem = F.normalize(memory, dim=1)\n",
    "        norm_key = F.normalize(key.unsqueeze(1), dim=1)\n",
    "        sim = torch.sum(norm_mem * norm_key, dim=-1)\n",
    "\n",
    "        weights = F.softmax(beta*sim, dim=-1)\n",
    "        erase_matrix = torch.bmm(weights.unsqueeze(-1), erase.unsqueeze(1))\n",
    "        add_matrix = torch.bmm(weights.unsqueeze(-1), add.unsqueeze(1))\n",
    "\n",
    "        memory = memory * (1-erase_matrix) + add_matrix\n",
    "        return memory, weights\n",
    "\n",
    "class NeuralTuringMachine(nn.Module):\n",
    "    def __init__(self, input_dim, controller_dim, output_dim, memory_units=128, memory_unit_size=20):\n",
    "        super().__init__()\n",
    "        self.controller = nn.LSTMCell(input_dim + memory_unit_size, controller_dim)\n",
    "        self.read_head = NTMReadHead(memory_units, memory_unit_size, controller_dim)\n",
    "        self.write_head = NTMWriteHead(memory_units, memory_unit_size, controller_dim)\n",
    "        self.output_layer = nn.Linear(controller_dim + memory_unit_size, output_dim)\n",
    "\n",
    "        self.memory_units = memory_units\n",
    "        self.memory_unit_size = memory_unit_size\n",
    "\n",
    "    def forward(self, x, memory, state):\n",
    "        h,c,read_vec = state\n",
    "        controller_input = torch.cat([x, read_vec], dim=-1)\n",
    "        h,c = self.controller(controller_input, (h,c))\n",
    "\n",
    "        read_vec, _ = self.read_head(memory, h)\n",
    "        memory, _ = self.write_head(memory, h)\n",
    "\n",
    "        output = self.output_layer(torch.cat([h, read_vec], dim=-1))\n",
    "        return output, memory, (h,c, read_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c5f14f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[ 0.0206, -0.1185,  0.1093, -0.0591, -0.0119,  0.0607, -0.0992, -0.0116]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Memory diff norm: tensor(0.0904, grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## Minimal Integration\n",
    "input_dim = 8\n",
    "controller_dim = 64\n",
    "output_dim = 8\n",
    "memory_units = 16\n",
    "memory_unit_size = 20\n",
    "\n",
    "ntm = NeuralTuringMachine(input_dim, controller_dim, output_dim, memory_units, memory_unit_size)\n",
    "\n",
    "batch_size = 1  # Set a valid batch size\n",
    "x = torch.randn(batch_size, input_dim)\n",
    "memory = torch.zeros(batch_size, memory_units, memory_unit_size)\n",
    "h = torch.zeros(batch_size, controller_dim)\n",
    "c = torch.zeros(batch_size, controller_dim)\n",
    "read_vec = torch.zeros(batch_size, memory_unit_size)\n",
    "state = (h, c, read_vec)\n",
    "\n",
    "output, update_memory, new_state = ntm(x, memory, state)\n",
    "\n",
    "print(\"Output:\", output)\n",
    "print(\"Memory diff norm:\", (update_memory - memory).norm())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b27a5df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/500], Loss: 0.3286\n",
      "Epoch [100/500], Loss: 0.0588\n",
      "Epoch [150/500], Loss: 0.0202\n",
      "Epoch [200/500], Loss: 0.0110\n",
      "Epoch [250/500], Loss: 0.0074\n",
      "Epoch [300/500], Loss: 0.0067\n",
      "Epoch [350/500], Loss: 0.0048\n",
      "Epoch [400/500], Loss: 0.0034\n",
      "Epoch [450/500], Loss: 0.0024\n",
      "Epoch [500/500], Loss: 0.0021\n"
     ]
    }
   ],
   "source": [
    "## Sequence Copying Taks for NTM\n",
    "\n",
    "SEQ_LEN = 5\n",
    "INPUT_DIM = 8\n",
    "OUTPUT_DIM = INPUT_DIM\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 500\n",
    "\n",
    "ntm = NeuralTuringMachine(INPUT_DIM, controller_dim, OUTPUT_DIM, memory_units, memory_unit_size)\n",
    "optimizer = optim.Adam(ntm.parameters(), lr=0.01)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def generate():\n",
    "    seq = torch.bernoulli(torch.full((BATCH_SIZE, SEQ_LEN, INPUT_DIM), 0.5))\n",
    "    return seq, seq.clone()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    x_seq, y_seq = generate()\n",
    "    memory = torch.zeros(BATCH_SIZE, 128, 20)\n",
    "    h = torch.zeros(BATCH_SIZE, 64)\n",
    "    c = torch.zeros(BATCH_SIZE, 64)\n",
    "    read_vec = torch.zeros(BATCH_SIZE, 20)\n",
    "    state = (h, c, read_vec)\n",
    "\n",
    "    loss = 0\n",
    "    outputs = []\n",
    "    for t in range(SEQ_LEN):\n",
    "        out, memory, state = ntm(x_seq[:, t], memory, state)\n",
    "        outputs.append(out)\n",
    "\n",
    "    outputs = torch.stack(outputs, dim=1)\n",
    "    loss = loss_fn(outputs, y_seq)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1)%50 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{EPOCHS}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcb8dd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50, Total Reward: 14.0\n",
      "Episode 100, Total Reward: 10.0\n",
      "Episode 150, Total Reward: 67.0\n",
      "Episode 200, Total Reward: 59.0\n",
      "Episode 250, Total Reward: 22.0\n",
      "Episode 300, Total Reward: 103.0\n",
      "Episode 350, Total Reward: 320.0\n",
      "Episode 400, Total Reward: 213.0\n",
      "Episode 450, Total Reward: 270.0\n",
      "Episode 500, Total Reward: 359.0\n"
     ]
    }
   ],
   "source": [
    "## Integrate NTM-Augmented RL Agent\n",
    "class NTMPolicy(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim, controller_dim=64, memory_units=32, memory_unit_size=20):\n",
    "        super().__init__()\n",
    "        self.ntm = NeuralTuringMachine(obs_dim, controller_dim, action_dim, memory_units, memory_unit_size)\n",
    "        self.actor = nn.Linear(controller_dim+memory_unit_size, action_dim)\n",
    "        self.critic = nn.Linear(controller_dim+memory_unit_size, 1)\n",
    "\n",
    "    def forward(self, obs, memory, state):\n",
    "        ntm_out, memory, state = self.ntm(obs, memory,state)\n",
    "        h, _, read_vec = state\n",
    "        features = torch.cat([h, read_vec], dim=-1)\n",
    "        logits = self.actor(features)\n",
    "        value = self.critic(features)\n",
    "        return logits, value, memory, state\n",
    "    \n",
    "env = gym.make('CartPole-v1')\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "policy = NTMPolicy(obs_dim, action_dim)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=0.001)\n",
    "\n",
    "def reinforce_train(episodes=500):\n",
    "    gamma=0.99\n",
    "    for ep in range(episodes):\n",
    "        obs = env.reset()[0]\n",
    "        memory = torch.zeros(1,32,20)\n",
    "        state = (torch.zeros(1,64), torch.zeros(1,64), torch.zeros(1,20))\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        total = 0\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "            logits, _, memory, state = policy(obs_tensor, memory, state)\n",
    "            dist = torch.distributions.Categorical(logits.softmax(dim=-1))\n",
    "            action = dist.sample()\n",
    "            log_probs.append(dist.log_prob(action))\n",
    "            obs, reward, truncated, terminated, _ = env.step(action.item())\n",
    "            done = truncated or terminated\n",
    "            rewards.append(reward)\n",
    "            total += reward\n",
    "\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for r in reversed(rewards):\n",
    "            R = r+gamma*R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "        returns = (returns-returns.mean())/(returns.std()+1e-8)\n",
    "\n",
    "        loss = -sum(lp * G for lp, G in zip(log_probs, returns))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (ep+1) % 50 == 0:\n",
    "            print(f'Episode {ep+1}, Total Reward: {total}')\n",
    "\n",
    "reinforce_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44faee6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "procgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
