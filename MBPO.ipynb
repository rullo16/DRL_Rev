{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-Based Policy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model-Free RL like SAC or PPO are highly flexible and generalizable. However, they require millions of interactions with the environment to learn effective policies.\n",
    "Each interaction is typically used only for a few gradient updates before being discarded, making learning process slow and data-inefficient.\n",
    "In real-world applications like robotics and autonomous driving, model-free can be impractical.\n",
    "\n",
    "Model-based Policy Optimization improves sample efficiency by introducing a learned model of the environement dynamics.\n",
    "- Instead of relying solely on costly real-world interactions, MBPO trains a NN to predict state transitions and rewards from state-action inputs.\n",
    "- This learned model is then used to simulate imaginary rollouts (hypothetical experience that supplements real data)\n",
    "- By treating the learned model as a simulator, MBPO allows the RL agent to learn from both real and synthetic experiences, dramatically reducing the number of real interactions needed.\n",
    "A Major problem with using a learned model is model bias, if the model makes inaccurate predictions, the policy trained on these predictions may become unreliable.\n",
    "\n",
    "MBPO solves this problem by:\n",
    "1. Learning an approximate dynamics model $p_\\theta(s_{t+1}|s_t, a_t)$ that predicts the next state and reward.\n",
    "2. Using short rollouts (1-5 steps) instead of long-term simulations to limit compounding errors.\n",
    "3. Maintain an ensemble of models to improve the reliability of predictions and estimate uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Formulation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RL agent aims to maximize the expected cumulative reward:\n",
    "$$J(\\pi) = \\mathbb{E}[\\sum_{t=0} \\gamma^t r_t]$$\n",
    "MBPO modifies this by incorporating synthetic experience into training. The learned model is trained using Maximum Likelihood Estimation (MLE):\n",
    "$$L_{model}(\\theta) = - \\mathbb{E}_{(s,a,s')\\sim D_{env}[\\log p_\\theta(s'|s,a)]}$$\n",
    "where $D_{env}$ is the real environment dataset of state transitions.\n",
    "\n",
    "The rollowut process starts from a real state $s_0$ and uses the model to simulate k steps:\n",
    "$$s_0 \\xrightarrow{a_0} s_1 \\xrightarrow{a_1} s_2 \\xrightarrow{a_2} ... \\xrightarrow{a_{k-1}} s_k$$\n",
    "\n",
    "The hybrid return estimate is:\n",
    "$$R_k(s_0;\\pi) = \\mathbb{E}_{a_t\\sim\\pi}[\\sum_{t=0}^{k-1} \\gamma^t r(s_t,a_t)+\\gamma^kV^\\pi(s_k)]$$\n",
    "where $V^\\pi(s_k)$ is the estimated value function.\n",
    "By limiting rollouts to a short horizon k, MBPO prevents model errors from accumulating too much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MBPO vs. Model-Free RL\n",
    "- SAC and PPO require large number of environment steps to achieve good performance.\n",
    "- MBPO achieves the same results in 5-10 times fewer environment steps by using synthetic data.\n",
    "- PPO is especially sample-inefficient becase is on-policy and cannot reuse past data.\n",
    "- MBPO maintains high final performance while significantly improving sample efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MBPO vs. Other Model-Based Methods\n",
    "\n",
    "Previous model-based RL struggled with:\n",
    "- Long-horizon model rollouts leading to severe model bias.\n",
    "- Overreliance on model predictions, causing poor final policy performance.\n",
    "- Complex computational planning techniques that were inefficient and hard to scale.\n",
    "\n",
    "MBPO addresses these issues by:\n",
    "- Limiting rollouts to a few steps to reduce error accumulation\n",
    "- Using an ensemble of models to improve prediction reliability.\n",
    "- Integrating synthetic and real data in a balanced way for stable learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamics model\n",
    "Predicts the next state and reward given a state-action pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DynamicModel(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(DynamicModel, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, state_dim+1) #predicts next state and reward\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        output = self.net(x)\n",
    "        next_state, reward = output[..., :-1], output[..., -1:]\n",
    "        return next_state, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Buffer RReplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\" A replay buffer to store real and model-generated experiences. \"\"\"\n",
    "    def __init__(self, max_size=int(1e6)):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\" Convert inputs to NumPy arrays before storing. \"\"\"\n",
    "        state = np.array(state, dtype=np.float32).flatten()\n",
    "        action = np.array(action, dtype=np.float32).flatten()\n",
    "        next_state = np.array(next_state, dtype=np.float32).flatten()\n",
    "        reward = np.float32(reward)  # Scalar\n",
    "        done = np.float32(done)  # Scalar\n",
    "        \n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\" Sample a batch of transitions and return PyTorch tensors. \"\"\"\n",
    "        batch = random.sample(self.buffer, min(batch_size, len(self.buffer)))\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        return (torch.tensor(np.stack(states), dtype=torch.float32),  \n",
    "                torch.tensor(np.stack(actions), dtype=torch.float32),\n",
    "                torch.tensor(rewards, dtype=torch.float32).unsqueeze(1),  \n",
    "                torch.tensor(np.stack(next_states), dtype=torch.float32),\n",
    "                torch.tensor(dones, dtype=torch.float32).unsqueeze(1))  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\" Stochastic policy network with Tanh squashing. \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, max_action, hidden_dim=256):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.mean = nn.Linear(hidden_dim, action_dim)\n",
    "        self.log_std = nn.Linear(hidden_dim, action_dim)\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        mean = self.mean(x)\n",
    "        log_std = self.log_std(x).clamp(-20, 2)\n",
    "        return mean, log_std\n",
    "\n",
    "    def sample_action(self, state):\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        normal_dist = torch.distributions.Normal(mean, std)\n",
    "        z = normal_dist.rsample()\n",
    "        action = torch.tanh(z) * self.max_action\n",
    "        log_prob = normal_dist.log_prob(z) - torch.log(1 - action.pow(2) + 1e-6)\n",
    "        log_prob = log_prob.sum(dim=-1, keepdim=True)\n",
    "        return action, log_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(Critic, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "        self.q1 = self.net\n",
    "        self.q2 = self.net\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        q1 = self.q1(x)\n",
    "        q2 = self.q2(x)\n",
    "        return q1, q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate model Rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_rollouts(model, policy, env_buffer, model_buffer, rollout_length=5, num_rollouts=400):\n",
    "    \"\"\"Generate synthetic transitions using an ensemble-based uncertainty threshold.\"\"\"\n",
    "    model.eval()\n",
    "    policy.eval()\n",
    "    \n",
    "    states, _, _, _, _ = env_buffer.sample(num_rollouts)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for state in states:\n",
    "            for rollout_step in range(rollout_length):\n",
    "                action, _ = policy.sample_action(state.unsqueeze(0))\n",
    "                next_state, reward = model.predict(state.unsqueeze(0), action.unsqueeze(0))\n",
    "\n",
    "                next_state = next_state.squeeze(0).numpy().flatten()\n",
    "                action = action.squeeze(0).numpy().flatten()\n",
    "\n",
    "                model_buffer.add(state.numpy(), action, reward.item(), next_state, False)\n",
    "                state = next_state\n",
    "\n",
    "    model.train()\n",
    "    policy.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rullo\\AppData\\Local\\Temp\\ipykernel_5564\\181819333.py:46: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  env_model_loss = F.mse_loss(pred_next_states, next_states) + F.mse_loss(pred_rewards, rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Env model completed\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('HalfCheetah-v5')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "env_model = DynamicModel(state_dim, action_dim)\n",
    "actor = Actor(state_dim, action_dim, max_action)\n",
    "critic = Critic(state_dim, action_dim)\n",
    "target_critic = Critic(state_dim, action_dim)\n",
    "\n",
    "target_critic.load_state_dict(critic.state_dict())\n",
    "\n",
    "env_model_optimizer = optim.Adam(env_model.parameters(), lr=1e-3)\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=1e-3)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=1e-3)\n",
    "\n",
    "env_buffer = ReplayBuffer()\n",
    "model_buffer = ReplayBuffer()\n",
    "\n",
    "num_env_steps = 50000\n",
    "env_steps_per_iter = 1000\n",
    "policy_updates_per_iter = 200\n",
    "rollout_length = 5\n",
    "batch_size = 256\n",
    "gamma = 0.99\n",
    "tau = 0.005\n",
    "\n",
    "state,_ = env.reset()\n",
    "for _ in range(env_steps_per_iter):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    env_buffer.add(state, action, reward, next_state, done)\n",
    "    state = next_state if not done else env.reset()\n",
    "\n",
    "    if len(env_buffer.buffer) >= batch_size:\n",
    "        for _ in range(50):\n",
    "            states, actions, rewards, next_states, dones = env_buffer.sample(batch_size)\n",
    "            pred_next_states, pred_rewards = env_model(states, actions)\n",
    "            pred_rewards = pred_rewards.squeeze(-1)\n",
    "            pred_rewards = pred_rewards.squeeze(-1)\n",
    "            env_model_loss = F.mse_loss(pred_next_states, next_states) + F.mse_loss(pred_rewards, rewards)\n",
    "            env_model_optimizer.zero_grad()\n",
    "            env_model_loss.backward()\n",
    "            env_model_optimizer.step()\n",
    "\n",
    "        generate_model_rollouts(env_model, actor, env_buffer, model_buffer, rollout_length=rollout_length, num_rollouts=100)\n",
    "\n",
    "print(\"Training Env model completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (128,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m batch_half_model \u001b[38;5;241m=\u001b[39m batch_size\u001b[38;5;241m-\u001b[39mbatch_half_env\n\u001b[0;32m     10\u001b[0m states_env, actions_env, rewards_env, next_states_env, dones_env \u001b[38;5;241m=\u001b[39m env_buffer\u001b[38;5;241m.\u001b[39msample(batch_half_env)\n\u001b[1;32m---> 11\u001b[0m states_model, actions_model, rewards_model, next_states_model, dones_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_half_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([states_env, states_model], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     14\u001b[0m actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([actions_env, actions_model], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 22\u001b[0m, in \u001b[0;36mReplayBuffer.sample\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     19\u001b[0m batch \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer, batch_size)\n\u001b[0;32m     20\u001b[0m states, actions, rewards, next_states, dones \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[0;32m     23\u001b[0m         torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(actions), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[0;32m     24\u001b[0m         torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(rewards), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     25\u001b[0m         torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(next_states), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[0;32m     26\u001b[0m         torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(dones), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (128,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# Run SAC\n",
    "alpha = 0.2\n",
    "\n",
    "state,_ = env.reset()\n",
    "for _ in range(policy_updates_per_iter):\n",
    "    batch_half = batch_size // 2\n",
    "    batch_half_env = min(batch_half, len(env_buffer))\n",
    "    batch_half_model = min(batch_half, len(model_buffer))\n",
    "\n",
    "    states_env, actions_env, rewards_env, next_states_env, dones_env = env_buffer.sample(batch_half_env)\n",
    "    states_model, actions_model, rewards_model, next_states_model, dones_model = model_buffer.sample(batch_half_model)\n",
    "\n",
    "    states = torch.cat([states_env, states_model], dim=0)\n",
    "    actions = torch.cat([actions_env, actions_model], dim=0)\n",
    "    rewards = torch.cat([rewards_env, rewards_model], dim=0)\n",
    "    next_states = torch.cat([next_states_env, next_states_model], dim=0)\n",
    "    dones = torch.cat([dones_env, dones_model], dim=0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_actions, next_log_probs = actor.sample_action(next_states)\n",
    "        target_q = torch.min(*target_critic(next_states, next_actions))-alpha * next_log_probs\n",
    "        y = rewards + gamma * (1-dones) * target_q\n",
    "\n",
    "    q1, q2 = critic(states, actions)\n",
    "    critic_loss = F.mse_loss(q1, y) + F.mse_loss(q2, y)\n",
    "\n",
    "    critic_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    critic_optimizer.step()\n",
    "\n",
    "    if _ % 2 == 0:\n",
    "        actor_loss = torch.mean(alpha * actor(states)[1] - critic(states, actor(states)[0])[0])\n",
    "        actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        actor_optimizer.step()\n",
    "\n",
    "        for param, target_param in zip(critic.parameters(), target_critic.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "    if _ % 10 == 0:\n",
    "        print(f\"Step: {_}, Actor Loss: {actor_loss.item()}, Critic Loss: {critic_loss.item()}\")\n",
    "\n",
    "print(\"Training SAC completed\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve Dynamics Model with Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single dynamics model can make inaccurate predictions, leading to model bias. Instead, we train an ensemble of models and randomly choose one for each synthetic step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleDynamicsModel(nn.Module):\n",
    "    \"\"\" Ensemble of neural networks to predict next state and reward. \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, num_models=5, hidden_dim=256):\n",
    "        super(EnsembleDynamicsModel, self).__init__()\n",
    "        self.models = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(state_dim + action_dim, hidden_dim), nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, state_dim + 1)  # Predicts next state (state_dim) and reward (1)\n",
    "            ) for _ in range(num_models)\n",
    "        ])\n",
    "        self.num_models = num_models\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        outputs = [model(x) for model in self.models]\n",
    "        next_states, rewards = zip(*[(out[..., :-1], out[..., -1:]) for out in outputs])\n",
    "        return next_states, rewards  \n",
    "\n",
    "    def predict(self, state, action):\n",
    "        \"\"\" Randomly select one model from the ensemble for a prediction \"\"\"\n",
    "        model_idx = np.random.randint(self.num_models)\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        output = self.models[model_idx](x)\n",
    "        next_state, reward = output[..., :-1], output[..., -1:]\n",
    "        return next_state, reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_model = EnsembleDynamicsModel(state_dim, action_dim, num_models=5)\n",
    "actor = Actor(state_dim, action_dim, max_action)\n",
    "critic = Critic(state_dim, action_dim)\n",
    "target_critic = Critic(state_dim, action_dim)\n",
    "\n",
    "target_critic.load_state_dict(critic.state_dict())\n",
    "\n",
    "env_model_optimizer = optim.Adam(env_model.parameters(), lr=1e-3)\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=1e-3)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=1e-3)\n",
    "\n",
    "env_buffer = ReplayBuffer()\n",
    "model_buffer = ReplayBuffer()\n",
    "\n",
    "num_env_steps = 50000\n",
    "env_steps_per_iter = 1000\n",
    "policy_updates_per_iter = 200\n",
    "rollout_length = 5\n",
    "batch_size = 256\n",
    "gamma = 0.99\n",
    "tau = 0.005\n",
    "\n",
    "state,_ = env.reset()\n",
    "for _ in range(env_steps_per_iter):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    env_buffer.add(state, action, reward, next_state, done)\n",
    "    state = next_state if not done else env.reset()\n",
    "\n",
    "    if len(env_buffer.buffer) >= batch_size:\n",
    "        for _ in range(50):\n",
    "            states, actions, rewards, next_states, dones = env_buffer.sample(batch_size)\n",
    "            pred_next_states, pred_rewards = env_model(states, actions)\n",
    "            pred_rewards = pred_rewards.squeeze(-1)\n",
    "            pred_rewards = pred_rewards.squeeze(-1)\n",
    "            env_model_loss = F.mse_loss(pred_next_states, next_states) + F.mse_loss(pred_rewards, rewards)\n",
    "            env_model_optimizer.zero_grad()\n",
    "            env_model_loss.backward()\n",
    "            env_model_optimizer.step()\n",
    "\n",
    "        generate_model_rollouts(env_model, actor, env_buffer, model_buffer, rollout_length=rollout_length, num_rollouts=100)\n",
    "\n",
    "print(\"Training Env model completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SAC\n",
    "alpha = 0.2\n",
    "\n",
    "state,_ = env.reset()\n",
    "for _ in range(policy_updates_per_iter):\n",
    "    batch_half = batch_size // 2\n",
    "    batch_half_env = min(batch_half, len(env_buffer))\n",
    "    batch_half_model = min(batch_half, len(model_buffer))\n",
    "\n",
    "    states_env, actions_env, rewards_env, next_states_env, dones_env = env_buffer.sample(batch_half_env)\n",
    "    states_model, actions_model, rewards_model, next_states_model, dones_model = model_buffer.sample(batch_half_model)\n",
    "\n",
    "    states = torch.cat([states_env, states_model], dim=0)\n",
    "    actions = torch.cat([actions_env, actions_model], dim=0)\n",
    "    rewards = torch.cat([rewards_env, rewards_model], dim=0)\n",
    "    next_states = torch.cat([next_states_env, next_states_model], dim=0)\n",
    "    dones = torch.cat([dones_env, dones_model], dim=0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_actions, next_log_probs = actor.sample_action(next_states)\n",
    "        target_q = torch.min(*target_critic(next_states, next_actions))-alpha * next_log_probs\n",
    "        y = rewards + gamma * (1-dones) * target_q\n",
    "\n",
    "    q1, q2 = critic(states, actions)\n",
    "    critic_loss = F.mse_loss(q1, y) + F.mse_loss(q2, y)\n",
    "\n",
    "    critic_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    critic_optimizer.step()\n",
    "\n",
    "    if _ % 2 == 0:\n",
    "        actor_loss = torch.mean(alpha * actor(states)[1] - critic(states, actor(states)[0])[0])\n",
    "        actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        actor_optimizer.step()\n",
    "\n",
    "        for param, target_param in zip(critic.parameters(), target_critic.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "    if _ % 10 == 0:\n",
    "        print(f\"Step: {_}, Actor Loss: {actor_loss.item()}, Critic Loss: {critic_loss.item()}\")\n",
    "\n",
    "print(\"Training SAC completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Rollout Length Based on Model Uncertainty\n",
    "\n",
    "MBPO uses a fixed rollout length, but longer rollouts can accumulate model errors, if the model has high uncertainty, longer rollouts can lead to bad policies.\n",
    "The solution is to measure the ensemble disagreement and stop rollouts when uncertainty is high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_rollouts(model, policy, env_buffer, model_buffer, max_rollout_length=5, num_rollouts=400):\n",
    "    model.eval()\n",
    "    policy.eval()\n",
    "\n",
    "    states, _,_,_,_= env_buffer.sample(num_rollouts)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for state in states:\n",
    "            for rollout_step in range(max_rollout_length):\n",
    "                action, _ = policy.sample_action(state.unsqueeze(0))\n",
    "                \n",
    "                next_state, reward = model(state.unsqueeze(0), action)\n",
    "                next_state = torch.stack(next_state)\n",
    "                reward = torch.stack(reward)\n",
    "\n",
    "                model_uncertainty = torch.var(next_state, dim=0).mean()\n",
    "\n",
    "                if model_uncertainty.item() > 0.05:\n",
    "                    break\n",
    "\n",
    "                next_state, reward = model(state.unsqueeze(0), action)\n",
    "\n",
    "                model_buffer.add(state.numpy(), action.numpy(), reward.item(), next_state.numpy(), False)\n",
    "\n",
    "                state = next_state.clone()\n",
    "\n",
    "    model.train()\n",
    "    policy.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_model = EnsembleDynamicsModel(state_dim, action_dim, num_models=5)\n",
    "actor = Actor(state_dim, action_dim, max_action)\n",
    "critic = Critic(state_dim, action_dim)\n",
    "target_critic = Critic(state_dim, action_dim)\n",
    "\n",
    "target_critic.load_state_dict(critic.state_dict())\n",
    "\n",
    "env_model_optimizer = optim.Adam(env_model.parameters(), lr=1e-3)\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=1e-3)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=1e-3)\n",
    "\n",
    "env_buffer = ReplayBuffer()\n",
    "model_buffer = ReplayBuffer()\n",
    "\n",
    "num_env_steps = 50000\n",
    "env_steps_per_iter = 1000\n",
    "policy_updates_per_iter = 200\n",
    "rollout_length = 5\n",
    "batch_size = 256\n",
    "gamma = 0.99\n",
    "tau = 0.005\n",
    "\n",
    "state,_ = env.reset()\n",
    "for _ in range(env_steps_per_iter):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    env_buffer.add(state, action, reward, next_state, done)\n",
    "    state = next_state if not done else env.reset()\n",
    "\n",
    "    if len(env_buffer.buffer) >= batch_size:\n",
    "        for _ in range(50):\n",
    "            states, actions, rewards, next_states, dones = env_buffer.sample(batch_size)\n",
    "            pred_next_states, pred_rewards = env_model(states, actions)\n",
    "            pred_rewards = pred_rewards.squeeze(-1)\n",
    "            pred_rewards = pred_rewards.squeeze(-1)\n",
    "            env_model_loss = F.mse_loss(pred_next_states, next_states) + F.mse_loss(pred_rewards, rewards)\n",
    "            env_model_optimizer.zero_grad()\n",
    "            env_model_loss.backward()\n",
    "            env_model_optimizer.step()\n",
    "\n",
    "        generate_model_rollouts(env_model, actor, env_buffer, model_buffer, rollout_length=rollout_length, num_rollouts=100)\n",
    "\n",
    "print(\"Training Env model completed\")\n",
    "# Run SAC\n",
    "alpha = 0.2\n",
    "\n",
    "state,_ = env.reset()\n",
    "for _ in range(policy_updates_per_iter):\n",
    "    batch_half = batch_size // 2\n",
    "    batch_half_env = min(batch_half, len(env_buffer.buffer))\n",
    "    batch_half_model = batch_size-batch_half_env\n",
    "\n",
    "    states_env, actions_env, rewards_env, next_states_env, dones_env = env_buffer.sample(batch_half_env)\n",
    "    states_model, actions_model, rewards_model, next_states_model, dones_model = model_buffer.sample(batch_size)\n",
    "\n",
    "    states = torch.cat([states_env, states_model], dim=0)\n",
    "    actions = torch.cat([actions_env, actions_model], dim=0)\n",
    "    rewards = torch.cat([rewards_env, rewards_model], dim=0)\n",
    "    next_states = torch.cat([next_states_env, next_states_model], dim=0)\n",
    "    dones = torch.cat([dones_env, dones_model], dim=0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_actions, next_log_probs = actor.sample_action(next_states)\n",
    "        target_q = torch.min(*target_critic(next_states, next_actions))-alpha * next_log_probs\n",
    "        y = rewards + gamma * (1-dones) * target_q\n",
    "\n",
    "    q1, q2 = critic(states, actions)\n",
    "    critic_loss = F.mse_loss(q1, y) + F.mse_loss(q2, y)\n",
    "\n",
    "    critic_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    critic_optimizer.step()\n",
    "\n",
    "    if _ % 2 == 0:\n",
    "        actor_loss = torch.mean(alpha * actor(states)[1] - critic(states, actor(states)[0])[0])\n",
    "        actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        actor_optimizer.step()\n",
    "\n",
    "        for param, target_param in zip(critic.parameters(), target_critic.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "    if _ % 10 == 0:\n",
    "        print(f\"Step: {_}, Actor Loss: {actor_loss.item()}, Critic Loss: {critic_loss.item()}\")\n",
    "\n",
    "print(\"Training SAC completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
